{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv89m9nBPXSh"
      },
      "source": [
        "# MNIST: Digits recognition with PyTorch \n",
        "\n",
        "All right, let's start then making sure we all know the basics! Let's recognize the ten handwritten digits learning from 60.000, 28x28 grayscale images.\n",
        "For simplicity let's import a loading script we have already developed inside the **Continual AI Colab** repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3BFVukM_y8i",
        "outputId": "e3c9a181-6cbd-44e1-d681-beb090625e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded!\n"
          ]
        }
      ],
      "source": [
        "from modules.data.mnist import init, load\n",
        "init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jIk6-G6AhWi",
        "outputId": "f6114ff9-6bd6-4c01-b4fb-7f164dc6a5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train dim and type:  (60000, 1, 28, 28) float32\n",
            "t_train dim and type:  (60000,) uint8\n",
            "x_test dim and type:  (10000, 1, 28, 28) float32\n",
            "t_test dim and type:  (10000,) uint8\n"
          ]
        }
      ],
      "source": [
        "x_train, t_train, x_test, t_test = load()\n",
        "\n",
        "print(\"x_train dim and type: \", x_train.shape, x_train.dtype)\n",
        "print(\"t_train dim and type: \", t_train.shape, t_train.dtype)\n",
        "print(\"x_test dim and type: \", x_test.shape, x_test.dtype)\n",
        "print(\"t_test dim and type: \", t_test.shape, t_test.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "60000it [00:00, 556939.75it/s]\n",
            "10000it [00:00, 444585.02it/s]\n"
          ]
        }
      ],
      "source": [
        "def episode_splits(train, test):\n",
        "    splits = {label: [] for label in np.unique(test)}\n",
        "    for img, label in tqdm(zip(train, test)):\n",
        "        splits[label].append(img)\n",
        "    return splits\n",
        "\n",
        "train_set = episode_splits(x_train, t_train)\n",
        "test_set = episode_splits(x_test, t_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEWG2PmbVvb7"
      },
      "source": [
        "Let's take a look at the actual images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "RyIuYAw8AuO6",
        "outputId": "07106299-84cf-4cef-b9ba-802d370a6fe9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "f, axarr = plt.subplots(2,2)\n",
        "axarr[0,0].imshow(train_set[0][0][0], cmap=\"gray\")\n",
        "axarr[0,1].imshow(train_set[1][0][0], cmap=\"gray\")\n",
        "axarr[1,0].imshow(train_set[3][0][0], cmap=\"gray\")\n",
        "axarr[1,1].imshow(train_set[4][0][0], cmap=\"gray\")\n",
        "np.vectorize(lambda ax:ax.axis('off'))(axarr);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baEsU4PGXsgS"
      },
      "source": [
        "Good! Let's now set up a few general setting before using torch..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ztZAPQNXZ4ll"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ek0mErIac6n"
      },
      "source": [
        "... and define our first conv-net! We will use 3 layers of convolutions and two fully connected layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ONMdybG4Be0z"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 5, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(5, 10, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(250, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 250)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num params: 3020\n"
          ]
        }
      ],
      "source": [
        "import posteriors\n",
        "import torchopt\n",
        "from torch import func\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "print(\"Num params:\", sum([p.numel() for p in model.parameters()]))\n",
        "\n",
        "def log_posterior(num_data):\n",
        "    def fn_call(params, batch):\n",
        "        images, labels = batch\n",
        "        output = func.functional_call(model, params, images)\n",
        "        log_post_val = (\n",
        "            -F.cross_entropy(output, labels)\n",
        "            +  posteriors.diag_normal_log_prob(params) / num_data\n",
        "        )\n",
        "        return log_post_val, output\n",
        "    return fn_call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:37<00:00,  3.77s/it]\n"
          ]
        }
      ],
      "source": [
        "def train_for_episode(params, optimizer, data, label, epochs=5, batch_size=100):\n",
        "    state = optimizer.init(params)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            x = torch.stack([torch.from_numpy(p) for p in data[i:i+batch_size]])\n",
        "            y = torch.ones((x.size(0),), dtype=torch.long) * label            \n",
        "            state = optimizer.update(state, (x.to(device), y.to(device)), inplace=False)\n",
        "\n",
        "    return state\n",
        "\n",
        "@torch.inference_mode\n",
        "def test_for_episode(params, data, label, batch_size=100):\n",
        "    num_correct = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        x = torch.stack([torch.from_numpy(p) for p in data[i:i+batch_size]])\n",
        "        outputs = func.functional_call(model, params, x.to(device))\n",
        "        outputs = outputs.argmax(-1)\n",
        "        num_correct += len(torch.where(outputs == label)[0])\n",
        "\n",
        "    return num_correct / len(data)\n",
        "\n",
        "episodes = list(train_set.keys())\n",
        "test_metrics = []\n",
        "params = dict(model.named_parameters())\n",
        "\n",
        "for episode in tqdm(episodes): \n",
        "    label = episode\n",
        "    episode_train_data = train_set[episode]\n",
        "    optimizer = posteriors.torchopt.build(log_posterior(len(episode_train_data)), torchopt.adam(lr=1e-3, maximize=True))\n",
        "    state = train_for_episode(params, optimizer, episode_train_data, label, epochs=1, batch_size=10)\n",
        "    params = state.params\n",
        "\n",
        "    step_i_test_metrics = {}\n",
        "    for prev_episodes in episodes:\n",
        "        episode_test_data = test_set[prev_episodes]\n",
        "        accuracy = test_for_episode(params, episode_test_data, prev_episodes, batch_size=10)\n",
        "        step_i_test_metrics[prev_episodes] = accuracy\n",
        "\n",
        "    test_metrics.append(step_i_test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 1: {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 2: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 1.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "Episode 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 1.0, 8: 0.0, 9: 0.0}\n",
            "Episode 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 1.0, 9: 0.0}\n",
            "Episode 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 1.0}\n"
          ]
        }
      ],
      "source": [
        "for i, metrics in enumerate(test_metrics):\n",
        "    print(f\"Episode {i}: {metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "permuted_and_split_mnist.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
