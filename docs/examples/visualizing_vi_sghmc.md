# Visualizing VI and SGHMC on a Multimodal Distribution

In this example, we'll compare `uqlib.vi.diag` and `vi.sgmcmc.sghmc` for approximating
a two dimensional distribution which we can visualize.

## Target distribution

We'll start by defining the target distribution, a two dimensional double well

```python
import torch
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import torchopt
import uqlib

torch.manual_seed(42)

def log_posterior(x, batch):
    log_prob = -torch.sum(x**4, axis=-1) / 10.0 + torch.sum(x**2, axis=-1)
    return log_prob, torch.tensor([])
```

Not that the `log_posterior` has to conform to the signature `log_posterior(params, batch) -> log_prob, aux` where `params` is a tensor of shape `(batch_size, num_params)` and 
`batch` is a dictionary of tensors. More info on the
[constructing log posteriors](../log_posteriors.md) page.

In this simple, example we don't have varying batches so we'll just ignore that input.
We also don't have any auxiliary information we'd like to keep hold of, so we just
return an empty tensor.


## Variational Inference

Now we'll fit a diagonal Gaussian variational distribution. This is easy with `uqlib`:

```python
vi_transform = uqlib.vi.diag.build(
    log_posterior, optimizer=torchopt.adam(lr=1e-2), init_log_sds=-2.0
)
n_vi_steps = 2000
vi_state = vi_transform.init(torch.zeros(2))

nelbos = []
for _ in range(n_vi_steps):
    vi_state = vi_transform.update(vi_state, None)
    nelbos.append(vi_state.nelbo.item())
```

Here, we've tracked the values of the negative ELBO (NELBO), let's have a look at them:

```python
plt.plot(nelbos)
plt.ylabel("NELBO")
```

![NELBO](https://storage.googleapis.com/posteriors/double_well_nelbo.png)

Looks like its converged, but there's a fair amount of variance around the minima,
maybe a Gaussian isn't a great fit for our target distribution....


## SGHMC

Let's generating samples with `uqlib.vi.sgmcmc.sghmc` instead:

```python
sghmc_transform = uqlib.sgmcmc.sghmc.build(log_posterior, lr=5e-2, alpha=1.0)
n_sghmc_steps = 10000
sghmc_state = sghmc_transform.init(torch.zeros(2))

samples = torch.zeros(1, 2)
log_posts = []
for _ in range(n_sghmc_steps):
    sghmc_state = sghmc_transform.update(sghmc_state, None)
    samples = torch.cat([samples, sghmc_state.params.unsqueeze(0)], axis=0)
    log_posts.append(sghmc_state.log_posterior.item())
```

Here we've tracked the values of the log posterior, let's have a look at them:

```python
plt.plot(log_posts)
plt.ylabel("SGHMC Log Posterior")
```

![SGHMC](https://storage.googleapis.com/posteriors/double_well_sghmc_log_post.png)

Certainly some exploration going on!


## Visualizing

Time to visualize the learnt variational distribution and the samples generated by SGHMC:

```python
lim = 4
x = torch.linspace(-lim, lim, 1000)
X, Y = torch.meshgrid(x, x)
Z = torch.vmap(log_posterior, in_dims=(0, None))(torch.stack([X, Y], axis=-1), None)[0]
plt.contourf(X, Y, Z, levels=50, cmap="Purples", alpha=0.3, zorder=-1)

mean = vi_state.params
sd_diag = torch.exp(vi_state.log_sd_diag)
Z_gauss = torch.vmap(
    lambda z: -torch.sum(torch.square((z - mean) / sd_diag), axis=-1) / 2.0,
)(torch.stack([X, Y], axis=-1))

plt.contour(X, Y, Z_gauss, levels=5, colors="black", alpha=0.5)
sghmc_samps = plt.scatter(
    samples[:, 0], samples[:, 1], c="r", s=0.5, alpha=0.5, label="SGHMC Samples"
)

vi_legend_line = mlines.Line2D(
    [], [], color="black", label="VI", alpha=0.5, linestyle="--"
)
plt.legend(handles=[vi_legend_line, sghmc_samps])
plt.xlim(-lim, lim)
plt.ylim(-lim, lim)
```
![Double Well](https://storage.googleapis.com/posteriors/double_well_compare.png)

We can see the variational Gaussian ignores the multiple modes, 
but SGHMC explores them well.

!!! note
    The raw code for this example can be found in the repo at [examples/double_well.py](https://github.com/normal-computing/uqlib/blob/main/examples/double_well.py).

