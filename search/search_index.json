{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>  \u00a0 posteriors </p>"},{"location":"#uncertainty-quantification-with-pytorch","title":"Uncertainty quantification with PyTorch","text":"<p> Get started Tutorials Contribute API </p> <p></p> <p>Composable: Use with <code>transformers</code>, <code>lightning</code>, <code>torchopt</code>, <code>torch.distributions</code> and more!</p> <p>Extensible: Add new methods! Add new models!</p> <p>Functional: Easier to test, closer to mathematics!</p> <p>Scalable: Big model? Big data? No problem!</p> <p>Swappable: Swap between algorithms with ease!</p>"},{"location":"contributing/","title":"Contributing","text":"<p>If you want to add a new algorithm, example, feature, report or fix a bug, please open  an issue on GitHub.  We'd love to have you involved in any capacity!</p> <p>If you are interested in contributing to <code>posteriors</code>, please follow these steps:</p> <ol> <li>Fork the repo from GitHub and clone it locally: <pre><code>git clone git@github.com/YourUserName/posteriors.git\ncd posteriors\n</code></pre></li> <li>Install the development dependencies and pre-commit hooks: <pre><code>pip install -e '.[test, docs]'\npre-commit install\n</code></pre></li> <li>Add your code. Add your tests. Update the docs if needed. Party on!     New methods should list <code>build</code>, <code>state</code>, <code>init</code> and <code>update</code>     at the top of the module in order.</li> <li>Check any changes in the docs render nicely: <pre><code>mkdocs serve\n</code></pre> and navigate to <code>http://localhost:8000/</code> in your browser.<sup>1</sup></li> <li>Make sure to run the linter, tests and check coverage: <pre><code>pre-commit run --all-files\npython -m pytest --cov=posteriors --cov-report term-missing\n</code></pre></li> <li>Commit your changes and push your new branch to your fork.</li> <li>Open a pull request on GitHub.</li> </ol> <p>Note</p> <p>Feel free to open a draft PR to discuss changes or get feedback.</p> <ol> <li> <p>For more docs info check out mkdocs-material.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>Install from PyPI with <code>pip</code>:</p> <pre><code>pip install posteriors\n</code></pre>"},{"location":"getting_started/#why-uq","title":"Why UQ?","text":"<p>Uncertainty quantification allows for informed decision making by averaging over multiple plausible model configurations rather than relying on a single point estimate. Thus providing a coherent framework for detecting out of distribution inputs and continual learning.</p> <p>For more info on the utility of UQ, check out our blog post introducing <code>posteriors</code>!</p>"},{"location":"getting_started/#quick-start","title":"Quick Start","text":"<p><code>posteriors</code> is a Python library for uncertainty quantification and machine learning  that is designed to be easy to use, flexible and extensible. It is built on top  of PyTorch and provides a range of  tools for probabilistic modelling, Bayesian inference, and online learning.</p> <p>Enough smalltalk, let's train a simple Bayesian neural network using <code>posteriors</code>:</p> <p><pre><code>from torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torch import nn, utils, func\nimport torchopt\nimport posteriors\n\ndataset = MNIST(root=\"./data\", transform=ToTensor(), download=True)\ntrain_loader = utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\nnum_data = len(dataset)\n\nclassifier = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 10))\nparams = dict(classifier.named_parameters())\n\n\ndef log_posterior(params, batch):\n    images, labels = batch\n    images = images.view(images.size(0), -1)\n    output = func.functional_call(classifier, params, images)\n    log_post_val = (\n        -nn.functional.cross_entropy(output, labels)\n        + posteriors.diag_normal_log_prob(params) / num_data\n    )\n    return log_post_val, output\n\n\ntransform = posteriors.vi.diag.build(\n    log_posterior, torchopt.adam(), temperature=1 / num_data\n)  # Can swap out for any posteriors algorithm\n\nstate = transform.init(params)\n\nfor batch in train_loader:\n    state, aux = transform.update(state, batch)\n</code></pre> Here:</p> <ul> <li><code>build</code> is a function that loads <code>config_args</code> into the <code>init</code> and <code>update</code> functions  and stores them within the <code>transform</code> instance. The <code>init</code> and <code>update</code>   functions then conform to a preset signature allowing for easy switching between algorithms.</li> <li><code>state</code> is a <code>NamedTuple</code>     encoding the state of the algorithm, including <code>params</code> and <code>aux</code> attributes.</li> <li><code>init</code> constructs the iteration-varying <code>state</code> based on the model parameters <code>params</code>.</li> <li><code>update</code> updates the <code>state</code> based on a new <code>batch</code> of data and also returns    any auxiliary information from the model call.</li> </ul> <p>We've here used <code>posteriors.vi.diag</code> but we could easily swap to any of the other <code>posteriors</code> algorithms such as <code>posteriors.laplace.diag_fisher</code> or <code>posteriors.sgmcmc.sghmc</code></p> <p>I want more!</p> <p>The Visualizing VI and SGHMC tutorial provides a walkthrough for a simple example demonstrating how to use <code>posteriors</code> and easily switch between algorithms.</p> <p><code>posteriors</code> expects <code>log_posterior</code> to take a certain form, learn more in the  constructing log posteriors page.</p> <p>Our API documentation provides detailed descriptions for all of the <code>posteriors</code> algorithms and utilities.</p>"},{"location":"getting_started/#pytrees","title":"PyTrees","text":"<p>The internals of <code>posteriors</code> rely on <code>optree</code> to apply functions across arbitrary PyTrees of tensors (i.e. TensorTrees). For example: <pre><code>params_squared = optree.tree_map(lambda x: x**2, params)\n</code></pre> will square all the tensors in the <code>params</code>, where <code>params</code> can be a  <code>dict</code>, <code>list</code>, <code>tuple</code>, or any other PyTree.</p> <p><code>posteriors</code> also provides a <code>posteriors.flexi_tree_map</code> function that allows for in-place support: <pre><code>params_squared = optree.flexi_tree_map(lambda x: x**2, params, inplace=True)\n</code></pre> In this case, the tensors of params are modified in-place, without assigning extra memory.</p>"},{"location":"getting_started/#torchfunc","title":"<code>torch.func</code>","text":"<p>Instead of using <code>torch</code>'s more common <code>loss.backward()</code> style automatic differentiation, <code>posteriors</code> uses a functional approach, via <code>torch.func.grad</code> and friends. The functional  approach is easier to test, composes better with other tools and importantly for <code>posteriors</code>  it makes for code that is closer to the mathematical notation.</p> <p>For example, the gradient of a function <code>f</code> with respect to <code>x</code> can be computed as: <pre><code>grad_f_x = torch.func.grad(f)(x)\n</code></pre> where <code>f</code> is a function that takes <code>x</code> as input and returns a scalar output. Again,  <code>x</code> can be a <code>dict</code>, <code>list</code>, <code>tuple</code>, or any other PyTree with <code>torch.Tensor</code> leaves.</p>"},{"location":"getting_started/#friends","title":"Friends","text":"<p>Compose <code>posteriors</code> with wonderful tools from the <code>torch</code> ecosystem</p> <ul> <li>Define priors and likelihoods with <code>torch.distributions</code>.     Remember to set <code>validate_args=False</code>     and construct the log posterior accordingly.</li> <li><code>torchopt</code> for functional optimizers.</li> <li><code>transfomers</code> for open source models.</li> <li><code>lightning</code> for logging and device management.     Check out the <code>lightning</code> integration tutorial.</li> </ul> <p>Additionally, the functional transform interface used in <code>posteriors</code> is strongly inspired by frameworks such as <code>optax</code> and <code>blackjax</code>.</p> <p>As well as other UQ libraries <code>fortuna</code>, <code>laplace</code>, <code>numpyro</code>, <code>pymc</code> and <code>uncertainty-baselines</code>.</p>"},{"location":"gotchas/","title":"Gotchas","text":""},{"location":"gotchas/#torchno_grad","title":"<code>torch.no_grad</code>","text":"<p>If you find yourself running out of memory when using <code>torch.func.grad</code> and friends, it might be because <code>torch</code> is trying to accumulate gradients through your <code>torch.func.grad</code>calls. To prevent this, somewhat counterintuitively,  wrap your code in <code>torch.no_grad</code>:</p> <pre><code>with torch.no_grad():\n    grad_f_x = torch.func.grad(f)(params, batch)\n</code></pre> <p>Don't worry, <code>torch.no_grad</code> won't prevent the gradients being calculated correctly in the functional call. However, <code>torch.inference_mode</code> will turn autograd off altogether. More info in the torch.func docs.</p>"},{"location":"gotchas/#validate_argsfalse-in-torchdistributions","title":"<code>validate_args=False</code> in <code>torch.distributions</code>","text":"<p><code>posteriors</code> uses <code>torch.vmap</code> internally to vectorize over functions, for cool things like per-sample gradients. The <code>validate_args=True</code> control flows in <code>torch.distributions</code> do not compose with the  control flows in <code>torch.vmap</code>. So it is recommended to set <code>validate_args=False</code> when  using <code>torch.distributions</code> in <code>posteriors</code>:</p> <pre><code>import torch\nfrom torch.distributions import Normal\n\ntorch.vmap(lambda x: Normal(0., 1.).log_prob(x))(torch.arange(3))\n# RuntimeError: vmap: It looks like you're attempting to use a\n# Tensor in some data-dependent control flow. We don't support that yet, \n# please shout over at https://github.com/pytorch/functorch/issues/257 .\n\ntorch.vmap(lambda x: Normal(0., 1., validate_args=False).log_prob(x))(torch.arange(3))\n# tensor([-0.9189, -1.4189, -2.9189])\n</code></pre>"},{"location":"gotchas/#auxiliary-information","title":"Auxiliary information","text":"<p><code>posteriors</code> enforces <code>log_posterior</code> and <code>log_likelihood</code> functions to have a <code>log_posterior(params, batch) -&gt; log_prob, aux</code> signature, where the second element contains any auxiliary information. If you don't have any auxiliary information, just return an empty tensor:</p> <pre><code>def log_posterior(params, batch):\n    log_prob = ...\n    return log_prob, torch.tensor([])\n</code></pre> <p>More info in the constructing log posteriors page.</p>"},{"location":"gotchas/#inplace","title":"<code>inplace</code>","text":"<p>All <code>posteriors</code> algorithms have an <code>update</code> function with signature <code>update(state, batch, inplace=False) -&gt; state, aux</code><sup>1</sup>. The <code>inplace</code> argument can be set to <code>True</code> to update the <code>state</code> in-place and save memory. However, <code>posteriors</code> is functional first, so has <code>inplace=False</code> as the default. </p> <pre><code>state2, aux = transform.update(state, batch)\n# state is not updated\n\nstate2, aux = transform.update(state, batch, inplace=True)\n# state is updated and state2 is a pointer to state\n</code></pre> <p>When adding a new algorithm, in-place support can be achieved by modifying <code>TensorTree</code>s via the <code>flexi_tree_map</code> function:</p> <pre><code>from posteriors.tree_utils import flexi_tree_map\n\nnew_state = flexi_tree_map(lambda x: x + 1, state, inplace=True)\n</code></pre> <p>As <code>posteriors</code> transform states are immutable <code>NamedTuple</code>s, in-place modification of <code>TensorTree</code> leaves can be achieved by modifying the data of the tensor directly with <code>tree_insert_</code>:</p> <pre><code>from posteriors.tree_utils import tree_insert_\n\ntree_insert_(state.log_posterior, log_post.detach())\n</code></pre> <p>However, the <code>aux</code> component of the <code>TransformState</code> is not guaranteed to be a <code>TensorTree</code>, and so in-place modification of <code>aux</code> is not supported. Using <code>state._replace(aux=aux)</code> will return a state with all <code>TensorTree</code> pointing to the same memory as input <code>state</code>, but with a new <code>aux</code> component (<code>aux</code> is not modified in the input <code>state</code> object).</p>"},{"location":"gotchas/#torchtensor-with-autograd","title":"<code>torch.tensor</code> with autograd","text":"<p>As specified in the documentation, <code>torch.tensor</code> does not preserve autograd history. If you want to construct a tensor within a differentiable function, use <code>torch.stack</code> instead:</p> <pre><code>def f_with_tensor(x):\n    return torch.tensor([x**2, x**3]).sum()\n\ntorch.func.grad(f_with_tensor)(torch.tensor(2.))\n# tensor(0.)\n\ndef f_with_stack(x):\n    return torch.stack([x**2, x**3]).sum()\n\ntorch.func.grad(f_with_stack)(torch.tensor(2.))\n# tensor(16.)\n</code></pre> <ol> <li> <p>Assuming all other args and kwargs have been pre-configured with by the <code>build</code> function\u00a0\u21a9</p> </li> </ol>"},{"location":"log_posteriors/","title":"Constructing Log Posteriors","text":"<p>TL;DR</p> <ul> <li><code>posteriors</code> enforces <code>log_posterior</code> or <code>log_likelihood</code> functions to have a <code>log_posterior(params, batch) -&gt; log_prob, aux</code> signature, where the second element is a tensor valued <code>PyTree</code> containing any auxiliary information.</li> <li>Define your <code>log_posterior</code> or <code>log_likelihood</code> to be averaged across the batch.</li> <li>Set <code>temperature=1/num_data</code> for Bayesian methods such as  <code>posteriors.sgmcmc.sghmc</code> and <code>posteriors.vi.diag</code>.</li> <li>This ensures that hyperparameters such as learning rate are consistent across  batchsizes.</li> </ul>"},{"location":"log_posteriors/#auxiliary-information","title":"Auxiliary information","text":"<p>Model calls can be expensive, and they might provide more information than just an output value (and gradient). In order to avoid, losing this information <code>posteriors</code> enforces the <code>log_posterior</code> or <code>log_likelihood</code> functions to have a <code>log_posterior(params, batch) -&gt; log_prob, aux</code> signature, where the second element contains any auxiliary information, such as predictions or alternative metrics. Although note the auxiliary information should be a <code>TensorTree</code> (a requirement to work with <code>torch.func.grad</code> and friends).</p> <p>The <code>update</code> function of a <code>posteriors</code> transform will output a tuple of the new state and the auxiliary information output by the <code>log_posterior</code> or <code>log_likelihood</code> function.</p> <p>See the In code boxes below for example code snippets on how to construct a <code>log_posterior</code> function and use it with a <code>posteriors</code> transform.</p>"},{"location":"log_posteriors/#gradient-ascent","title":"Gradient Ascent","text":"<p>Normally in gradient descent we minimize a loss function such as cross-entropy or  mean squared error. This is equivalent to gradient ascent to maximise a log likelihood  function e.g. cross-entropy  loss corresponds to the log likelihood of a categorical  distribution:</p> \\[\\begin{aligned} \\log p(y_{1:N} \\mid x_{1:N}, \\theta) &amp;= \\sum_{i=1}^N \\log p(y_{i} \\mid x_i, \\theta) \\\\ p(y_{i} \\mid x_i, \\theta) &amp;= \\text{Categorical}(y_i \\mid f_\\theta(x_i)) \\\\ \\log p(y_{i} \\mid x_i, \\theta) &amp;=  \\sum_{k=1}^K \\mathbb{I}[y_i = k] [\\log f_\\theta(x_i)]_k \\end{aligned}\\] <p>Here \\(K\\) is the number of classes and \\(\\log f_\\theta(x_i)\\) is a vector (length  \\(K\\)) of  logits from the model (i.e. neural network) for input \\(x_i\\) and parameters \\(\\theta\\).</p> <p>In code</p> <p><pre><code>import torch.nn.functional as F\nmean_log_lik = - F.cross_entropy(logits, labels.squeeze(-1))\n</code></pre> or equivalently <pre><code>from torch.distributions import Categorical\nmean_log_lik = Categorical(logits=logits, validate_args=False).log_prob(labels).mean()\n</code></pre></p>"},{"location":"log_posteriors/#going-bayesian","title":"Going Bayesian","text":"<p>To do Bayesian inference on the parameters \\(\\theta\\) we look to approximate the posterior distribution</p> \\[\\begin{aligned} p(\\theta \\mid y_{1:N}, x_{1:N}) &amp;= \\frac{ p(\\theta) p(y_{1:N} \\mid x_{1:N}, \\theta) }{Z} =  \\frac{ p(\\theta) \\prod_{i=1}^N p(y_{i} \\mid x_{i}, \\theta) }{Z} \\\\ \\log p(\\theta \\mid y_{1:N}, x_{1:N}) &amp;= \\log p(\\theta) + \\sum_{i=1}^N \\log p(y_{i} \\mid x_{i}, \\theta) - \\log Z \\end{aligned}\\] <p>Here \\(p(\\theta)\\) is some prior of the parameter which we have to define, \\(Z\\) is a  normalizing constant which is independent of \\(\\theta\\) and therefore we can ignore it  (it disappears when we take the gradient).</p> <p>Again we have to take minibatches giving us the stochastic log posterior \\begin{aligned} \\log p(\\theta \\mid y_{1:n}, x_{1:n}) = \\log p(\\theta) +  \\frac{N}{n} \\sum_{i=1}^n \\log p(y_{i} \\mid x_i, \\theta) \\end{aligned}</p> <p>But the problem here is that the value of \\(\\log p(\\theta \\mid y_{1:n}, x_{1:n})\\)  will be very large in the realistic case when \\(N\\) is very large. Instead we should consider the averaged stochastic log posterior which remains on the same scale as either \\(N\\) or \\(n\\) increaase.</p> \\[\\begin{aligned} \\frac{1}{N} \\log p(\\theta \\mid y_{1:n}, x_{1:n}) = \\frac1N \\log p(\\theta) +  \\frac{1}{n} \\sum_{i=1}^n \\log p(y_{i} \\mid x_i, \\theta) \\end{aligned}\\] <p>In code</p> <pre><code>import posteriors, torch\nfrom optree import tree_map, tree_reduce\nfrom torch.distributions import Categorical\n\nmodel_function = posteriors.model_to_function(model)\n\ndef log_posterior(params, batch):\n    logits = model_function(params, **batch)\n    log_prior = diag_normal_log_prob(params, sd=1., normalize=False)\n    mean_log_lik = Categorical(logits=logits).log_prob(batch['labels']).mean()\n    mean_log_post = log_prior / num_data + mean_log_lik\n    return mean_log_post, torch.tensor([])\n</code></pre> <p>See auxiliary information for why we return an additional empty tensor.</p> <p>The issue with running Bayesian methods (such as VI or SGHMC) on this mean log posterior function is that naive application will result in approximating the tempered posterior</p> <p>\\begin{aligned} p(\\theta \\mid y_{1:N}, x_{1:N})^{\\frac1N} &amp;= \\frac{ p(\\theta)^{\\frac1N} p(y_{1:N} \\mid x_{1:N}, \\theta)^{\\frac1N} }{Z} \\end{aligned} (a tempered distribution is \\(q(x, T) := p(x)^{\\frac1T}/Z\\) for temperature \\(T\\)).</p> <p>This tempered posterior is much less concentrated than the true posterior  \\(p(\\theta \\mid y_{1:N}, x_{1:N})\\). To correct for this we can either supply our Bayesian inference algorithm with: <pre><code>temperature=1/num_data\n</code></pre> Note that with this support, optimization can often be obtained by simply setting <code>temperature=0</code>.</p> <p>Example</p> <pre><code>import torchopt\n# Define log_posterior as above\n# Load dataloader\nnum_data = len(dataloader.dataset)\n\nvi_transform = posteriors.vi.diag.build(\n    log_posterior=log_posterior,\n    optimizer = torchopt.adam(lr=1e-3),\n    temperature=1/num_data\n)\n\nvi_state = vi_transform.init(params)\n\nfor batch in dataloader:\n    vi_state, aux = vi_transform.update(vi_state, batch)\n</code></pre> <p>Alternatively, we can rescale the log posterior  <code>log_post = mean_log_post * num_data</code> but this may not scale well as <code>log_post</code> values become extremely large resulting in e.g. the need for an extremely small learning rate.</p>"},{"location":"log_posteriors/#prior-hyperparameters","title":"Prior Hyperparameters","text":"<p>Observe the mean log posterior function \\begin{aligned} \\frac{1}{N} \\log p(\\theta \\mid y_{1:n}, x_{1:n}) = \\frac1N \\log p(\\theta) +  \\frac{1}{n} \\sum_{i=1}^n \\log p(y_{i} \\mid x_i, \\theta) \\end{aligned}</p> <p>Typically the prior \\(p(\\theta)\\) will have some scale hyperparameter \\(\\sigma^2\\): $$ p(\\theta) = e^{\\frac{1}{\\sigma^2}\\gamma(\\theta)} / Z(\\sigma^2) $$ (such as a normal distribution). The mean log posterior becomes \\begin{aligned} \\frac{1}{N} \\log p(\\theta \\mid y_{1:n}, x_{1:n}) = \\frac{1}{N\\sigma^2} \\gamma(\\theta) +  \\frac{1}{n} \\sum_{i=1}^n \\log p(y_{i} \\mid x_i, \\theta) \\end{aligned} We are free to choose \\(\\sigma^2\\) and indeed it controls the strength of the prior vs the likelihood. In most cases we probably want the prior to be quite weak and therefore the  variance \\(\\sigma^2\\) quite large. As we can see if \\(\\sigma^2\\) is large then the prior  term becomes very small. We can ignore the normalising constant \\(Z(\\sigma^2)\\) because it does not depend on \\(\\theta\\), in fact this often recomended to keep the <code>log_posterior</code>  values on a nice scale comparable to loss functions we are accustomed to, this can be  achieved for a normal prior with <code>posteriors.diag_normal_log_prob(x, normalize=False)</code>.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#extended-kalman-filter-ekf","title":"Extended Kalman filter (EKF)","text":"<ul> <li><code>ekf.dense_fisher</code> applies an online Bayesian update based  on a Taylor approximation of the log-likelihood. Uses the empirical Fisher information matrix as a positive-definite alternative to the Hessian. Natural gradient descent equivalence following Ollivier, 2019.</li> <li><code>ekf.diag_fisher</code> same as <code>ekf.dense_fisher</code> but uses the diagonal of the empirical Fisher information matrix instead.</li> </ul>"},{"location":"api/#laplace-approximation","title":"Laplace approximation","text":"<ul> <li><code>laplace.dense_fisher</code> calculates the empirical Fisher information matrix and uses it to approximate the posterior precision, i.e. a Laplace approximation.</li> <li><code>laplace.dense_ggn</code> calculates the Generalised Gauss-Newton matrix which is equivalent to the non-empirical Fisher in most neural network settings - see Martens, 2020.</li> <li><code>laplace.dense_hessian</code> calculates the Hessian of the negative log posterior.</li> <li><code>laplace.diag_fisher</code> same as <code>laplace.dense_fisher</code> but uses the diagonal of the empirical Fisher information matrix instead.</li> <li><code>laplace.diag_ggn</code> same as <code>laplace.dense_ggn</code> but uses the diagonal of the Generalised Gauss-Newton matrix instead.</li> </ul> <p>All Laplace transforms leave the parameters unmodified. Comprehensive details on Laplace approximations can be found in Daxberger et al, 2021.</p>"},{"location":"api/#stochastic-gradient-markov-chain-monte-carlo-sgmcmc","title":"Stochastic gradient Markov chain Monte Carlo (SGMCMC)","text":"<ul> <li><code>sgmcmc.sgld</code> implements stochastic gradient Langevin dynamics (SGLD) from Welling and Teh, 2011.</li> <li><code>sgmcmc.sghmc</code> implements the stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm from Chen et al, 2014 (without momenta resampling).</li> <li><code>sgmcmc.sgnht</code> implements the stochastic gradient Nos\u00e9-Hoover thermostat (SGNHT) algorithm from Ding et al, 2014, (SGHMC with adaptive friction coefficient).</li> <li><code>sgmcmc.baoa</code> implements the BAOA integrator for SGHMC from Leimkuhler and Matthews, 2015 - p271.</li> </ul> <p>For an overview and unifying framework for SGMCMC methods, see Ma et al, 2015.</p>"},{"location":"api/#variational-inference-vi","title":"Variational inference (VI)","text":"<ul> <li><code>vi.dense</code> implements a Gaussian variational distribution. Expects a <code>torchopt</code> optimizer for handling the minimization of the NELBO. Also find <code>vi.dense.nelbo</code> for simply calculating the NELBO  with respect to a <code>log_posterior</code> and Gaussian distribution.</li> <li><code>vi.diag</code> same as <code>vi.dense</code> but uses the diagonal of the Gaussian variational distribution.</li> </ul> <p>A review of variational inference can be found in Blei et al, 2017.</p>"},{"location":"api/#optim","title":"Optim","text":"<ul> <li><code>optim</code> wrapper for <code>torch.optim</code> optimizers within the unified <code>posteriors</code>  API that allows for easy swapping with UQ methods.</li> </ul>"},{"location":"api/#torchopt","title":"TorchOpt","text":"<ul> <li><code>torchopt</code> wrapper for <code>torchopt</code> optimizers within the unified <code>posteriors</code> API that allows for easy swapping with UQ methods.</li> </ul>"},{"location":"api/optim/","title":"Optim","text":""},{"location":"api/optim/#posteriors.optim.build","title":"<code>posteriors.optim.build(loss_fn, optimizer, **kwargs)</code>","text":"<p>Builds an optimizer transform from torch.optim</p> <pre><code>transform = build(loss_fn, torch.optim.Adam, lr=0.1)\nstate = transform.init(params)\n\nfor batch in dataloader:\n    state, aux = transform.update(state, batch)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>LogProbFn</code> <p>Function that takes the parameters and returns the loss. of the form <code>loss, aux = fn(params, batch)</code>.</p> required <code>optimizer</code> <code>Type[Optimizer]</code> <p>Optimizer class from torch.optim.</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the optimizer class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Transform</code> <p><code>torch.optim</code> transform instance.</p> Source code in <code>posteriors/optim.py</code> <pre><code>def build(\n    loss_fn: LogProbFn,\n    optimizer: Type[torch.optim.Optimizer],\n    **kwargs: Any,\n) -&gt; Transform:\n    \"\"\"Builds an optimizer transform from [torch.optim](https://pytorch.org/docs/stable/optim.html)\n\n    ```\n    transform = build(loss_fn, torch.optim.Adam, lr=0.1)\n    state = transform.init(params)\n\n    for batch in dataloader:\n        state, aux = transform.update(state, batch)\n    ```\n\n    Args:\n        loss_fn: Function that takes the parameters and returns the loss.\n            of the form `loss, aux = fn(params, batch)`.\n        optimizer: Optimizer class from torch.optim.\n        **kwargs: Keyword arguments to pass to the optimizer class.\n\n    Returns:\n        `torch.optim` transform instance.\n    \"\"\"\n    init_fn = partial(init, optimizer_cls=optimizer, **kwargs)\n    update_fn = partial(update, loss_fn=loss_fn)\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/optim/#posteriors.optim.OptimState","title":"<code>posteriors.optim.OptimState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State of an optimizer from torch.optim.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Parameters to be optimized.</p> <code>optimizer</code> <code>Optimizer</code> <p>torch.optim optimizer instance.</p> <code>loss</code> <code>Tensor</code> <p>Loss value.</p> Source code in <code>posteriors/optim.py</code> <pre><code>class OptimState(TensorClass[\"frozen\"]):\n    \"\"\"State of an optimizer from [torch.optim](https://pytorch.org/docs/stable/optim.html).\n\n    Attributes:\n        params: Parameters to be optimized.\n        optimizer: torch.optim optimizer instance.\n        loss: Loss value.\n    \"\"\"\n\n    params: TensorTree\n    optimizer: torch.optim.Optimizer\n    loss: torch.Tensor = torch.tensor([])\n</code></pre>"},{"location":"api/optim/#posteriors.optim.init","title":"<code>posteriors.optim.init(params, optimizer_cls, *args, **kwargs)</code>","text":"<p>Initialise a torch.optim optimizer state.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Parameters to be optimized.</p> required <code>optimizer_cls</code> <code>Type[Optimizer]</code> <p>Optimizer class from torch.optim.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the optimizer class.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the optimizer class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OptimState</code> <p>Initial OptimState.</p> Source code in <code>posteriors/optim.py</code> <pre><code>def init(\n    params: TensorTree,\n    optimizer_cls: Type[torch.optim.Optimizer],\n    *args: Any,\n    **kwargs: Any,\n) -&gt; OptimState:\n    \"\"\"Initialise a [torch.optim](https://pytorch.org/docs/stable/optim.html) optimizer\n    state.\n\n    Args:\n        params: Parameters to be optimized.\n        optimizer_cls: Optimizer class from torch.optim.\n        *args: Positional arguments to pass to the optimizer class.\n        **kwargs: Keyword arguments to pass to the optimizer class.\n\n    Returns:\n        Initial OptimState.\n    \"\"\"\n    optimizer = optimizer_cls(tree_leaves(params), *args, **kwargs)\n    return OptimState(params, optimizer)\n</code></pre>"},{"location":"api/optim/#posteriors.optim.update","title":"<code>posteriors.optim.update(state, batch, loss_fn, inplace=True)</code>","text":"<p>Perform a single update step of a torch.optim optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>OptimState</code> <p>Current optimizer state.</p> required <code>batch</code> <code>TensorTree</code> <p>Input data to loss_fn.</p> required <code>loss_fn</code> <code>LogProbFn</code> <p>Function that takes the parameters and returns the loss. of the form <code>loss, aux = fn(params, batch)</code>.</p> required <code>inplace</code> <code>bool</code> <p>Whether to update the parameters in place. inplace=False not supported for posteriors.optim</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[OptimState, TensorTree]</code> <p>Updated OptimState and auxiliary information.</p> Source code in <code>posteriors/optim.py</code> <pre><code>def update(\n    state: OptimState,\n    batch: TensorTree,\n    loss_fn: LogProbFn,\n    inplace: bool = True,\n) -&gt; tuple[OptimState, TensorTree]:\n    \"\"\"Perform a single update step of a [torch.optim](https://pytorch.org/docs/stable/optim.html)\n    optimizer.\n\n    Args:\n        state: Current optimizer state.\n        batch: Input data to loss_fn.\n        loss_fn: Function that takes the parameters and returns the loss.\n            of the form `loss, aux = fn(params, batch)`.\n        inplace: Whether to update the parameters in place.\n            inplace=False not supported for posteriors.optim\n\n    Returns:\n        Updated OptimState and auxiliary information.\n    \"\"\"\n    if not inplace:\n        raise NotImplementedError(\"inplace=False not supported for posteriors.optim\")\n    state.optimizer.zero_grad()\n    with CatchAuxError():\n        loss, aux = loss_fn(state.params, batch)\n    loss.backward()\n    state.optimizer.step()\n    tree_insert_(state.loss, loss.detach())\n    return state, aux\n</code></pre>"},{"location":"api/torchopt/","title":"TorchOpt","text":""},{"location":"api/torchopt/#posteriors.torchopt.build","title":"<code>posteriors.torchopt.build(loss_fn, optimizer)</code>","text":"<p>Build a TorchOpt optimizer transformation.</p> <p>Make sure to use the lower case functional optimizers e.g. <code>torchopt.adam()</code>.</p> <pre><code>transform = build(loss_fn, torchopt.adam(lr=0.1))\nstate = transform.init(params)\n\nfor batch in dataloader:\n    state, aux = transform.update(state, batch)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>LogProbFn</code> <p>Loss function.</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer. Make sure to use lower case e.g. torchopt.adam()</p> required <p>Returns:</p> Type Description <code>Transform</code> <p>Torchopt optimizer transform instance.</p> Source code in <code>posteriors/torchopt.py</code> <pre><code>def build(\n    loss_fn: LogProbFn,\n    optimizer: torchopt.base.GradientTransformation,\n) -&gt; Transform:\n    \"\"\"Build a [TorchOpt](https://github.com/metaopt/torchopt) optimizer transformation.\n\n    Make sure to use the lower case functional optimizers e.g. `torchopt.adam()`.\n\n    ```\n    transform = build(loss_fn, torchopt.adam(lr=0.1))\n    state = transform.init(params)\n\n    for batch in dataloader:\n        state, aux = transform.update(state, batch)\n    ```\n\n    Args:\n        loss_fn: Loss function.\n        optimizer: TorchOpt functional optimizer.\n            Make sure to use lower case e.g. torchopt.adam()\n\n    Returns:\n        Torchopt optimizer transform instance.\n    \"\"\"\n    init_fn = partial(init, optimizer=optimizer)\n    update_fn = partial(update, optimizer=optimizer, loss_fn=loss_fn)\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/torchopt/#posteriors.torchopt.TorchOptState","title":"<code>posteriors.torchopt.TorchOptState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State of a TorchOpt optimizer.</p> <p>Contains the parameters, the optimizer state for the TorchOpt optimizer, loss value, and auxiliary information.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Parameters to be optimized.</p> <code>opt_state</code> <code>OptState</code> <p>TorchOpt optimizer state.</p> <code>loss</code> <code>Tensor</code> <p>Loss value.</p> Source code in <code>posteriors/torchopt.py</code> <pre><code>class TorchOptState(TensorClass[\"frozen\"]):\n    \"\"\"State of a [TorchOpt](https://github.com/metaopt/torchopt) optimizer.\n\n    Contains the parameters, the optimizer state for the TorchOpt optimizer,\n    loss value, and auxiliary information.\n\n    Attributes:\n        params: Parameters to be optimized.\n        opt_state: TorchOpt optimizer state.\n        loss: Loss value.\n    \"\"\"\n\n    params: TensorTree\n    opt_state: torchopt.typing.OptState\n    loss: torch.Tensor = torch.tensor([])\n</code></pre>"},{"location":"api/torchopt/#posteriors.torchopt.init","title":"<code>posteriors.torchopt.init(params, optimizer)</code>","text":"<p>Initialise a TorchOpt optimizer.</p> <p>Make sure to use the lower case functional optimizers e.g. <code>torchopt.adam()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Parameters to be optimized.</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer. Make sure to use lower case e.g. torchopt.adam()</p> required <p>Returns:</p> Type Description <code>TorchOptState</code> <p>Initial TorchOptState.</p> Source code in <code>posteriors/torchopt.py</code> <pre><code>def init(\n    params: TensorTree,\n    optimizer: torchopt.base.GradientTransformation,\n) -&gt; TorchOptState:\n    \"\"\"Initialise a [TorchOpt](https://github.com/metaopt/torchopt) optimizer.\n\n    Make sure to use the lower case functional optimizers e.g. `torchopt.adam()`.\n\n    Args:\n        params: Parameters to be optimized.\n        optimizer: TorchOpt functional optimizer.\n            Make sure to use lower case e.g. torchopt.adam()\n\n    Returns:\n        Initial TorchOptState.\n    \"\"\"\n    opt_state = optimizer.init(params)\n    return TorchOptState(params, opt_state)\n</code></pre>"},{"location":"api/torchopt/#posteriors.torchopt.update","title":"<code>posteriors.torchopt.update(state, batch, loss_fn, optimizer, inplace=False)</code>","text":"<p>Update the TorchOpt optimizer state.</p> <p>Make sure to use the lower case functional optimizers e.g. <code>torchopt.adam()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TorchOptState</code> <p>Current state.</p> required <code>batch</code> <code>TensorTree</code> <p>Batch of data.</p> required <code>loss_fn</code> <code>LogProbFn</code> <p>Loss function.</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer. Make sure to use lower case like torchopt.adam()</p> required <code>inplace</code> <code>bool</code> <p>Whether to update the state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[TorchOptState, TensorTree]</code> <p>Updated TorchOptState and auxiliary information.</p> Source code in <code>posteriors/torchopt.py</code> <pre><code>def update(\n    state: TorchOptState,\n    batch: TensorTree,\n    loss_fn: LogProbFn,\n    optimizer: torchopt.base.GradientTransformation,\n    inplace: bool = False,\n) -&gt; tuple[TorchOptState, TensorTree]:\n    \"\"\"Update the [TorchOpt](https://github.com/metaopt/torchopt) optimizer state.\n\n    Make sure to use the lower case functional optimizers e.g. `torchopt.adam()`.\n\n    Args:\n        state: Current state.\n        batch: Batch of data.\n        loss_fn: Loss function.\n        optimizer: TorchOpt functional optimizer.\n            Make sure to use lower case like torchopt.adam()\n        inplace: Whether to update the state in place.\n\n    Returns:\n        Updated TorchOptState and auxiliary information.\n    \"\"\"\n    params = state.params\n    opt_state = state.opt_state\n    with torch.no_grad(), CatchAuxError():\n        grads, (loss, aux) = torch.func.grad_and_value(loss_fn, has_aux=True)(\n            params, batch\n        )\n    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n    params = torchopt.apply_updates(params, updates, inplace=inplace)\n    if inplace:\n        tree_insert_(state.loss, loss.detach())\n        return state, aux\n\n    return TorchOptState(params, opt_state, loss), aux\n</code></pre>"},{"location":"api/tree_utils/","title":"Tree Utils","text":""},{"location":"api/tree_utils/#posteriors.tree_utils.tree_size","title":"<code>posteriors.tree_utils.tree_size(tree)</code>","text":"<p>Returns the total number of elements in a PyTree. Not the number of leaves, but the total number of elements for all tensors in the tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>TensorTree</code> <p>A PyTree of tensors.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of elements in the PyTree.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def tree_size(tree: TensorTree) -&gt; int:\n    \"\"\"Returns the total number of elements in a PyTree.\n    Not the number of leaves, but the total number of elements for all tensors in the\n    tree.\n\n    Args:\n        tree: A PyTree of tensors.\n\n    Returns:\n        Number of elements in the PyTree.\n    \"\"\"\n\n    def ensure_tensor(x):\n        return x if isinstance(x, torch.Tensor) else torch.tensor(x)\n\n    return tree_reduce(torch.add, tree_map(lambda x: ensure_tensor(x).numel(), tree))\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.tree_extract","title":"<code>posteriors.tree_utils.tree_extract(tree, f)</code>","text":"<p>Extracts values from a PyTree where f returns True. False values are replaced with empty tensors.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>TensorTree</code> <p>A PyTree.</p> required <code>f</code> <code>Callable[[tensor], bool]</code> <p>A function that takes a PyTree element and returns True or False.</p> required <p>Returns:</p> Type Description <code>TensorTree</code> <p>A PyTree with the same structure as tree where f returns True.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def tree_extract(\n    tree: TensorTree,\n    f: Callable[[torch.tensor], bool],\n) -&gt; TensorTree:\n    \"\"\"Extracts values from a PyTree where f returns True.\n    False values are replaced with empty tensors.\n\n    Args:\n        tree: A PyTree.\n        f: A function that takes a PyTree element and returns True or False.\n\n    Returns:\n        A PyTree with the same structure as tree where f returns True.\n    \"\"\"\n    return tree_map(lambda x: x if f(x) else torch.tensor([], device=x.device), tree)\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.tree_insert","title":"<code>posteriors.tree_utils.tree_insert(full_tree, sub_tree, f=lambda _: True)</code>","text":"<p>Inserts sub_tree into full_tree where full_tree tensors evaluate f to True. Both PyTrees must have the same structure.</p> <p>Parameters:</p> Name Type Description Default <code>full_tree</code> <code>TensorTree</code> <p>A PyTree to insert sub_tree into.</p> required <code>sub_tree</code> <code>TensorTree</code> <p>A PyTree to insert into full_tree.</p> required <code>f</code> <code>Callable[[tensor], bool]</code> <p>A function that takes a PyTree element and returns True or False. Defaults to lambda _: True. I.e. insert on all leaves.</p> <code>lambda _: True</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>A PyTree with sub_tree inserted into full_tree.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def tree_insert(\n    full_tree: TensorTree,\n    sub_tree: TensorTree,\n    f: Callable[[torch.tensor], bool] = lambda _: True,\n) -&gt; TensorTree:\n    \"\"\"Inserts sub_tree into full_tree where full_tree tensors evaluate f to True.\n    Both PyTrees must have the same structure.\n\n    Args:\n        full_tree: A PyTree to insert sub_tree into.\n        sub_tree: A PyTree to insert into full_tree.\n        f: A function that takes a PyTree element and returns True or False.\n            Defaults to lambda _: True. I.e. insert on all leaves.\n\n    Returns:\n        A PyTree with sub_tree inserted into full_tree.\n    \"\"\"\n    return tree_map(\n        lambda sub, full: sub if f(full) else full,\n        sub_tree,\n        full_tree,\n    )\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.tree_insert_","title":"<code>posteriors.tree_utils.tree_insert_(full_tree, sub_tree, f=lambda _: True)</code>","text":"<p>Inserts sub_tree into full_tree in-place where full_tree tensors evaluate f to True. Both PyTrees must have the same structure.</p> <p>Parameters:</p> Name Type Description Default <code>full_tree</code> <code>TensorTree</code> <p>A PyTree to insert sub_tree into.</p> required <code>sub_tree</code> <code>TensorTree</code> <p>A PyTree to insert into full_tree.</p> required <code>f</code> <code>Callable[[tensor], bool]</code> <p>A function that takes a PyTree element and returns True or False. Defaults to lambda _: True. I.e. insert on all leaves.</p> <code>lambda _: True</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>A pointer to full_tree with sub_tree inserted.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def tree_insert_(\n    full_tree: TensorTree,\n    sub_tree: TensorTree,\n    f: Callable[[torch.tensor], bool] = lambda _: True,\n) -&gt; TensorTree:\n    \"\"\"Inserts sub_tree into full_tree in-place where full_tree tensors evaluate\n    f to True. Both PyTrees must have the same structure.\n\n    Args:\n        full_tree: A PyTree to insert sub_tree into.\n        sub_tree: A PyTree to insert into full_tree.\n        f: A function that takes a PyTree element and returns True or False.\n            Defaults to lambda _: True. I.e. insert on all leaves.\n\n    Returns:\n        A pointer to full_tree with sub_tree inserted.\n    \"\"\"\n\n    def insert_(full, sub):\n        if f(full):\n            full.data = sub.data\n\n    return tree_map_(insert_, full_tree, sub_tree)\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.extract_requires_grad","title":"<code>posteriors.tree_utils.extract_requires_grad(tree)</code>","text":"<p>Extracts only parameters that require gradients.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>TensorTree</code> <p>A PyTree of tensors.</p> required <p>Returns:</p> Type Description <code>TensorTree</code> <p>A PyTree of tensors that require gradients.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def extract_requires_grad(tree: TensorTree) -&gt; TensorTree:\n    \"\"\"Extracts only parameters that require gradients.\n\n    Args:\n        tree: A PyTree of tensors.\n\n    Returns:\n        A PyTree of tensors that require gradients.\n    \"\"\"\n    return tree_extract(tree, lambda x: x.requires_grad)\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.insert_requires_grad","title":"<code>posteriors.tree_utils.insert_requires_grad(full_tree, sub_tree)</code>","text":"<p>Inserts sub_tree into full_tree where full_tree tensors requires_grad. Both PyTrees must have the same structure.</p> <p>Parameters:</p> Name Type Description Default <code>full_tree</code> <code>TensorTree</code> <p>A PyTree to insert sub_tree into.</p> required <code>sub_tree</code> <code>TensorTree</code> <p>A PyTree to insert into full_tree.</p> required <p>Returns:</p> Type Description <code>TensorTree</code> <p>A PyTree with sub_tree inserted into full_tree.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def insert_requires_grad(full_tree: TensorTree, sub_tree: TensorTree) -&gt; TensorTree:\n    \"\"\"Inserts sub_tree into full_tree where full_tree tensors requires_grad.\n    Both PyTrees must have the same structure.\n\n    Args:\n        full_tree: A PyTree to insert sub_tree into.\n        sub_tree: A PyTree to insert into full_tree.\n\n    Returns:\n        A PyTree with sub_tree inserted into full_tree.\n    \"\"\"\n    return tree_insert(full_tree, sub_tree, lambda x: x.requires_grad)\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.insert_requires_grad_","title":"<code>posteriors.tree_utils.insert_requires_grad_(full_tree, sub_tree)</code>","text":"<p>Inserts sub_tree into full_tree in-place where full_tree tensors requires_grad. Both PyTrees must have the same structure.</p> <p>Parameters:</p> Name Type Description Default <code>full_tree</code> <code>TensorTree</code> <p>A PyTree to insert sub_tree into.</p> required <code>sub_tree</code> <code>TensorTree</code> <p>A PyTree to insert into full_tree.</p> required <p>Returns:</p> Type Description <code>TensorTree</code> <p>A pointer to full_tree with sub_tree inserted.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def insert_requires_grad_(full_tree: TensorTree, sub_tree: TensorTree) -&gt; TensorTree:\n    \"\"\"Inserts sub_tree into full_tree in-place where full_tree tensors requires_grad.\n    Both PyTrees must have the same structure.\n\n    Args:\n        full_tree: A PyTree to insert sub_tree into.\n        sub_tree: A PyTree to insert into full_tree.\n\n    Returns:\n        A pointer to full_tree with sub_tree inserted.\n    \"\"\"\n    return tree_insert_(full_tree, sub_tree, lambda x: x.requires_grad)\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.extract_requires_grad_and_func","title":"<code>posteriors.tree_utils.extract_requires_grad_and_func(tree, func, inplace=False)</code>","text":"<p>Extracts only parameters that require gradients and converts a function that takes the full parameter tree (in its first argument) into one that takes the subtree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>TensorTree</code> <p>A PyTree of tensors.</p> required <code>func</code> <code>Callable</code> <p>A function that takes tree in its first argument.</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the tree inplace or not whe the new function is called.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[TensorTree, Callable]</code> <p>A PyTree of tensors that require gradients and a modified func that takes the subtree structure rather than full tree in its first argument.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def extract_requires_grad_and_func(\n    tree: TensorTree, func: Callable, inplace: bool = False\n) -&gt; Tuple[TensorTree, Callable]:\n    \"\"\"Extracts only parameters that require gradients and converts a function\n    that takes the full parameter tree (in its first argument)\n    into one that takes the subtree.\n\n    Args:\n        tree: A PyTree of tensors.\n        func: A function that takes tree in its first argument.\n        inplace: Whether to modify the tree inplace or not whe the new function\n            is called.\n\n    Returns:\n        A PyTree of tensors that require gradients and a modified func that takes the\n            subtree structure rather than full tree in its first argument.\n    \"\"\"\n    subtree = extract_requires_grad(tree)\n\n    insert = insert_requires_grad_ if inplace else insert_requires_grad\n\n    def subfunc(subtree, *args, **kwargs):\n        return func(insert(tree, subtree), *args, **kwargs)\n\n    return subtree, subfunc\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.inplacify","title":"<code>posteriors.tree_utils.inplacify(func)</code>","text":"<p>Converts a function that takes a tensor as its first argument into one that takes the same arguments but modifies the first argument tensor in-place with the output of the function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function that takes a tensor as its first argument and a returns a modified version of said tensor.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function that takes a tensor as its first argument and modifies it in-place.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def inplacify(func: Callable) -&gt; Callable:\n    \"\"\"Converts a function that takes a tensor as its first argument\n    into one that takes the same arguments but modifies the first argument\n    tensor in-place with the output of the function.\n\n    Args:\n        func: A function that takes a tensor as its first argument and a returns\n            a modified version of said tensor.\n\n    Returns:\n        A function that takes a tensor as its first argument and modifies it\n            in-place.\n    \"\"\"\n\n    def func_(tens, *args, **kwargs):\n        tens.data = func(tens, *args, **kwargs)\n        return tens\n\n    return func_\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.tree_map_inplacify_","title":"<code>posteriors.tree_utils.tree_map_inplacify_(func, tree, *rests, is_leaf=None, none_is_leaf=False, namespace='')</code>","text":"<p>Applies a pure function to each tensor in a PyTree in-place.</p> <p>Like optree.tree_map_ but takes a pure function as input (and takes replaces its first argument with its output in-place) rather than a side-effect function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function that takes a tensor as its first argument and a returns a modified version of said tensor.</p> required <code>tree</code> <code>pytree</code> <p>A pytree to be mapped over, with each leaf providing the first positional argument to function <code>func</code>.</p> required <code>rests</code> <code>tuple of pytree</code> <p>A tuple of pytrees, each of which has the same structure as <code>tree</code> or has <code>tree</code> as a prefix.</p> <code>()</code> <code>is_leaf</code> <code>callable</code> <p>An optionally specified function that will be called at each flattening step. It should return a boolean, with <code>True</code> stopping the traversal and the whole subtree being treated as a leaf, and <code>False</code> indicating the flattening should traverse the current object.</p> <code>None</code> <code>none_is_leaf</code> <code>bool</code> <p>Whether to treat <code>None</code> as a leaf. If <code>False</code>, <code>None</code> is a non-leaf node with arity 0. Thus <code>None</code> is contained in the treespec rather than in the leaves list and <code>None</code> will be remain in the result pytree. (default: <code>False</code>)</p> <code>False</code> <code>namespace</code> <code>str</code> <p>The registry namespace used for custom pytree node types. (default: :const:<code>''</code>, i.e., the global namespace)</p> <code>''</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>The original <code>tree</code> with the value at each leaf is given by the side-effect of function <code>func(x, *xs)</code> (not the return value) where <code>x</code> is the value at the corresponding leaf in <code>tree</code> and <code>xs</code> is the tuple of values at values at corresponding nodes in <code>rests</code>.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def tree_map_inplacify_(\n    func: Callable,\n    tree: TensorTree,\n    *rests: TensorTree,\n    is_leaf: Callable[[TensorTree], bool] | None = None,\n    none_is_leaf: bool = False,\n    namespace: str = \"\",\n) -&gt; TensorTree:\n    \"\"\"Applies a pure function to each tensor in a PyTree in-place.\n\n    Like [optree.tree_map_](https://optree.readthedocs.io/en/latest/ops.html#optree.tree_map_)\n    but takes a pure function as input (and takes replaces its first argument with its\n    output in-place) rather than a side-effect function.\n\n    Args:\n        func: A function that takes a tensor as its first argument and a returns\n            a modified version of said tensor.\n        tree (pytree): A pytree to be mapped over, with each leaf providing the first\n            positional argument to function ``func``.\n        rests (tuple of pytree): A tuple of pytrees, each of which has the same\n            structure as ``tree`` or has ``tree`` as a prefix.\n        is_leaf (callable, optional): An optionally specified function that will be\n            called at each flattening step. It should return a boolean, with\n            `True` stopping the traversal and the whole subtree being treated as a\n            leaf, and `False` indicating the flattening should traverse the\n            current object.\n        none_is_leaf (bool, optional): Whether to treat `None` as a leaf. If\n            `False`, `None` is a non-leaf node with arity 0. Thus `None` is contained in\n            the treespec rather than in the leaves list and `None` will be remain in the\n            result pytree. (default: `False`)\n        namespace (str, optional): The registry namespace used for custom pytree node\n            types. (default: :const:`''`, i.e., the global namespace)\n\n    Returns:\n        The original ``tree`` with the value at each leaf is given by the side-effect of\n            function ``func(x, *xs)`` (not the return value) where ``x`` is the value at\n            the corresponding leaf in ``tree`` and ``xs`` is the tuple of values at\n            values at corresponding nodes in ``rests``.\n    \"\"\"\n    return tree_map_(\n        inplacify(func),\n        tree,\n        *rests,\n        is_leaf=is_leaf,\n        none_is_leaf=none_is_leaf,\n        namespace=namespace,\n    )\n</code></pre>"},{"location":"api/tree_utils/#posteriors.tree_utils.flexi_tree_map","title":"<code>posteriors.tree_utils.flexi_tree_map(func, tree, *rests, inplace=False, is_leaf=None, none_is_leaf=False, namespace='')</code>","text":"<p>Applies a pure function to each tensor in a PyTree, with inplace argument.</p> <pre><code>out_tensor = func(tensor, *rest_tensors)\n</code></pre> <p>where <code>out_tensor</code> is of the same shape as <code>tensor</code>. Therefore</p> <pre><code>out_tree = func(tree, *rests, inplace=True)\n</code></pre> <p>will return <code>out_tree</code> a pointer to the original <code>tree</code> with leaves (tensors) modified in place. If <code>inplace=False</code>, <code>flexi_tree_map</code> is equivalent to <code>optree.tree_map</code> and returns a new tree.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A pure function that takes a tensor as its first argument and a returns a modified version of said tensor.</p> required <code>tree</code> <code>pytree</code> <p>A pytree to be mapped over, with each leaf providing the first positional argument to function <code>func</code>.</p> required <code>rests</code> <code>tuple of pytree</code> <p>A tuple of pytrees, each of which has the same structure as <code>tree</code> or has <code>tree</code> as a prefix.</p> <code>()</code> <code>inplace</code> <code>bool</code> <p>Whether to modify the tree in-place or not.</p> <code>False</code> <code>is_leaf</code> <code>callable</code> <p>An optionally specified function that will be called at each flattening step. It should return a boolean, with <code>True</code> stopping the traversal and the whole subtree being treated as a leaf, and <code>False</code> indicating the flattening should traverse the current object.</p> <code>None</code> <code>none_is_leaf</code> <code>bool</code> <p>Whether to treat <code>None</code> as a leaf. If <code>False</code>, <code>None</code> is a non-leaf node with arity 0. Thus <code>None</code> is contained in the treespec rather than in the leaves list and <code>None</code> will be remain in the result pytree. (default: <code>False</code>)</p> <code>False</code> <code>namespace</code> <code>str</code> <p>The registry namespace used for custom pytree node types. (default: :const:<code>''</code>, i.e., the global namespace)</p> <code>''</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Either the original tree modified in-place or a new tree depending on the <code>inplace</code> argument.</p> Source code in <code>posteriors/tree_utils.py</code> <pre><code>def flexi_tree_map(\n    func: Callable,\n    tree: TensorTree,\n    *rests: TensorTree,\n    inplace: bool = False,\n    is_leaf: Callable[[TensorTree], bool] | None = None,\n    none_is_leaf: bool = False,\n    namespace: str = \"\",\n) -&gt; TensorTree:\n    \"\"\"Applies a pure function to each tensor in a PyTree, with inplace argument.\n\n    ```\n    out_tensor = func(tensor, *rest_tensors)\n    ```\n\n    where `out_tensor` is of the same shape as `tensor`.\n    Therefore\n\n    ```\n    out_tree = func(tree, *rests, inplace=True)\n    ```\n\n    will return `out_tree` a pointer to the original `tree` with leaves (tensors)\n    modified in place.\n    If `inplace=False`, `flexi_tree_map` is equivalent to [`optree.tree_map`](https://optree.readthedocs.io/en/latest/ops.html#optree.tree_map)\n    and returns a new tree.\n\n    Args:\n        func: A pure function that takes a tensor as its first argument and a returns\n            a modified version of said tensor.\n        tree (pytree): A pytree to be mapped over, with each leaf providing the first\n            positional argument to function ``func``.\n        rests (tuple of pytree): A tuple of pytrees, each of which has the same\n            structure as ``tree`` or has ``tree`` as a prefix.\n        inplace (bool, optional): Whether to modify the tree in-place or not.\n        is_leaf (callable, optional): An optionally specified function that will be\n            called at each flattening step. It should return a boolean, with `True`\n            stopping the traversal and the whole subtree being treated as a leaf, and\n            `False` indicating the flattening should traverse the current object.\n        none_is_leaf (bool, optional): Whether to treat `None` as a leaf. If `False`,\n            `None` is a non-leaf node with arity 0. Thus `None` is contained in the\n            treespec rather than in the leaves list and `None` will be remain in the\n            result pytree. (default: `False`)\n        namespace (str, optional): The registry namespace used for custom pytree node\n            types. (default: :const:`''`, i.e., the global namespace)\n\n    Returns:\n        Either the original tree modified in-place or a new tree depending on the\n            `inplace` argument.\n    \"\"\"\n    tm = tree_map_inplacify_ if inplace else tree_map\n    return tm(\n        func,\n        tree,\n        *rests,\n        is_leaf=is_leaf,\n        none_is_leaf=none_is_leaf,\n        namespace=namespace,\n    )\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#posteriors.types.InitFn","title":"<code>posteriors.types.InitFn</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>posteriors/types.py</code> <pre><code>class InitFn(Protocol):\n    def __call__(\n        self,\n        params: TensorTree,\n    ) -&gt; TensorClass:\n        \"\"\"Initiate a posteriors state with unified API:\n\n        ```\n        state = init(params)\n        ```\n\n        where params is a PyTree of parameters. The produced `state` is a\n        `tensordict.TensorClass` containing the required information for the\n        posteriors iterative algorithm defined by the `init` and `update` functions.\n\n        Note that this represents the `init` function as stored in a `Transform`\n        returned by an algorithm's `build` function, the internal `init` function in\n        the algorithm module can and likely will have additional arguments.\n\n        Args:\n            params: PyTree containing initial value of parameters.\n\n        Returns:\n            The initial state, a `tensordict.tensorclass` with `params` and `aux`\n            attributes but possibly other attributes too.\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/types/#posteriors.types.InitFn.__call__","title":"<code>__call__(params)</code>","text":"<p>Initiate a posteriors state with unified API:</p> <pre><code>state = init(params)\n</code></pre> <p>where params is a PyTree of parameters. The produced <code>state</code> is a <code>tensordict.TensorClass</code> containing the required information for the posteriors iterative algorithm defined by the <code>init</code> and <code>update</code> functions.</p> <p>Note that this represents the <code>init</code> function as stored in a <code>Transform</code> returned by an algorithm's <code>build</code> function, the internal <code>init</code> function in the algorithm module can and likely will have additional arguments.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>PyTree containing initial value of parameters.</p> required <p>Returns:</p> Type Description <code>TensorClass</code> <p>The initial state, a <code>tensordict.tensorclass</code> with <code>params</code> and <code>aux</code></p> <code>TensorClass</code> <p>attributes but possibly other attributes too.</p> Source code in <code>posteriors/types.py</code> <pre><code>def __call__(\n    self,\n    params: TensorTree,\n) -&gt; TensorClass:\n    \"\"\"Initiate a posteriors state with unified API:\n\n    ```\n    state = init(params)\n    ```\n\n    where params is a PyTree of parameters. The produced `state` is a\n    `tensordict.TensorClass` containing the required information for the\n    posteriors iterative algorithm defined by the `init` and `update` functions.\n\n    Note that this represents the `init` function as stored in a `Transform`\n    returned by an algorithm's `build` function, the internal `init` function in\n    the algorithm module can and likely will have additional arguments.\n\n    Args:\n        params: PyTree containing initial value of parameters.\n\n    Returns:\n        The initial state, a `tensordict.tensorclass` with `params` and `aux`\n        attributes but possibly other attributes too.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/types/#posteriors.types.UpdateFn","title":"<code>posteriors.types.UpdateFn</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>posteriors/types.py</code> <pre><code>class UpdateFn(Protocol):\n    def __call__(\n        self,\n        state: TensorClass,\n        batch: Any,\n        inplace: bool = False,\n    ) -&gt; tuple[TensorClass, TensorTree]:\n        \"\"\"Transform a posteriors state with unified API:\n\n        ```\n        state, aux = update(state, batch, inplace=False)\n        ```\n\n        where state is a `tensordict.TensorClass` containing the required information\n        for the posteriors iterative algorithm defined by the `init` and `update`\n        functions. `aux` is an arbitrary info object returned by the\n        `log_posterior` or `log_likelihood` function.\n\n        Note that this represents the `update` function as stored in a `Transform`\n        returned by an algorithm's `build` function, the internal `update` function in\n        the algorithm module can and likely will have additional arguments.\n\n        Args:\n            state: The `tensordict.tensorclass` state of the iterative algorithm.\n            batch: The data batch.\n            inplace: Whether to modify state using inplace operations. Defaults to True.\n\n        Returns:\n            Tuple of `state` and `aux`.\n                `state` is a `tensordict.tensorclass` with `params` attributes\n                but possibly other attributes too. Must be of the same type as\n                the input state.\n                `aux` is an arbitrary info object returned by the\n                `log_posterior` or `log_likelihood` function.\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/types/#posteriors.types.UpdateFn.__call__","title":"<code>__call__(state, batch, inplace=False)</code>","text":"<p>Transform a posteriors state with unified API:</p> <pre><code>state, aux = update(state, batch, inplace=False)\n</code></pre> <p>where state is a <code>tensordict.TensorClass</code> containing the required information for the posteriors iterative algorithm defined by the <code>init</code> and <code>update</code> functions. <code>aux</code> is an arbitrary info object returned by the <code>log_posterior</code> or <code>log_likelihood</code> function.</p> <p>Note that this represents the <code>update</code> function as stored in a <code>Transform</code> returned by an algorithm's <code>build</code> function, the internal <code>update</code> function in the algorithm module can and likely will have additional arguments.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TensorClass</code> <p>The <code>tensordict.tensorclass</code> state of the iterative algorithm.</p> required <code>batch</code> <code>Any</code> <p>The data batch.</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify state using inplace operations. Defaults to True.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[TensorClass, TensorTree]</code> <p>Tuple of <code>state</code> and <code>aux</code>. <code>state</code> is a <code>tensordict.tensorclass</code> with <code>params</code> attributes but possibly other attributes too. Must be of the same type as the input state. <code>aux</code> is an arbitrary info object returned by the <code>log_posterior</code> or <code>log_likelihood</code> function.</p> Source code in <code>posteriors/types.py</code> <pre><code>def __call__(\n    self,\n    state: TensorClass,\n    batch: Any,\n    inplace: bool = False,\n) -&gt; tuple[TensorClass, TensorTree]:\n    \"\"\"Transform a posteriors state with unified API:\n\n    ```\n    state, aux = update(state, batch, inplace=False)\n    ```\n\n    where state is a `tensordict.TensorClass` containing the required information\n    for the posteriors iterative algorithm defined by the `init` and `update`\n    functions. `aux` is an arbitrary info object returned by the\n    `log_posterior` or `log_likelihood` function.\n\n    Note that this represents the `update` function as stored in a `Transform`\n    returned by an algorithm's `build` function, the internal `update` function in\n    the algorithm module can and likely will have additional arguments.\n\n    Args:\n        state: The `tensordict.tensorclass` state of the iterative algorithm.\n        batch: The data batch.\n        inplace: Whether to modify state using inplace operations. Defaults to True.\n\n    Returns:\n        Tuple of `state` and `aux`.\n            `state` is a `tensordict.tensorclass` with `params` attributes\n            but possibly other attributes too. Must be of the same type as\n            the input state.\n            `aux` is an arbitrary info object returned by the\n            `log_posterior` or `log_likelihood` function.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/types/#posteriors.types.Transform","title":"<code>posteriors.types.Transform</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A transform contains <code>init</code> and <code>update</code> functions defining an iterative     algorithm.</p> <p>Within the <code>Transform</code> all algorithm specific arguments are predefined, so that the <code>init</code> and <code>update</code> functions have a unified API: <pre><code>state = transform.init(params)\nstate, aux = transform.update(state, batch, inplace=False)\n</code></pre></p> <p>Note that this represents the <code>Transform</code> function is returned by an algorithm's <code>build</code> function, the internal <code>init</code> and <code>update</code> functions in the algorithm module can and likely will have additional arguments.</p> <p>Attributes:</p> Name Type Description <code>init</code> <code>InitFn</code> <p>The init function.</p> <code>update</code> <code>UpdateFn</code> <p>The update function.</p> Source code in <code>posteriors/types.py</code> <pre><code>class Transform(NamedTuple):\n    \"\"\"A transform contains `init` and `update` functions defining an iterative\n        algorithm.\n\n    Within the `Transform` all algorithm specific arguments are predefined, so that the\n    `init` and `update` functions have a unified API:\n    ```\n    state = transform.init(params)\n    state, aux = transform.update(state, batch, inplace=False)\n    ```\n\n    Note that this represents the `Transform` function is returned by an algorithm's\n    `build` function, the internal `init` and `update` functions in the\n    algorithm module can and likely will have additional arguments.\n\n    Attributes:\n        init: The init function.\n        update: The update function.\n\n    \"\"\"\n\n    init: InitFn\n    update: UpdateFn\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#posteriors.utils.CatchAuxError","title":"<code>posteriors.utils.CatchAuxError</code>","text":"<p>               Bases: <code>AbstractContextManager</code></p> <p>Context manager to catch errors when auxiliary output is not found.</p> Source code in <code>posteriors/utils.py</code> <pre><code>class CatchAuxError(contextlib.AbstractContextManager):\n    \"\"\"Context manager to catch errors when auxiliary output is not found.\"\"\"\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is not None:\n            if NO_AUX_ERROR_MSG in str(exc_value):\n                raise RuntimeError(\n                    \"Auxiliary output not found. Perhaps you have forgotten to return \"\n                    \"the aux output?\\n\"\n                    \"\\tIf you don't have any auxiliary info, simply amend to e.g. \"\n                    \"log_posterior(params, batch) -&gt; Tuple[float, torch.tensor([])].\\n\"\n                    \"\\tMore info at https://normal-computing.github.io/posteriors/log_posteriors\"\n                )\n            elif NON_TENSOR_AUX_ERROR_MSG in str(exc_value):\n                raise RuntimeError(\n                    \"Auxiliary output should be a TensorTree. If you don't have any \"\n                    \"auxiliary info, simply amend to e.g. \"\n                    \"log_posterior(params, batch) -&gt; Tuple[float, torch.tensor([])].\\n\"\n                    \"\\tMore info at https://normal-computing.github.io/posteriors/log_posteriors\"\n                )\n        return False\n</code></pre>"},{"location":"api/utils/#posteriors.utils.model_to_function","title":"<code>posteriors.utils.model_to_function(model)</code>","text":"<p>Converts a model into a function that maps parameters and inputs to outputs.</p> <p>Convenience wrapper around torch.functional_call.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>torch.nn.Module with parameters stored in .named_parameters().</p> required <p>Returns:</p> Type Description <code>Callable[[TensorTree, Any], Any]</code> <p>Function that takes a PyTree of parameters as well as any input arg or kwargs and returns the output of the model.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def model_to_function(model: torch.nn.Module) -&gt; Callable[[TensorTree, Any], Any]:\n    \"\"\"Converts a model into a function that maps parameters and inputs to outputs.\n\n    Convenience wrapper around [torch.functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html).\n\n    Args:\n        model: torch.nn.Module with parameters stored in .named_parameters().\n\n    Returns:\n        Function that takes a PyTree of parameters as well as any input\n            arg or kwargs and returns the output of the model.\n    \"\"\"\n\n    def func_model(p_dict, *args, **kwargs):\n        return functional_call(model, p_dict, args=args, kwargs=kwargs)\n\n    return func_model\n</code></pre>"},{"location":"api/utils/#posteriors.utils.linearized_forward_diag","title":"<code>posteriors.utils.linearized_forward_diag(forward_func, params, batch, sd_diag)</code>","text":"<p>Compute the linearized forward mean and its square root covariance, assuming posterior covariance over parameters is diagonal.</p> <p>$$ f(x | \u03b8) \\sim N(x | f(x | \u03b8\u2098), J(x | \u03b8\u2098) \\Sigma J(x | \u03b8\u2098)^T) $$ where \\(\u03b8\u2098\\) is the MAP estimate, \\(\\Sigma\\) is the diagonal covariance approximation at the MAP and \\(J(x | \u03b8\u2098)\\) is the Jacobian of the forward function \\(f(x | \u03b8\u2098)\\) with respect to \\(\u03b8\u2098\\).</p> <p>For more info on linearized models see Foong et al, 2019.</p> <p>Parameters:</p> Name Type Description Default <code>forward_func</code> <code>ForwardFn</code> <p>A function that takes params and batch and returns the forward values and any auxiliary information. Forward values must be a dim=2 Tensor with batch dimension in its first axis.</p> required <code>params</code> <code>TensorTree</code> <p>PyTree of tensors.</p> required <code>batch</code> <code>TensorTree</code> <p>PyTree of tensors.</p> required <code>sd_diag</code> <code>TensorTree</code> <p>PyTree of tensors of same shape as params.</p> required <p>Returns:</p> Type Description <code>Tuple[TensorTree, Tensor, TensorTree]</code> <p>A tuple of (forward_vals, chol, aux) where forward_vals is the output of the forward function (mean), chol is the tensor square root of the covariance matrix (non-diagonal) and aux is auxiliary info from the forward function.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def linearized_forward_diag(\n    forward_func: ForwardFn, params: TensorTree, batch: TensorTree, sd_diag: TensorTree\n) -&gt; Tuple[TensorTree, Tensor, TensorTree]:\n    \"\"\"Compute the linearized forward mean and its square root covariance, assuming\n    posterior covariance over parameters is diagonal.\n\n    $$\n    f(x | \u03b8) \\\\sim N(x | f(x | \u03b8\u2098), J(x | \u03b8\u2098) \\\\Sigma J(x | \u03b8\u2098)^T)\n    $$\n    where $\u03b8\u2098$ is the MAP estimate, $\\\\Sigma$ is the diagonal covariance approximation\n    at the MAP and $J(x | \u03b8\u2098)$ is the Jacobian of the forward function $f(x | \u03b8\u2098)$ with\n    respect to $\u03b8\u2098$.\n\n    For more info on linearized models see [Foong et al, 2019](https://arxiv.org/abs/1906.11537).\n\n    Args:\n        forward_func: A function that takes params and batch and returns the forward\n            values and any auxiliary information. Forward values must be a dim=2 Tensor\n            with batch dimension in its first axis.\n        params: PyTree of tensors.\n        batch: PyTree of tensors.\n        sd_diag: PyTree of tensors of same shape as params.\n\n    Returns:\n        A tuple of (forward_vals, chol, aux) where forward_vals is the output of the\n            forward function (mean), chol is the tensor square root of the covariance\n            matrix (non-diagonal) and aux is auxiliary info from the forward function.\n    \"\"\"\n    forward_vals, aux = forward_func(params, batch)\n\n    with torch.no_grad(), CatchAuxError():\n        jac, _ = jacrev(forward_func, has_aux=True)(params, batch)\n\n    # Convert Jacobian to be flat in parameter dimension\n    jac = tree_flatten(jac)[0]\n    jac = torch.cat([x.flatten(start_dim=2) for x in jac], dim=2)\n\n    # Flatten the diagonal square root covariance\n    sd_diag = tree_flatten(sd_diag)[0]\n    sd_diag = torch.cat([x.flatten() for x in sd_diag])\n\n    # Cholesky of J @ \u03a3 @ J^T\n    linearised_chol = torch.linalg.cholesky((jac * sd_diag**2) @ jac.transpose(-1, -2))\n\n    return forward_vals, linearised_chol, aux\n</code></pre>"},{"location":"api/utils/#posteriors.utils.hvp","title":"<code>posteriors.utils.hvp(f, primals, tangents, has_aux=False)</code>","text":"<p>Hessian vector product.</p> <p>H(primals) @ tangents</p> <p>where H(primals) is the Hessian of f evaluated at primals.</p> <p>Taken from jacobians_hessians.html. Follows API from <code>torch.func.jvp</code>.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>A function with scalar output.</p> required <code>primals</code> <code>tuple</code> <p>Tuple of e.g. tensor or dict with tensor values to evalute f at.</p> required <code>tangents</code> <code>tuple</code> <p>Tuple matching structure of primals.</p> required <code>has_aux</code> <code>bool</code> <p>Whether f returns auxiliary information.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[float, TensorTree] | Tuple[float, TensorTree, Any]</code> <p>Returns a (gradient, hvp_out) tuple containing the gradient of func evaluated at primals and the Hessian-vector product. If has_aux is True, then instead returns a (gradient, hvp_out, aux) tuple.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def hvp(\n    f: Callable, primals: tuple, tangents: tuple, has_aux: bool = False\n) -&gt; Tuple[float, TensorTree] | Tuple[float, TensorTree, Any]:\n    \"\"\"Hessian vector product.\n\n    H(primals) @ tangents\n\n    where H(primals) is the Hessian of f evaluated at primals.\n\n    Taken from [jacobians_hessians.html](https://pytorch.org/functorch/nightly/notebooks/jacobians_hessians.html).\n    Follows API from [`torch.func.jvp`](https://pytorch.org/docs/stable/generated/torch.func.jvp.html).\n\n    Args:\n        f: A function with scalar output.\n        primals: Tuple of e.g. tensor or dict with tensor values to evalute f at.\n        tangents: Tuple matching structure of primals.\n        has_aux: Whether f returns auxiliary information.\n\n    Returns:\n        Returns a (gradient, hvp_out) tuple containing the gradient of func evaluated at\n            primals and the Hessian-vector product. If has_aux is True, then instead\n            returns a (gradient, hvp_out, aux) tuple.\n    \"\"\"\n    return jvp(grad(f, has_aux=has_aux), primals, tangents, has_aux=has_aux)\n</code></pre>"},{"location":"api/utils/#posteriors.utils.fvp","title":"<code>posteriors.utils.fvp(f, primals, tangents, has_aux=False, normalize=False)</code>","text":"<p>Empirical Fisher vector product.</p> <p>F(primals) @ tangents</p> <p>where F(primals) is the empirical Fisher of f evaluated at primals.</p> <p>The empirical Fisher is defined as: $$ F(\u03b8) = J_f(\u03b8) J_f(\u03b8)^T $$ where typically \\(f_\u03b8\\) is the per-sample log likelihood (with elements \\(\\log p(y_i | x_i, \u03b8)\\) for a model with <code>primals</code> \\(\u03b8\\) given inputs \\(x_i\\) and labels \\(y_i\\)).</p> <p>If <code>normalize=True</code>, then \\(F(\u03b8)\\) is divided by the number of outputs from f (i.e. batchsize).</p> <p>Follows API from <code>torch.func.jvp</code>.</p> <p>More info on empirical Fisher matrices can be found in Martens, 2020.</p> <p>Examples:</p> <pre><code>from functools import partial\nfrom optree import tree_map\nimport torch\nfrom posteriors import fvp\n\n# Load model that outputs logits\n# Load batch = {'inputs': ..., 'labels': ...}\n\ndef log_likelihood_per_sample(params, batch):\n    output = torch.func.functional_call(model, params, batch[\"inputs\"])\n    return -torch.nn.functional.cross_entropy(\n        output, batch[\"labels\"], reduction=\"none\"\n    )\n\nparams = dict(model.parameters())\nv = tree_map(lambda x: torch.randn_like(x), params)\nfvp_result = fvp(\n    partial(log_likelihood_per_sample, batch=batch),\n    (params,),\n    (v,)\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>A function with tensor output. Typically this is the per-sample log likelihood of a model.</p> required <code>primals</code> <code>tuple</code> <p>Tuple of e.g. tensor or dict with tensor values to evaluate f at.</p> required <code>tangents</code> <code>tuple</code> <p>Tuple matching structure of primals.</p> required <code>has_aux</code> <code>bool</code> <p>Whether f returns auxiliary information.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize, divide by the dimension of the output from f.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[float, TensorTree] | Tuple[float, TensorTree, Any]</code> <p>Returns a (output, fvp_out) tuple containing the output of func evaluated at primals and the empirical Fisher-vector product. If has_aux is True, then instead returns a (output, fvp_out, aux) tuple.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def fvp(\n    f: Callable,\n    primals: tuple,\n    tangents: tuple,\n    has_aux: bool = False,\n    normalize: bool = False,\n) -&gt; Tuple[float, TensorTree] | Tuple[float, TensorTree, Any]:\n    \"\"\"Empirical Fisher vector product.\n\n    F(primals) @ tangents\n\n    where F(primals) is the empirical Fisher of f evaluated at primals.\n\n    The empirical Fisher is defined as:\n    $$\n    F(\u03b8) = J_f(\u03b8) J_f(\u03b8)^T\n    $$\n    where typically $f_\u03b8$ is the per-sample log likelihood (with elements\n    $\\\\log p(y_i | x_i, \u03b8)$ for a model with `primals` $\u03b8$ given inputs $x_i$ and\n    labels $y_i$).\n\n    If `normalize=True`, then $F(\u03b8)$ is divided by the number of outputs from f\n    (i.e. batchsize).\n\n    Follows API from [`torch.func.jvp`](https://pytorch.org/docs/stable/generated/torch.func.jvp.html).\n\n    More info on empirical Fisher matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf).\n\n    Examples:\n        ```python\n        from functools import partial\n        from optree import tree_map\n        import torch\n        from posteriors import fvp\n\n        # Load model that outputs logits\n        # Load batch = {'inputs': ..., 'labels': ...}\n\n        def log_likelihood_per_sample(params, batch):\n            output = torch.func.functional_call(model, params, batch[\"inputs\"])\n            return -torch.nn.functional.cross_entropy(\n                output, batch[\"labels\"], reduction=\"none\"\n            )\n\n        params = dict(model.parameters())\n        v = tree_map(lambda x: torch.randn_like(x), params)\n        fvp_result = fvp(\n            partial(log_likelihood_per_sample, batch=batch),\n            (params,),\n            (v,)\n        )\n        ```\n\n    Args:\n        f: A function with tensor output.\n            Typically this is the [per-sample log likelihood of a model](https://pytorch.org/tutorials/intermediate/per_sample_grads.html).\n        primals: Tuple of e.g. tensor or dict with tensor values to evaluate f at.\n        tangents: Tuple matching structure of primals.\n        has_aux: Whether f returns auxiliary information.\n        normalize: Whether to normalize, divide by the dimension of the output from f.\n\n    Returns:\n        Returns a (output, fvp_out) tuple containing the output of func evaluated at\n            primals and the empirical Fisher-vector product. If has_aux is True, then\n            instead returns a (output, fvp_out, aux) tuple.\n    \"\"\"\n    jvp_output = jvp(f, primals, tangents, has_aux=has_aux)\n    Jv = jvp_output[1]\n    f_vjp = vjp(f, *primals, has_aux=has_aux)[1]\n    Fv = f_vjp(Jv)[0]\n\n    if normalize:\n        output_dim = tree_flatten(jvp_output[0])[0][0].shape[0]\n        Fv = tree_map(lambda x: x / output_dim, Fv)\n\n    return jvp_output[0], Fv, *jvp_output[2:]\n</code></pre>"},{"location":"api/utils/#posteriors.utils.empirical_fisher","title":"<code>posteriors.utils.empirical_fisher(f, argnums=0, has_aux=False, normalize=False)</code>","text":"<p>Constructs function to compute the empirical Fisher information matrix of a function f with respect to its parameters, defined as (unnormalized): $$ F(\u03b8) = J_f(\u03b8) J_f(\u03b8)^T $$ where typically \\(f_\u03b8\\) is the per-sample log likelihood (with elements \\(\\log p(y_i | x_i, \u03b8)\\) for a model with <code>primals</code> \\(\u03b8\\) given inputs \\(x_i\\) and labels \\(y_i\\)).</p> <p>If <code>normalize=True</code>, then \\(F(\u03b8)\\) is divided by the number of outputs from f (i.e. batchsize).</p> <p>The empirical Fisher will be provided as a square tensor with respect to the ravelled parameters. <code>flat_params, params_unravel = optree.tree_ravel(params)</code>.</p> <p>Follows API from <code>torch.func.jacrev</code>.</p> <p>More info on empirical Fisher matrices can be found in Martens, 2020.</p> <p>Examples:</p> <pre><code>import torch\nfrom posteriors import empirical_fisher, per_samplify\n\n# Load model that outputs logits\n# Load batch = {'inputs': ..., 'labels': ...}\n\ndef log_likelihood(params, batch):\n    output = torch.func.functional_call(model, params, batch['inputs'])\n    return -torch.nn.functional.cross_entropy(output, batch['labels'])\n\nlikelihood_per_sample = per_samplify(log_likelihood)\nparams = dict(model.parameters())\nef_result = empirical_fisher(log_likelihood_per_sample)(params, batch)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors. Typically this is the per-sample log likelihood of a model.</p> required <code>argnums</code> <code>int | Sequence[int]</code> <p>Optional, integer or sequence of integers. Specifies which positional argument(s) to differentiate with respect to.</p> <code>0</code> <code>has_aux</code> <code>bool</code> <p>Whether f returns auxiliary information.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize, divide by the dimension of the output from f.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function with the same arguments as f that returns the empirical Fisher, F. If has_aux is True, then the function instead returns a tuple of (F, aux).</p> Source code in <code>posteriors/utils.py</code> <pre><code>def empirical_fisher(\n    f: Callable,\n    argnums: int | Sequence[int] = 0,\n    has_aux: bool = False,\n    normalize: bool = False,\n) -&gt; Callable:\n    \"\"\"\n    Constructs function to compute the empirical Fisher information matrix of a function\n    f with respect to its parameters, defined as (unnormalized):\n    $$\n    F(\u03b8) = J_f(\u03b8) J_f(\u03b8)^T\n    $$\n    where typically $f_\u03b8$ is the per-sample log likelihood (with elements\n    $\\\\log p(y_i | x_i, \u03b8)$ for a model with `primals` $\u03b8$ given inputs $x_i$ and\n    labels $y_i$).\n\n    If `normalize=True`, then $F(\u03b8)$ is divided by the number of outputs from f\n    (i.e. batchsize).\n\n    The empirical Fisher will be provided as a square tensor with respect to the\n    ravelled parameters.\n    `flat_params, params_unravel = optree.tree_ravel(params)`.\n\n    Follows API from [`torch.func.jacrev`](https://pytorch.org/functorch/stable/generated/functorch.jacrev.html).\n\n    More info on empirical Fisher matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf).\n\n    Examples:\n        ```python\n        import torch\n        from posteriors import empirical_fisher, per_samplify\n\n        # Load model that outputs logits\n        # Load batch = {'inputs': ..., 'labels': ...}\n\n        def log_likelihood(params, batch):\n            output = torch.func.functional_call(model, params, batch['inputs'])\n            return -torch.nn.functional.cross_entropy(output, batch['labels'])\n\n        likelihood_per_sample = per_samplify(log_likelihood)\n        params = dict(model.parameters())\n        ef_result = empirical_fisher(log_likelihood_per_sample)(params, batch)\n        ```\n\n    Args:\n        f:  A Python function that takes one or more arguments, one of which must be a\n            Tensor, and returns one or more Tensors.\n            Typically this is the [per-sample log likelihood of a model](https://pytorch.org/tutorials/intermediate/per_sample_grads.html).\n        argnums: Optional, integer or sequence of integers. Specifies which\n            positional argument(s) to differentiate with respect to.\n        has_aux: Whether f returns auxiliary information.\n        normalize: Whether to normalize, divide by the dimension of the output from f.\n\n    Returns:\n        A function with the same arguments as f that returns the empirical Fisher, F.\n            If has_aux is True, then the function instead returns a tuple of (F, aux).\n    \"\"\"\n\n    def f_to_flat(*args, **kwargs):\n        f_out = f(*args, **kwargs)\n        f_out_val = f_out[0] if has_aux else f_out\n        f_out_val = tree_ravel(f_out_val)[0]\n        return (f_out_val, f_out[1]) if has_aux else f_out_val\n\n    def fisher(*args, **kwargs):\n        jac_output = jacrev(f_to_flat, argnums=argnums, has_aux=has_aux)(\n            *args, **kwargs\n        )\n        jac = jac_output[0] if has_aux else jac_output\n\n        # Convert Jacobian to tensor, flat in parameter dimension\n        jac = torch.vmap(lambda x: tree_ravel(x)[0])(jac)\n\n        rescale = 1 / jac.shape[0] if normalize else 1\n\n        if has_aux:\n            return jac.T @ jac * rescale, jac_output[1]\n        else:\n            return jac.T @ jac * rescale\n\n    return fisher\n</code></pre>"},{"location":"api/utils/#posteriors.utils.ggnvp","title":"<code>posteriors.utils.ggnvp(forward, loss, primals, tangents, forward_has_aux=False, loss_has_aux=False, normalize=False)</code>","text":"<p>Generalised Gauss-Newton vector product.</p> <p>Equivalent to the (non-empirical) Fisher vector product when <code>loss</code> is the negative log likelihood of an exponential family distribution as a function of its natural parameter.</p> <p>Defined as $$ G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T $$ where \\(z = f(\u03b8)\\) is the output of the forward function \\(f\\) and \\(l(z)\\) is a loss function with scalar output.</p> <p>Thus \\(J_f(\u03b8)\\) is the Jacobian of the forward function \\(f\\) evaluated at <code>primals</code> \\(\u03b8\\), with dimensions <code>(dz, d\u03b8)</code>. And \\(H_l(z)\\) is the Hessian of the loss function \\(l\\) evaluated at <code>z = f(\u03b8)</code>, with dimensions <code>(dz, dz)</code>.</p> <p>Follows API from <code>torch.func.jvp</code>.</p> <p>More info on Fisher and GGN matrices can be found in Martens, 2020.</p> <p>Examples:</p> <pre><code>from functools import partial\nfrom optree import tree_map\nimport torch\nfrom posteriors import ggnvp\n\n# Load model that outputs logits\n# Load batch = {'inputs': ..., 'labels': ...}\n\ndef forward(params, inputs):\n    return torch.func.functional_call(model, params, inputs)\n\ndef loss(logits, labels):\n    return torch.nn.functional.cross_entropy(logits, labels)\n\nparams = dict(model.parameters())\nv = tree_map(lambda x: torch.randn_like(x), params)\nggnvp_result = ggnvp(\n    partial(forward, inputs=batch['inputs']),\n    partial(loss, labels=batch['labels']),\n    (params,),\n    (v,),\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>Callable</code> <p>A function with tensor output.</p> required <code>loss</code> <code>Callable</code> <p>A function that maps the output of forward to a scalar output.</p> required <code>primals</code> <code>tuple</code> <p>Tuple of e.g. tensor or dict with tensor values to evaluate f at.</p> required <code>tangents</code> <code>tuple</code> <p>Tuple matching structure of primals.</p> required <code>forward_has_aux</code> <code>bool</code> <p>Whether forward returns auxiliary information.</p> <code>False</code> <code>loss_has_aux</code> <code>bool</code> <p>Whether loss returns auxiliary information.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize, divide by the first dimension of the output from f.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[float, TensorTree] | Tuple[float, TensorTree, Any] | Tuple[float, TensorTree, Any, Any]</code> <p>Returns a (output, ggnvp_out) tuple, where output is a tuple of <code>(forward(primals), grad(loss)(forward(primals)))</code>. If forward_has_aux or loss_has_aux is True, then instead returns a (output, ggnvp_out, aux) or (output, ggnvp_out, forward_aux, loss_aux) tuple accordingly.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def ggnvp(\n    forward: Callable,\n    loss: Callable,\n    primals: tuple,\n    tangents: tuple,\n    forward_has_aux: bool = False,\n    loss_has_aux: bool = False,\n    normalize: bool = False,\n) -&gt; (\n    Tuple[float, TensorTree]\n    | Tuple[float, TensorTree, Any]\n    | Tuple[float, TensorTree, Any, Any]\n):\n    \"\"\"Generalised Gauss-Newton vector product.\n\n    Equivalent to the (non-empirical) Fisher vector product when `loss` is the negative\n    log likelihood of an exponential family distribution as a function of its natural\n    parameter.\n\n    Defined as\n    $$\n    G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T\n    $$\n    where $z = f(\u03b8)$ is the output of the forward function $f$ and $l(z)$\n    is a loss function with scalar output.\n\n    Thus $J_f(\u03b8)$ is the Jacobian of the forward function $f$ evaluated\n    at `primals` $\u03b8$, with dimensions `(dz, d\u03b8)`.\n    And $H_l(z)$ is the Hessian of the loss function $l$ evaluated at `z = f(\u03b8)`, with\n    dimensions `(dz, dz)`.\n\n    Follows API from [`torch.func.jvp`](https://pytorch.org/docs/stable/generated/torch.func.jvp.html).\n\n    More info on Fisher and GGN matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf).\n\n    Examples:\n        ```python\n        from functools import partial\n        from optree import tree_map\n        import torch\n        from posteriors import ggnvp\n\n        # Load model that outputs logits\n        # Load batch = {'inputs': ..., 'labels': ...}\n\n        def forward(params, inputs):\n            return torch.func.functional_call(model, params, inputs)\n\n        def loss(logits, labels):\n            return torch.nn.functional.cross_entropy(logits, labels)\n\n        params = dict(model.parameters())\n        v = tree_map(lambda x: torch.randn_like(x), params)\n        ggnvp_result = ggnvp(\n            partial(forward, inputs=batch['inputs']),\n            partial(loss, labels=batch['labels']),\n            (params,),\n            (v,),\n        )\n        ```\n\n    Args:\n        forward: A function with tensor output.\n        loss: A function that maps the output of forward to a scalar output.\n        primals: Tuple of e.g. tensor or dict with tensor values to evaluate f at.\n        tangents: Tuple matching structure of primals.\n        forward_has_aux: Whether forward returns auxiliary information.\n        loss_has_aux: Whether loss returns auxiliary information.\n        normalize: Whether to normalize, divide by the first dimension of the output\n            from f.\n\n    Returns:\n        Returns a (output, ggnvp_out) tuple, where output is a tuple of\n            `(forward(primals), grad(loss)(forward(primals)))`.\n            If forward_has_aux or loss_has_aux is True, then instead returns a\n            (output, ggnvp_out, aux) or\n            (output, ggnvp_out, forward_aux, loss_aux) tuple accordingly.\n    \"\"\"\n\n    jvp_output = jvp(forward, primals, tangents, has_aux=forward_has_aux)\n    z = jvp_output[0]\n    Jv = jvp_output[1]\n    HJv_output = hvp(loss, (z,), (Jv,), has_aux=loss_has_aux)\n    HJv = HJv_output[1]\n\n    if normalize:\n        output_dim = tree_flatten(jvp_output[0])[0][0].shape[0]\n        HJv = tree_map(lambda x: x / output_dim, HJv)\n\n    forward_vjp = vjp(forward, *primals, has_aux=forward_has_aux)[1]\n    JTHJv = forward_vjp(HJv)[0]\n\n    return (jvp_output[0], HJv_output[0]), JTHJv, *jvp_output[2:], *HJv_output[2:]\n</code></pre>"},{"location":"api/utils/#posteriors.utils.ggn","title":"<code>posteriors.utils.ggn(forward, loss, argnums=0, forward_has_aux=False, loss_has_aux=False, normalize=False)</code>","text":"<p>Constructs function to compute the Generalised Gauss-Newton matrix.</p> <p>Equivalent to the (non-empirical) Fisher when <code>loss</code> is the negative log likelihood of an exponential family distribution as a function of its natural parameter.</p> <p>Defined as $$ G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T $$ where \\(z = f(\u03b8)\\) is the output of the forward function \\(f\\) and \\(l(z)\\) is a loss function with scalar output.</p> <p>Thus \\(J_f(\u03b8)\\) is the Jacobian of the forward function \\(f\\) evaluated at <code>primals</code> \\(\u03b8\\). And \\(H_l(z)\\) is the Hessian of the loss function \\(l\\) evaluated at <code>z = f(\u03b8)</code>.</p> <p>Requires output from <code>forward</code> to be a tensor and therefore <code>loss</code> takes a tensor as input. Although both support <code>aux</code> output.</p> <p>If <code>normalize=True</code>, then \\(G(\u03b8)\\) is divided by the size of the leading dimension of outputs from <code>forward</code> (i.e. batchsize).</p> <p>The GGN will be provided as a square tensor with respect to the ravelled parameters. <code>flat_params, params_unravel = optree.tree_ravel(params)</code>.</p> <p>Follows API from <code>torch.func.jacrev</code>.</p> <p>More info on Fisher and GGN matrices can be found in Martens, 2020.</p> <p>Examples:</p> <pre><code>from functools import partial\nimport torch\nfrom posteriors import ggn\n\n# Load model that outputs logits\n# Load batch = {'inputs': ..., 'labels': ...}\n\ndef forward(params, inputs):\n    return torch.func.functional_call(model, params, inputs)\n\ndef loss(logits, labels):\n    return torch.nn.functional.cross_entropy(logits, labels)\n\nparams = dict(model.parameters())\nggn_result = ggn(\n    partial(forward, inputs=batch['inputs']),\n    partial(loss, labels=batch['labels']),\n)(params)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>Callable</code> <p>A function with tensor output.</p> required <code>loss</code> <code>Callable</code> <p>A function that maps the output of forward to a scalar output. Takes a single input and returns a scalar (and possibly aux).</p> required <code>argnums</code> <code>int | Sequence[int]</code> <p>Optional, integer or sequence of integers. Specifies which positional argument(s) to differentiate <code>forward</code> with respect to.</p> <code>0</code> <code>forward_has_aux</code> <code>bool</code> <p>Whether forward returns auxiliary information.</p> <code>False</code> <code>loss_has_aux</code> <code>bool</code> <p>Whether loss returns auxiliary information.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize, divide by the first dimension of the output from f.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function with the same arguments as f that returns the tensor GGN. If has_aux is True, then the function instead returns a tuple of (F, aux).</p> Source code in <code>posteriors/utils.py</code> <pre><code>def ggn(\n    forward: Callable,\n    loss: Callable,\n    argnums: int | Sequence[int] = 0,\n    forward_has_aux: bool = False,\n    loss_has_aux: bool = False,\n    normalize: bool = False,\n) -&gt; Callable:\n    \"\"\"\n    Constructs function to compute the Generalised Gauss-Newton matrix.\n\n    Equivalent to the (non-empirical) Fisher when `loss` is the negative\n    log likelihood of an exponential family distribution as a function of its natural\n    parameter.\n\n    Defined as\n    $$\n    G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T\n    $$\n    where $z = f(\u03b8)$ is the output of the forward function $f$ and $l(z)$\n    is a loss function with scalar output.\n\n    Thus $J_f(\u03b8)$ is the Jacobian of the forward function $f$ evaluated\n    at `primals` $\u03b8$. And $H_l(z)$ is the Hessian of the loss function $l$ evaluated\n    at `z = f(\u03b8)`.\n\n    Requires output from `forward` to be a tensor and therefore `loss` takes a tensor as\n    input. Although both support `aux` output.\n\n    If `normalize=True`, then $G(\u03b8)$ is divided by the size of the leading dimension of\n    outputs from `forward` (i.e. batchsize).\n\n    The GGN will be provided as a square tensor with respect to the\n    ravelled parameters.\n    `flat_params, params_unravel = optree.tree_ravel(params)`.\n\n    Follows API from [`torch.func.jacrev`](https://pytorch.org/functorch/stable/generated/functorch.jacrev.html).\n\n    More info on Fisher and GGN matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf).\n\n    Examples:\n        ```python\n        from functools import partial\n        import torch\n        from posteriors import ggn\n\n        # Load model that outputs logits\n        # Load batch = {'inputs': ..., 'labels': ...}\n\n        def forward(params, inputs):\n            return torch.func.functional_call(model, params, inputs)\n\n        def loss(logits, labels):\n            return torch.nn.functional.cross_entropy(logits, labels)\n\n        params = dict(model.parameters())\n        ggn_result = ggn(\n            partial(forward, inputs=batch['inputs']),\n            partial(loss, labels=batch['labels']),\n        )(params)\n        ```\n\n    Args:\n        forward: A function with tensor output.\n        loss: A function that maps the output of forward to a scalar output.\n            Takes a single input and returns a scalar (and possibly aux).\n        argnums: Optional, integer or sequence of integers. Specifies which\n            positional argument(s) to differentiate `forward` with respect to.\n        forward_has_aux: Whether forward returns auxiliary information.\n        loss_has_aux: Whether loss returns auxiliary information.\n        normalize: Whether to normalize, divide by the first dimension of the output\n            from f.\n\n    Returns:\n        A function with the same arguments as f that returns the tensor GGN.\n            If has_aux is True, then the function instead returns a tuple of (F, aux).\n    \"\"\"\n    assert argnums == 0, \"Only argnums=0 is supported for now.\"\n\n    def internal_ggn(params):\n        flat_params, params_unravel = tree_ravel(params)\n\n        def flat_params_to_forward(fps):\n            return forward(params_unravel(fps))\n\n        jac, hess, aux = _hess_and_jac_for_ggn(\n            flat_params_to_forward,\n            loss,\n            argnums,\n            forward_has_aux,\n            loss_has_aux,\n            normalize,\n            flat_params,\n        )\n\n        if aux:\n            return jac.T @ (hess @ jac), *aux\n        else:\n            return jac.T @ (hess @ jac)\n\n    return internal_ggn\n</code></pre>"},{"location":"api/utils/#posteriors.utils.diag_ggn","title":"<code>posteriors.utils.diag_ggn(forward, loss, argnums=0, forward_has_aux=False, loss_has_aux=False, normalize=False)</code>","text":"<p>Constructs function to compute the diagonal of the Generalised Gauss-Newton matrix.</p> <p>Equivalent to the (non-empirical) diagonal Fisher when <code>loss</code> is the negative log likelihood of an exponential family distribution as a function of its natural parameter.</p> <p>The GGN is defined as $$ G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T $$ where \\(z = f(\u03b8)\\) is the output of the forward function \\(f\\) and \\(l(z)\\) is a loss function with scalar output.</p> <p>Thus \\(J_f(\u03b8)\\) is the Jacobian of the forward function \\(f\\) evaluated at <code>primals</code> \\(\u03b8\\). And \\(H_l(z)\\) is the Hessian of the loss function \\(l\\) evaluated at <code>z = f(\u03b8)</code>.</p> <p>Requires output from <code>forward</code> to be a tensor and therefore <code>loss</code> takes a tensor as input. Although both support <code>aux</code> output.</p> <p>If <code>normalize=True</code>, then \\(G(\u03b8)\\) is divided by the size of the leading dimension of outputs from <code>forward</code> (i.e. batchsize).</p> <p>Unlike <code>posteriors.ggn</code>, the output will be in PyTree form matching the input.</p> <p>Follows API from <code>torch.func.jacrev</code>.</p> <p>More info on Fisher and GGN matrices can be found in Martens, 2020.</p> <p>Examples:</p> <pre><code>from functools import partial\nimport torch\nfrom posteriors import diag_ggn\n\n# Load model that outputs logits\n# Load batch = {'inputs': ..., 'labels': ...}\n\ndef forward(params, inputs):\n    return torch.func.functional_call(model, params, inputs)\n\ndef loss(logits, labels):\n    return torch.nn.functional.cross_entropy(logits, labels)\n\nparams = dict(model.parameters())\nggndiag_result = diag_ggn(\n    partial(forward, inputs=batch['inputs']),\n    partial(loss, labels=batch['labels']),\n)(params)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>Callable</code> <p>A function with tensor output.</p> required <code>loss</code> <code>Callable</code> <p>A function that maps the output of forward to a scalar output. Takes a single input and returns a scalar (and possibly aux).</p> required <code>argnums</code> <code>int | Sequence[int]</code> <p>Optional, integer or sequence of integers. Specifies which positional argument(s) to differentiate <code>forward</code> with respect to.</p> <code>0</code> <code>forward_has_aux</code> <code>bool</code> <p>Whether forward returns auxiliary information.</p> <code>False</code> <code>loss_has_aux</code> <code>bool</code> <p>Whether loss returns auxiliary information.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize, divide by the first dimension of the output from f.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function with the same arguments as f that returns the diagonal GGN. If has_aux is True, then the function instead returns a tuple of (F, aux).</p> Source code in <code>posteriors/utils.py</code> <pre><code>def diag_ggn(\n    forward: Callable,\n    loss: Callable,\n    argnums: int | Sequence[int] = 0,\n    forward_has_aux: bool = False,\n    loss_has_aux: bool = False,\n    normalize: bool = False,\n) -&gt; Callable:\n    \"\"\"\n    Constructs function to compute the diagonal of the Generalised Gauss-Newton matrix.\n\n    Equivalent to the (non-empirical) diagonal Fisher when `loss` is the negative\n    log likelihood of an exponential family distribution as a function of its natural\n    parameter.\n\n    The GGN is defined as\n    $$\n    G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T\n    $$\n    where $z = f(\u03b8)$ is the output of the forward function $f$ and $l(z)$\n    is a loss function with scalar output.\n\n    Thus $J_f(\u03b8)$ is the Jacobian of the forward function $f$ evaluated\n    at `primals` $\u03b8$. And $H_l(z)$ is the Hessian of the loss function $l$ evaluated\n    at `z = f(\u03b8)`.\n\n    Requires output from `forward` to be a tensor and therefore `loss` takes a tensor as\n    input. Although both support `aux` output.\n\n    If `normalize=True`, then $G(\u03b8)$ is divided by the size of the leading dimension of\n    outputs from `forward` (i.e. batchsize).\n\n    Unlike `posteriors.ggn`, the output will be in PyTree form matching the input.\n\n    Follows API from [`torch.func.jacrev`](https://pytorch.org/functorch/stable/generated/functorch.jacrev.html).\n\n    More info on Fisher and GGN matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf).\n\n    Examples:\n        ```python\n        from functools import partial\n        import torch\n        from posteriors import diag_ggn\n\n        # Load model that outputs logits\n        # Load batch = {'inputs': ..., 'labels': ...}\n\n        def forward(params, inputs):\n            return torch.func.functional_call(model, params, inputs)\n\n        def loss(logits, labels):\n            return torch.nn.functional.cross_entropy(logits, labels)\n\n        params = dict(model.parameters())\n        ggndiag_result = diag_ggn(\n            partial(forward, inputs=batch['inputs']),\n            partial(loss, labels=batch['labels']),\n        )(params)\n        ```\n\n    Args:\n        forward: A function with tensor output.\n        loss: A function that maps the output of forward to a scalar output.\n            Takes a single input and returns a scalar (and possibly aux).\n        argnums: Optional, integer or sequence of integers. Specifies which\n            positional argument(s) to differentiate `forward` with respect to.\n        forward_has_aux: Whether forward returns auxiliary information.\n        loss_has_aux: Whether loss returns auxiliary information.\n        normalize: Whether to normalize, divide by the first dimension of the output\n            from f.\n\n    Returns:\n        A function with the same arguments as f that returns the diagonal GGN.\n            If has_aux is True, then the function instead returns a tuple of (F, aux).\n    \"\"\"\n    assert argnums == 0, \"Only argnums=0 is supported for now.\"\n\n    def internal_ggn(params):\n        flat_params, params_unravel = tree_ravel(params)\n\n        def flat_params_to_forward(fps):\n            return forward(params_unravel(fps))\n\n        jac, hess, aux = _hess_and_jac_for_ggn(\n            flat_params_to_forward,\n            loss,\n            argnums,\n            forward_has_aux,\n            loss_has_aux,\n            normalize,\n            flat_params,\n        )\n\n        G_diag = torch.einsum(\"ji,jk,ki-&gt;i\", jac, hess, jac)\n        G_diag = params_unravel(G_diag)\n\n        if aux:\n            return G_diag, *aux\n        else:\n            return G_diag\n\n    return internal_ggn\n</code></pre>"},{"location":"api/utils/#posteriors.utils.cg","title":"<code>posteriors.utils.cg(A, b, x0=None, *, maxiter=None, damping=0.0, tol=1e-05, atol=0.0, M=_identity)</code>","text":"<p>Use Conjugate Gradient iteration to solve <code>Ax = b</code>. <code>A</code> is supplied as a function instead of a matrix.</p> <p>Adapted from <code>jax.scipy.sparse.linalg.cg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Callable</code> <p>Callable that calculates the linear map (matrix-vector product) <code>Ax</code> when called like <code>A(x)</code>. <code>A</code> must represent a hermitian, positive definite matrix, and must return array(s) with the same structure and shape as its argument.</p> required <code>b</code> <code>TensorTree</code> <p>Right hand side of the linear system representing a single vector.</p> required <code>x0</code> <code>TensorTree</code> <p>Starting guess for the solution. Must have the same structure as <code>b</code>.</p> <code>None</code> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations.  Iteration will stop after maxiter steps even if the specified tolerance has not been achieved.</p> <code>None</code> <code>damping</code> <code>float</code> <p>damping term for the mvp function. Acts as regularization.</p> <code>0.0</code> <code>tol</code> <code>float</code> <p>Tolerance for convergence.</p> <code>1e-05</code> <code>atol</code> <code>float</code> <p>Tolerance for convergence. <code>norm(residual) &lt;= max(tol*norm(b), atol)</code>. The behaviour will differ from SciPy unless you explicitly pass <code>atol</code> to SciPy's <code>cg</code>.</p> <code>0.0</code> <code>M</code> <code>Callable</code> <p>Preconditioner for A. See the preconditioned CG method.</p> <code>_identity</code> <p>Returns:</p> Name Type Description <code>x</code> <code>TensorTree</code> <p>The converged solution. Has the same structure as <code>b</code>.</p> <code>info</code> <code>Any</code> <p>Placeholder for convergence information.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def cg(\n    A: Callable,\n    b: TensorTree,\n    x0: TensorTree = None,\n    *,\n    maxiter: int = None,\n    damping: float = 0.0,\n    tol: float = 1e-5,\n    atol: float = 0.0,\n    M: Callable = _identity,\n) -&gt; Tuple[TensorTree, Any]:\n    \"\"\"Use Conjugate Gradient iteration to solve ``Ax = b``.\n    ``A`` is supplied as a function instead of a matrix.\n\n    Adapted from [`jax.scipy.sparse.linalg.cg`](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.sparse.linalg.cg.html).\n\n    Args:\n        A:  Callable that calculates the linear map (matrix-vector\n            product) ``Ax`` when called like ``A(x)``. ``A`` must represent\n            a hermitian, positive definite matrix, and must return array(s) with the\n            same structure and shape as its argument.\n        b:  Right hand side of the linear system representing a single vector.\n        x0: Starting guess for the solution. Must have the same structure as ``b``.\n        maxiter: Maximum number of iterations.  Iteration will stop after maxiter\n            steps even if the specified tolerance has not been achieved.\n        damping: damping term for the mvp function. Acts as regularization.\n        tol: Tolerance for convergence.\n        atol: Tolerance for convergence. ``norm(residual) &lt;= max(tol*norm(b), atol)``.\n            The behaviour will differ from SciPy unless you explicitly pass\n            ``atol`` to SciPy's ``cg``.\n        M: Preconditioner for A.\n            See [the preconditioned CG method.](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_preconditioned_conjugate_gradient_method)\n\n    Returns:\n        x : The converged solution. Has the same structure as ``b``.\n        info : Placeholder for convergence information.\n    \"\"\"\n    if x0 is None:\n        x0 = tree_map(torch.zeros_like, b)\n\n    if maxiter is None:\n        maxiter = 10 * tree_size(b)  # copied from scipy\n\n    tol *= torch.tensor([1.0])\n    atol *= torch.tensor([1.0])\n\n    # tolerance handling uses the \"non-legacy\" behavior of scipy.sparse.linalg.cg\n    bs = _vdot_real_tree(b, b)\n    atol2 = torch.maximum(torch.square(tol) * bs, torch.square(atol))\n\n    def A_damped(p):\n        return _add(A(p), _mul(damping, p))\n\n    def cond_fun(value):\n        _, r, gamma, _, k = value\n        rs = gamma.real if M is _identity else _vdot_real_tree(r, r)\n        return (rs &gt; atol2) &amp; (k &lt; maxiter)\n\n    def body_fun(value):\n        x, r, gamma, p, k = value\n        Ap = A_damped(p)\n        alpha = gamma / _vdot_real_tree(p, Ap)\n        x_ = _add(x, _mul(alpha, p))\n        r_ = _sub(r, _mul(alpha, Ap))\n        z_ = M(r_)\n        gamma_ = _vdot_real_tree(r_, z_)\n        beta_ = gamma_ / gamma\n        p_ = _add(z_, _mul(beta_, p))\n        return x_, r_, gamma_, p_, k + 1\n\n    r0 = _sub(b, A_damped(x0))\n    p0 = z0 = r0\n    gamma0 = _vdot_real_tree(r0, z0)\n    initial_value = (x0, r0, gamma0, p0, 0)\n\n    value = initial_value\n\n    while cond_fun(value):\n        value = body_fun(value)\n\n    x_final, r, gamma, _, k = value\n    # compute the final error and whether it has converged.\n    rs = gamma if M is _identity else _vdot_real_tree(r, r)\n    converged = rs &lt;= atol2\n\n    # additional info output structure\n    info = {\"error\": rs, \"converged\": converged, \"niter\": k}\n\n    return x_final, info\n</code></pre>"},{"location":"api/utils/#posteriors.utils.diag_normal_log_prob","title":"<code>posteriors.utils.diag_normal_log_prob(x, mean=0.0, sd_diag=1.0, normalize=True)</code>","text":"<p>Evaluate multivariate normal log probability for a diagonal covariance matrix.</p> <p>If either mean or sd_diag are scalars, they will be broadcast to the same shape as x (in a memory efficient manner).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>TensorTree</code> <p>Value to evaluate log probability at.</p> required <code>mean</code> <code>float | TensorTree</code> <p>Mean of the distribution.</p> <code>0.0</code> <code>sd_diag</code> <code>float | TensorTree</code> <p>Square-root diagonal of the covariance matrix.</p> <code>1.0</code> <code>normalize</code> <code>bool</code> <p>Whether to compute normalized log probability. If False the elementwise log prob is -0.5 * ((x - mean) / sd_diag)**2.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>Scalar log probability.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def diag_normal_log_prob(\n    x: TensorTree,\n    mean: float | TensorTree = 0.0,\n    sd_diag: float | TensorTree = 1.0,\n    normalize: bool = True,\n) -&gt; float:\n    \"\"\"Evaluate multivariate normal log probability for a diagonal covariance matrix.\n\n    If either mean or sd_diag are scalars, they will be broadcast to the same shape as x\n    (in a memory efficient manner).\n\n    Args:\n        x: Value to evaluate log probability at.\n        mean: Mean of the distribution.\n        sd_diag: Square-root diagonal of the covariance matrix.\n        normalize: Whether to compute normalized log probability.\n            If False the elementwise log prob is -0.5 * ((x - mean) / sd_diag)**2.\n\n    Returns:\n        Scalar log probability.\n    \"\"\"\n    if tree_size(mean) == 1:\n        mean = tree_map(lambda t: torch.tensor(mean, device=t.device), x)\n    if tree_size(sd_diag) == 1:\n        sd_diag = tree_map(lambda t: torch.tensor(sd_diag, device=t.device), x)\n\n    if normalize:\n\n        def univariate_norm_and_sum(v, m, sd):\n            return Normal(m, sd, validate_args=False).log_prob(v).sum()\n    else:\n\n        def univariate_norm_and_sum(v, m, sd):\n            return (-0.5 * ((v - m) / sd) ** 2).sum()\n\n    log_probs = tree_map(\n        univariate_norm_and_sum,\n        x,\n        mean,\n        sd_diag,\n    )\n    log_prob = tree_reduce(torch.add, log_probs)\n    return log_prob\n</code></pre>"},{"location":"api/utils/#posteriors.utils.diag_normal_sample","title":"<code>posteriors.utils.diag_normal_sample(mean, sd_diag, sample_shape=torch.Size([]))</code>","text":"<p>Sample from multivariate normal with diagonal covariance matrix.</p> <p>If sd_diag is scalar, it will be broadcast to the same shape as mean (in a memory efficient manner).</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>TensorTree</code> <p>Mean of the distribution.</p> required <code>sd_diag</code> <code>float | TensorTree</code> <p>Square-root diagonal of the covariance matrix.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the sample.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>dict</code> <p>Sample(s) from normal distribution with the same structure as mean and sd_diag.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def diag_normal_sample(\n    mean: TensorTree,\n    sd_diag: float | TensorTree,\n    sample_shape: torch.Size = torch.Size([]),\n) -&gt; dict:\n    \"\"\"Sample from multivariate normal with diagonal covariance matrix.\n\n    If sd_diag is scalar, it will be broadcast to the same shape as mean\n    (in a memory efficient manner).\n\n    Args:\n        mean: Mean of the distribution.\n        sd_diag: Square-root diagonal of the covariance matrix.\n        sample_shape: Shape of the sample.\n\n    Returns:\n        Sample(s) from normal distribution with the same structure as mean and sd_diag.\n    \"\"\"\n    if tree_size(sd_diag) == 1:\n        sd_diag = tree_map(lambda t: torch.tensor(sd_diag, device=t.device), mean)\n\n    return tree_map(\n        lambda m, sd: m + torch.randn(sample_shape + m.shape, device=m.device) * sd,\n        mean,\n        sd_diag,\n    )\n</code></pre>"},{"location":"api/utils/#posteriors.utils.per_samplify","title":"<code>posteriors.utils.per_samplify(f)</code>","text":"<p>Converts a function that takes params and batch into one that provides an output for each batch sample.</p> <pre><code>output = f(params, batch)\nper_sample_output = per_samplify(f)(params, batch)\n</code></pre> <p>For more info see per_sample_grads.html</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[TensorTree, TensorTree], Any]</code> <p>A function that takes params and batch provides an output with size independent of batchsize (i.e. averaged).</p> required <p>Returns:</p> Type Description <code>Callable[[TensorTree, TensorTree], Any]</code> <p>A new function that provides an output for each batch sample. <code>per_sample_output  = per_samplify(f)(params, batch)</code></p> Source code in <code>posteriors/utils.py</code> <pre><code>def per_samplify(\n    f: Callable[[TensorTree, TensorTree], Any],\n) -&gt; Callable[[TensorTree, TensorTree], Any]:\n    \"\"\"Converts a function that takes params and batch into one that provides an output\n    for each batch sample.\n\n    ```\n    output = f(params, batch)\n    per_sample_output = per_samplify(f)(params, batch)\n    ```\n\n    For more info see [per_sample_grads.html](https://pytorch.org/tutorials/intermediate/per_sample_grads.html)\n\n    Args:\n        f: A function that takes params and batch provides an output with size\n            independent of batchsize (i.e. averaged).\n\n    Returns:\n        A new function that provides an output for each batch sample.\n            `per_sample_output  = per_samplify(f)(params, batch)`\n    \"\"\"\n\n    @partial(torch.vmap, in_dims=(None, 0))\n    def f_per_sample(params, batch):\n        batch = tree_map(lambda x: x.unsqueeze(0), batch)\n        return f(params, batch)\n\n    @wraps(f)\n    def f_per_sample_ensure_no_kwargs(params, batch):\n        return f_per_sample(params, batch)  # vmap in_dims requires no kwargs\n\n    return f_per_sample_ensure_no_kwargs\n</code></pre>"},{"location":"api/utils/#posteriors.utils.is_scalar","title":"<code>posteriors.utils.is_scalar(x)</code>","text":"<p>Returns True if x is a scalar (int, float, bool) or a tensor with a single element.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Any object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if x is a scalar.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def is_scalar(x: Any) -&gt; bool:\n    \"\"\"Returns True if x is a scalar (int, float, bool) or a tensor with a single element.\n\n    Args:\n        x: Any object.\n\n    Returns:\n        True if x is a scalar.\n    \"\"\"\n    return isinstance(x, (int, float)) or (torch.is_tensor(x) and x.numel() == 1)\n</code></pre>"},{"location":"api/utils/#posteriors.utils.L_from_flat","title":"<code>posteriors.utils.L_from_flat(L_flat)</code>","text":"<p>Returns lower triangular matrix from a flat representation of its nonzero elements.</p> <p>Parameters:</p> Name Type Description Default <code>L_flat</code> <code>Tensor</code> <p>Flat representation of nonzero lower triangular matrix elements.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Lower triangular matrix.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def L_from_flat(L_flat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns lower triangular matrix from a flat representation of its nonzero elements.\n\n    Args:\n        L_flat: Flat representation of nonzero lower triangular matrix elements.\n\n    Returns:\n        Lower triangular matrix.\n    \"\"\"\n    k = torch.tensor(L_flat.shape[0], dtype=L_flat.dtype, device=L_flat.device)\n    n = (-1 + (1 + 8 * k).sqrt()) / 2\n    num_params = round(n.item())\n\n    tril_indices = torch.tril_indices(num_params, num_params)\n    L = torch.zeros((num_params, num_params), device=L_flat.device)\n    L[tril_indices[0], tril_indices[1]] = L_flat\n    return L\n</code></pre>"},{"location":"api/utils/#posteriors.utils.L_to_flat","title":"<code>posteriors.utils.L_to_flat(L)</code>","text":"<p>Returns flat representation of the nonzero elements of a lower triangular matrix.</p> <p>Parameters:</p> Name Type Description Default <code>L</code> <code>Tensor</code> <p>Lower triangular matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Flat representation of the nonzero lower triangular matrix elements.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def L_to_flat(L: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns flat representation of the nonzero elements of a lower triangular matrix.\n\n    Args:\n        L: Lower triangular matrix.\n\n    Returns:\n        Flat representation of the nonzero lower triangular matrix elements.\n    \"\"\"\n\n    num_params = L.shape[0]\n    tril_indices = torch.tril_indices(num_params, num_params)\n    L_flat = L[tril_indices[0], tril_indices[1]].clone()\n    return L_flat\n</code></pre>"},{"location":"api/utils/#posteriors.utils.cumulative_mean_and_cov","title":"<code>posteriors.utils.cumulative_mean_and_cov(xs)</code>","text":"<p>Compute the cumulative mean and covariance of a sequence of tensors.</p> <p>This is a numerically efficient way to calculate the cumulative covariances in particular. It costs O(n d^2) time compared to a naive O(n^2 d^2) implementation.</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Tensor</code> <p>Sequence of tensors.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of cumulative mean and covariance.</p> Source code in <code>posteriors/utils.py</code> <pre><code>def cumulative_mean_and_cov(xs: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the cumulative mean and covariance of a sequence of tensors.\n\n    This is a numerically efficient way to calculate the cumulative covariances\n    in particular. It costs O(n d^2) time compared to a naive O(n^2 d^2)\n    implementation.\n\n    Args:\n        xs: Sequence of tensors.\n\n    Returns:\n        Tuple of cumulative mean and covariance.\n    \"\"\"\n    n, d = xs.shape\n    out_means = torch.zeros((n, d))\n    out_covs = torch.zeros((n, d, d))\n\n    out_means[0] = xs[0]\n    out_covs[0] = torch.eye(d)\n\n    for i in range(1, n):\n        n = i + 1\n        # Update mean\n        out_means[i] = out_means[i - 1] * (n - 1) / n + xs[i] / n\n\n        # Update covariance\n        delta_n = xs[i] - out_means[i - 1]\n        out_covs[i] = (\n            out_covs[i - 1] * (n - 2) / (n - 1) + torch.outer(delta_n, delta_n) / n\n        )\n\n    return out_means, out_covs\n</code></pre>"},{"location":"api/ekf/dense_fisher/","title":"EKF Dense Fisher","text":""},{"location":"api/ekf/dense_fisher/#posteriors.ekf.dense_fisher.build","title":"<code>posteriors.ekf.dense_fisher.build(log_likelihood, lr, transition_cov=0.0, per_sample=False, init_cov=1.0)</code>","text":"<p>Builds a transform to implement an extended Kalman Filter update.</p> <p>EKF applies an online update to a Gaussian posterior over the parameters.</p> <p>The approximate Bayesian update is based on the linearization $$ \\log p(\u03b8 | y) \u2248 \\log p(\u03b8) +  \u03b5 g(\u03bc)\u1d40(\u03b8 - \u03bc) +  \\frac12 \u03b5 (\u03b8 - \u03bc)^T F(\u03bc) (\u03b8 - \u03bc) $$ where \\(\u03bc\\) is the mean of the prior distribution, \\(\u03b5\\) is the learning rate (or equivalently the likelihood inverse temperature), \\(g(\u03bc)\\) is the gradient of the log likelihood at \u03bc and \\(F(\u03bc)\\) is the empirical Fisher information matrix at \\(\u03bc\\) for data \\(y\\).</p> <p>For more information on extended Kalman filtering as well as an equivalence to (online) natural gradient descent see Ollivier, 2019.</p> <p>Parameters:</p> Name Type Description Default <code>log_likelihood</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log-likelihood value as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Inverse temperature of the update, which behaves like a learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>transition_cov</code> <code>Tensor | float</code> <p>Covariance of the transition noise, to additively inflate the covariance before the update.</p> <code>0.0</code> <code>per_sample</code> <code>bool</code> <p>If True, then log_likelihood is assumed to return a vector of log likelihoods for each sample in the batch. If False, then log_likelihood is assumed to return a scalar log likelihood for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_likelihood to be per sample.</p> <code>False</code> <code>init_cov</code> <code>Tensor | float</code> <p>Initial covariance of the Normal distribution. Can be torch.Tensor or scalar.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>EKF transform instance.</p> Source code in <code>posteriors/ekf/dense_fisher.py</code> <pre><code>def build(\n    log_likelihood: LogProbFn,\n    lr: float | Schedule,\n    transition_cov: torch.Tensor | float = 0.0,\n    per_sample: bool = False,\n    init_cov: torch.Tensor | float = 1.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform to implement an extended Kalman Filter update.\n\n    EKF applies an online update to a Gaussian posterior over the parameters.\n\n    The approximate Bayesian update is based on the linearization\n    $$\n    \\\\log p(\u03b8 | y) \u2248 \\\\log p(\u03b8) +  \u03b5 g(\u03bc)\u1d40(\u03b8 - \u03bc) +  \\\\frac12 \u03b5 (\u03b8 - \u03bc)^T F(\u03bc) (\u03b8 - \u03bc)\n    $$\n    where $\u03bc$ is the mean of the prior distribution, $\u03b5$ is the learning rate\n    (or equivalently the likelihood inverse temperature),\n    $g(\u03bc)$ is the gradient of the log likelihood at \u03bc and $F(\u03bc)$ is the\n    empirical Fisher information matrix at $\u03bc$ for data $y$.\n\n    For more information on extended Kalman filtering as well as an equivalence\n    to (online) natural gradient descent see [Ollivier, 2019](https://arxiv.org/abs/1703.00209).\n\n    Args:\n        log_likelihood: Function that takes parameters and input batch and\n            returns the log-likelihood value as well as auxiliary information,\n            e.g. from the model call.\n        lr: Inverse temperature of the update, which behaves like a learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        transition_cov: Covariance of the transition noise, to additively\n            inflate the covariance before the update.\n        per_sample: If True, then log_likelihood is assumed to return a vector of\n            log likelihoods for each sample in the batch. If False, then log_likelihood\n            is assumed to return a scalar log likelihood for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_likelihood to be per sample.\n        init_cov: Initial covariance of the Normal distribution. Can be torch.Tensor or scalar.\n\n    Returns:\n        EKF transform instance.\n    \"\"\"\n    init_fn = partial(init, init_cov=init_cov)\n    update_fn = partial(\n        update,\n        log_likelihood=log_likelihood,\n        lr=lr,\n        transition_cov=transition_cov,\n        per_sample=per_sample,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/ekf/dense_fisher/#posteriors.ekf.dense_fisher.EKFDenseState","title":"<code>posteriors.ekf.dense_fisher.EKFDenseState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a Normal distribution over parameters.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>cov</code> <code>Tensor</code> <p>Covariance matrix of the Normal distribution.</p> <code>log_likelihood</code> <code>Tensor</code> <p>Log likelihood of the data given the parameters.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/ekf/dense_fisher.py</code> <pre><code>class EKFDenseState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a Normal distribution over parameters.\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        cov: Covariance matrix of the\n            Normal distribution.\n        log_likelihood: Log likelihood of the data given the parameters.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    cov: torch.Tensor\n    log_likelihood: torch.Tensor = torch.tensor([])\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/ekf/dense_fisher/#posteriors.ekf.dense_fisher.init","title":"<code>posteriors.ekf.dense_fisher.init(params, init_cov=1.0)</code>","text":"<p>Initialise Multivariate Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Initial mean of the Normal distribution.</p> required <code>init_cov</code> <code>Tensor | float</code> <p>Initial covariance matrix of the Multivariate Normal distribution. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>EKFDenseState</code> <p>Initial EKFDenseState.</p> Source code in <code>posteriors/ekf/dense_fisher.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_cov: torch.Tensor | float = 1.0,\n) -&gt; EKFDenseState:\n    \"\"\"Initialise Multivariate Normal distribution over parameters.\n\n    Args:\n        params: Initial mean of the Normal distribution.\n        init_cov: Initial covariance matrix of the Multivariate Normal distribution.\n            If it is a float, it is defined as an identity matrix scaled by that float.\n\n    Returns:\n        Initial EKFDenseState.\n    \"\"\"\n    if is_scalar(init_cov):\n        num_params = tree_size(params)\n        init_cov = init_cov * torch.eye(num_params, requires_grad=False)\n\n    return EKFDenseState(params, init_cov)\n</code></pre>"},{"location":"api/ekf/dense_fisher/#posteriors.ekf.dense_fisher.update","title":"<code>posteriors.ekf.dense_fisher.update(state, batch, log_likelihood, lr, transition_cov=0.0, per_sample=False, inplace=False)</code>","text":"<p>Applies an extended Kalman Filter update to the Multivariate Normal distribution.</p> <p>See build for details.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>EKFDenseState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_likelihood.</p> required <code>log_likelihood</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log-likelihood value as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float</code> <p>Inverse temperature of the update, which behaves like a learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>transition_cov</code> <code>Tensor | float</code> <p>Covariance of the transition noise, to additively inflate the covariance before the update.</p> <code>0.0</code> <code>per_sample</code> <code>bool</code> <p>If True, then log_likelihood is assumed to return a vector of log likelihoods for each sample in the batch. If False, then log_likelihood is assumed to return a scalar log likelihood for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_likelihood to be per sample.</p> <code>False</code> <code>inplace</code> <code>bool</code> <p>Whether to update the state parameters in-place.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[EKFDenseState, TensorTree]</code> <p>Updated EKFDenseState and auxiliary information.</p> Source code in <code>posteriors/ekf/dense_fisher.py</code> <pre><code>def update(\n    state: EKFDenseState,\n    batch: Any,\n    log_likelihood: LogProbFn,\n    lr: float,\n    transition_cov: torch.Tensor | float = 0.0,\n    per_sample: bool = False,\n    inplace: bool = False,\n) -&gt; tuple[EKFDenseState, TensorTree]:\n    \"\"\"Applies an extended Kalman Filter update to the Multivariate Normal distribution.\n\n    See [build](dense_fisher.md#posteriors.ekf.dense_fisher.build) for details.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_likelihood.\n        log_likelihood: Function that takes parameters and input batch and\n            returns the log-likelihood value as well as auxiliary information,\n            e.g. from the model call.\n        lr: Inverse temperature of the update, which behaves like a learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        transition_cov: Covariance of the transition noise, to additively\n            inflate the covariance before the update.\n        per_sample: If True, then log_likelihood is assumed to return a vector of\n            log likelihoods for each sample in the batch. If False, then log_likelihood\n            is assumed to return a scalar log likelihood for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_likelihood to be per sample.\n        inplace: Whether to update the state parameters in-place.\n\n    Returns:\n        Updated EKFDenseState and auxiliary information.\n    \"\"\"\n    if not per_sample:\n        log_likelihood = per_samplify(log_likelihood)\n\n    lr = lr(state.step) if callable(lr) else lr\n\n    with torch.no_grad(), CatchAuxError():\n\n        def log_likelihood_reduced(params, batch):\n            per_samp_log_lik, internal_aux = log_likelihood(params, batch)\n            return per_samp_log_lik.mean(), internal_aux\n\n        grad, (log_liks, aux) = grad_and_value(log_likelihood_reduced, has_aux=True)(\n            state.params, batch\n        )\n        fisher, _ = empirical_fisher(\n            lambda p: log_likelihood(p, batch), has_aux=True, normalize=True\n        )(state.params)\n\n        predict_cov = state.cov + transition_cov\n        predict_cov_inv = torch.cholesky_inverse(torch.linalg.cholesky(predict_cov))\n        update_cov_inv = predict_cov_inv - lr * fisher\n        update_cov = torch.cholesky_inverse(torch.linalg.cholesky(update_cov_inv))\n\n        mu_raveled, mu_unravel_f = tree_ravel(state.params)\n        update_mean = mu_raveled + lr * update_cov @ tree_ravel(grad)[0]\n        update_mean = mu_unravel_f(update_mean)\n\n    if inplace:\n        tree_insert_(state.params, update_mean)\n        tree_insert_(state.cov, update_cov)\n        tree_insert_(state.log_likelihood, log_liks.mean().detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n\n    return EKFDenseState(\n        update_mean, update_cov, log_liks.mean().detach(), state.step + 1\n    ), aux\n</code></pre>"},{"location":"api/ekf/dense_fisher/#posteriors.ekf.dense_fisher.sample","title":"<code>posteriors.ekf.dense_fisher.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Single sample from Multivariate Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>EKFDenseState</code> <p>State encoding mean and covariance.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from Multivariate Normal distribution.</p> Source code in <code>posteriors/ekf/dense_fisher.py</code> <pre><code>def sample(\n    state: EKFDenseState, sample_shape: torch.Size = torch.Size([])\n) -&gt; TensorTree:\n    \"\"\"Single sample from Multivariate Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and covariance.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from Multivariate Normal distribution.\n    \"\"\"\n    mean_flat, unravel_func = tree_ravel(state.params)\n\n    samples = torch.distributions.MultivariateNormal(\n        loc=mean_flat,\n        covariance_matrix=state.cov,\n        validate_args=False,\n    ).sample(sample_shape)\n\n    samples = torch.vmap(unravel_func)(samples)\n    return samples\n</code></pre>"},{"location":"api/ekf/diag_fisher/","title":"EKF Diagonal Fisher","text":""},{"location":"api/ekf/diag_fisher/#posteriors.ekf.diag_fisher.build","title":"<code>posteriors.ekf.diag_fisher.build(log_likelihood, lr, transition_sd=0.0, per_sample=False, init_sds=1.0)</code>","text":"<p>Builds a transform to implement an extended Kalman Filter update.</p> <p>EKF applies an online update to a (diagonal) Gaussian posterior over the parameters.</p> <p>The approximate Bayesian update is based on the linearization $$ \\log p(\u03b8 | y) \u2248 \\log p(\u03b8) +  \u03b5 g(\u03bc)\u1d40(\u03b8 - \u03bc) +  \\frac12 \u03b5 (\u03b8 - \u03bc)^T F_d(\u03bc) (\u03b8 - \u03bc) $$ where \\(\u03bc\\) is the mean of the prior distribution, \\(\u03b5\\) is the learning rate (or equivalently the likelihood inverse temperature), \\(g(\u03bc)\\) is the gradient of the log likelihood at \u03bc and \\(F_d(\u03bc)\\) is the diagonal empirical Fisher information matrix at \\(\u03bc\\) for data \\(y\\). Completing the square regains a diagonal Normal distribution over the parameters.</p> <p>For more information on extended Kalman filtering as well as an equivalence to (online) natural gradient descent see Ollivier, 2019.</p> <p>Parameters:</p> Name Type Description Default <code>log_likelihood</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log-likelihood value as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Inverse temperature of the update, which behaves like a learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>transition_sd</code> <code>float</code> <p>Standard deviation of the transition noise, to additively inflate the diagonal covariance before the update.</p> <code>0.0</code> <code>per_sample</code> <code>bool</code> <p>If True, then log_likelihood is assumed to return a vector of log likelihoods for each sample in the batch. If False, then log_likelihood is assumed to return a scalar log likelihood for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_likelihood to be per sample.</p> <code>False</code> <code>init_sds</code> <code>TensorTree | float</code> <p>Initial square-root diagonal of the covariance matrix of the Normal distribution. Can be tree like params or scalar.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Diagonal EKF transform instance.</p> Source code in <code>posteriors/ekf/diag_fisher.py</code> <pre><code>def build(\n    log_likelihood: LogProbFn,\n    lr: float | Schedule,\n    transition_sd: float = 0.0,\n    per_sample: bool = False,\n    init_sds: TensorTree | float = 1.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform to implement an extended Kalman Filter update.\n\n    EKF applies an online update to a (diagonal) Gaussian posterior over the parameters.\n\n    The approximate Bayesian update is based on the linearization\n    $$\n    \\\\log p(\u03b8 | y) \u2248 \\\\log p(\u03b8) +  \u03b5 g(\u03bc)\u1d40(\u03b8 - \u03bc) +  \\\\frac12 \u03b5 (\u03b8 - \u03bc)^T F_d(\u03bc) (\u03b8 - \u03bc)\n    $$\n    where $\u03bc$ is the mean of the prior distribution, $\u03b5$ is the learning rate\n    (or equivalently the likelihood inverse temperature),\n    $g(\u03bc)$ is the gradient of the log likelihood at \u03bc and $F_d(\u03bc)$ is the diagonal\n    empirical Fisher information matrix at $\u03bc$ for data $y$. Completing the square\n    regains a diagonal Normal distribution over the parameters.\n\n    For more information on extended Kalman filtering as well as an equivalence\n    to (online) natural gradient descent see [Ollivier, 2019](https://arxiv.org/abs/1703.00209).\n\n    Args:\n        log_likelihood: Function that takes parameters and input batch and\n            returns the log-likelihood value as well as auxiliary information,\n            e.g. from the model call.\n        lr: Inverse temperature of the update, which behaves like a learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        transition_sd: Standard deviation of the transition noise, to additively\n            inflate the diagonal covariance before the update.\n        per_sample: If True, then log_likelihood is assumed to return a vector of\n            log likelihoods for each sample in the batch. If False, then log_likelihood\n            is assumed to return a scalar log likelihood for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_likelihood to be per sample.\n        init_sds: Initial square-root diagonal of the covariance matrix\n            of the Normal distribution. Can be tree like params or scalar.\n\n    Returns:\n        Diagonal EKF transform instance.\n    \"\"\"\n    init_fn = partial(init, init_sds=init_sds)\n    update_fn = partial(\n        update,\n        log_likelihood=log_likelihood,\n        lr=lr,\n        transition_sd=transition_sd,\n        per_sample=per_sample,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/ekf/diag_fisher/#posteriors.ekf.diag_fisher.EKFDiagState","title":"<code>posteriors.ekf.diag_fisher.EKFDiagState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a diagonal Normal distribution over parameters.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>sd_diag</code> <code>TensorTree</code> <p>Square-root diagonal of the covariance matrix of the Normal distribution.</p> <code>log_likelihood</code> <code>Tensor</code> <p>Log likelihood of the data given the parameters.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/ekf/diag_fisher.py</code> <pre><code>class EKFDiagState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a diagonal Normal distribution over parameters.\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        sd_diag: Square-root diagonal of the covariance matrix of the\n            Normal distribution.\n        log_likelihood: Log likelihood of the data given the parameters.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    sd_diag: TensorTree\n    log_likelihood: torch.Tensor = torch.tensor([])\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/ekf/diag_fisher/#posteriors.ekf.diag_fisher.init","title":"<code>posteriors.ekf.diag_fisher.init(params, init_sds=1.0)</code>","text":"<p>Initialise diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Initial mean of the Normal distribution.</p> required <code>init_sds</code> <code>TensorTree | float</code> <p>Initial square-root diagonal of the covariance matrix of the Normal distribution. Can be tree like params or scalar.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>EKFDiagState</code> <p>Initial EKFDiagState.</p> Source code in <code>posteriors/ekf/diag_fisher.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_sds: TensorTree | float = 1.0,\n) -&gt; EKFDiagState:\n    \"\"\"Initialise diagonal Normal distribution over parameters.\n\n    Args:\n        params: Initial mean of the Normal distribution.\n        init_sds: Initial square-root diagonal of the covariance matrix\n            of the Normal distribution. Can be tree like params or scalar.\n\n    Returns:\n        Initial EKFDiagState.\n    \"\"\"\n    if is_scalar(init_sds):\n        init_sds = tree_map(\n            lambda x: torch.full_like(x, init_sds, requires_grad=x.requires_grad),\n            params,\n        )\n\n    return EKFDiagState(params, init_sds)\n</code></pre>"},{"location":"api/ekf/diag_fisher/#posteriors.ekf.diag_fisher.update","title":"<code>posteriors.ekf.diag_fisher.update(state, batch, log_likelihood, lr, transition_sd=0.0, per_sample=False, inplace=False)</code>","text":"<p>Applies an extended Kalman Filter update to the diagonal Normal distribution.</p> <p>See build for details.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>EKFDiagState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_likelihood.</p> required <code>log_likelihood</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log-likelihood value as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Inverse temperature of the update, which behaves like a learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>transition_sd</code> <code>float</code> <p>Standard deviation of the transition noise, to additively inflate the diagonal covariance before the update.</p> <code>0.0</code> <code>per_sample</code> <code>bool</code> <p>If True, then log_likelihood is assumed to return a vector of log likelihoods for each sample in the batch. If False, then log_likelihood is assumed to return a scalar log likelihood for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_likelihood to be per sample.</p> <code>False</code> <code>inplace</code> <code>bool</code> <p>Whether to update the state parameters in-place.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[EKFDiagState, TensorTree]</code> <p>Updated EKFDiagState and auxiliary information.</p> Source code in <code>posteriors/ekf/diag_fisher.py</code> <pre><code>def update(\n    state: EKFDiagState,\n    batch: Any,\n    log_likelihood: LogProbFn,\n    lr: float | Schedule,\n    transition_sd: float = 0.0,\n    per_sample: bool = False,\n    inplace: bool = False,\n) -&gt; tuple[EKFDiagState, TensorTree]:\n    \"\"\"Applies an extended Kalman Filter update to the diagonal Normal distribution.\n\n    See [build](diag_fisher.md#posteriors.ekf.diag_fisher.build) for details.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_likelihood.\n        log_likelihood: Function that takes parameters and input batch and\n            returns the log-likelihood value as well as auxiliary information,\n            e.g. from the model call.\n        lr: Inverse temperature of the update, which behaves like a learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        transition_sd: Standard deviation of the transition noise, to additively\n            inflate the diagonal covariance before the update.\n        per_sample: If True, then log_likelihood is assumed to return a vector of\n            log likelihoods for each sample in the batch. If False, then log_likelihood\n            is assumed to return a scalar log likelihood for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_likelihood to be per sample.\n        inplace: Whether to update the state parameters in-place.\n\n    Returns:\n        Updated EKFDiagState and auxiliary information.\n    \"\"\"\n\n    if not per_sample:\n        log_likelihood = per_samplify(log_likelihood)\n\n    lr = lr(state.step) if callable(lr) else lr\n\n    predict_sd_diag = flexi_tree_map(\n        lambda x: (x**2 + transition_sd**2) ** 0.5, state.sd_diag, inplace=inplace\n    )\n    with torch.no_grad(), CatchAuxError():\n        log_liks, aux = log_likelihood(state.params, batch)\n        jac, _ = jacrev(log_likelihood, has_aux=True)(state.params, batch)\n        grad = tree_map(lambda x: x.mean(0), jac)\n        diag_lik_hessian_approx = tree_map(lambda x: -(x**2).mean(0), jac)\n\n    update_sd_diag = flexi_tree_map(\n        lambda sig, h: (sig**-2 - lr * h) ** -0.5,\n        predict_sd_diag,\n        diag_lik_hessian_approx,\n        inplace=inplace,\n    )\n    update_mean = flexi_tree_map(\n        lambda mu, sig, g: mu + sig**2 * lr * g,\n        state.params,\n        update_sd_diag,\n        grad,\n        inplace=inplace,\n    )\n\n    if inplace:\n        tree_insert_(state.log_likelihood, log_liks.mean().detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n\n    return EKFDiagState(\n        update_mean, update_sd_diag, log_liks.mean().detach(), state.step + 1\n    ), aux\n</code></pre>"},{"location":"api/ekf/diag_fisher/#posteriors.ekf.diag_fisher.sample","title":"<code>posteriors.ekf.diag_fisher.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Single sample from diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>EKFDiagState</code> <p>State encoding mean and standard deviations.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from Normal distribution.</p> Source code in <code>posteriors/ekf/diag_fisher.py</code> <pre><code>def sample(\n    state: EKFDiagState, sample_shape: torch.Size = torch.Size([])\n) -&gt; TensorTree:\n    \"\"\"Single sample from diagonal Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and standard deviations.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from Normal distribution.\n    \"\"\"\n    return diag_normal_sample(state.params, state.sd_diag, sample_shape=sample_shape)\n</code></pre>"},{"location":"api/laplace/dense_fisher/","title":"Laplace Dense Fisher","text":""},{"location":"api/laplace/dense_fisher/#posteriors.laplace.dense_fisher.build","title":"<code>posteriors.laplace.dense_fisher.build(log_posterior, per_sample=False, init_prec=0.0)</code>","text":"<p>Builds a transform for dense empirical Fisher information Laplace approximation.</p> <p>The empirical Fisher is defined here as: $$ F(\u03b8) = \\sum_i \u2207_\u03b8 \\log p(y_i, \u03b8 | x_i) \u2207_\u03b8 \\log p(y_i, \u03b8 | x_i)^T $$ where \\(p(y_i, \u03b8 | x_i)\\) is the joint model distribution (equivalent to the posterior up to proportionality) with parameters \\(\u03b8\\), inputs \\(x_i\\) and labels \\(y_i\\).</p> <p>More info on empirical Fisher matrices can be found in Martens, 2020 and their use within a Laplace approximation in Daxberger et al, 2021.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>per_sample</code> <code>bool</code> <p>If True, then log_posterior is assumed to return a vector of log posteriors for each sample in the batch. If False, then log_posterior is assumed to return a scalar log posterior for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_posterior to be per sample.</p> <code>False</code> <code>init_prec</code> <code>Tensor | float</code> <p>Initial precision matrix. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Empirical Fisher information Laplace approximation transform instance.</p> Source code in <code>posteriors/laplace/dense_fisher.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    per_sample: bool = False,\n    init_prec: torch.Tensor | float = 0.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for dense empirical Fisher information\n    Laplace approximation.\n\n    The empirical Fisher is defined here as:\n    $$\n    F(\u03b8) = \\\\sum_i \u2207_\u03b8 \\\\log p(y_i, \u03b8 | x_i) \u2207_\u03b8 \\\\log p(y_i, \u03b8 | x_i)^T\n    $$\n    where $p(y_i, \u03b8 | x_i)$ is the joint model distribution (equivalent to the posterior\n    up to proportionality) with parameters $\u03b8$, inputs $x_i$ and labels $y_i$.\n\n    More info on empirical Fisher matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf) and\n    their use within a Laplace approximation in [Daxberger et al, 2021](https://arxiv.org/abs/2106.14806).\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        per_sample: If True, then log_posterior is assumed to return a vector of\n            log posteriors for each sample in the batch. If False, then log_posterior\n            is assumed to return a scalar log posterior for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_posterior to be per sample.\n        init_prec: Initial precision matrix.\n            If it is a float, it is defined as an identity matrix\n            scaled by that float.\n\n    Returns:\n        Empirical Fisher information Laplace approximation transform instance.\n    \"\"\"\n    init_fn = partial(init, init_prec=init_prec)\n    update_fn = partial(update, log_posterior=log_posterior, per_sample=per_sample)\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/laplace/dense_fisher/#posteriors.laplace.dense_fisher.DenseLaplaceState","title":"<code>posteriors.laplace.dense_fisher.DenseLaplaceState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a Normal distribution over parameters, with a dense precision matrix</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>prec</code> <code>Tensor</code> <p>Precision matrix of the Normal distribution.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/laplace/dense_fisher.py</code> <pre><code>class DenseLaplaceState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a Normal distribution over parameters,\n    with a dense precision matrix\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        prec: Precision matrix of the Normal distribution.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    prec: torch.Tensor\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/laplace/dense_fisher/#posteriors.laplace.dense_fisher.init","title":"<code>posteriors.laplace.dense_fisher.init(params, init_prec=0.0)</code>","text":"<p>Initialise Normal distribution over parameters with a dense precision matrix.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> required <code>init_prec</code> <code>Tensor | float</code> <p>Initial precision matrix. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DenseLaplaceState</code> <p>Initial DenseLaplaceState.</p> Source code in <code>posteriors/laplace/dense_fisher.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_prec: torch.Tensor | float = 0.0,\n) -&gt; DenseLaplaceState:\n    \"\"\"Initialise Normal distribution over parameters\n    with a dense precision matrix.\n\n    Args:\n        params: Mean of the Normal distribution.\n        init_prec: Initial precision matrix.\n            If it is a float, it is defined as an identity matrix\n            scaled by that float.\n\n    Returns:\n        Initial DenseLaplaceState.\n    \"\"\"\n\n    if is_scalar(init_prec):\n        num_params = tree_size(params)\n        init_prec = init_prec * torch.eye(num_params, requires_grad=False)\n\n    return DenseLaplaceState(params, init_prec)\n</code></pre>"},{"location":"api/laplace/dense_fisher/#posteriors.laplace.dense_fisher.update","title":"<code>posteriors.laplace.dense_fisher.update(state, batch, log_posterior, per_sample=False, inplace=False)</code>","text":"<p>Adds empirical Fisher information matrix of covariance summed over given batch.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DenseLaplaceState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised)</p> required <code>per_sample</code> <code>bool</code> <p>If True, then log_posterior is assumed to return a vector of log posteriors for each sample in the batch. If False, then log_posterior is assumed to return a scalar log posterior for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_posterior to be per sample.</p> <code>False</code> <code>inplace</code> <code>bool</code> <p>If True, the state is updated in place. Otherwise, a new state is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DenseLaplaceState, TensorTree]</code> <p>Updated DenseLaplaceState and auxiliary information.</p> Source code in <code>posteriors/laplace/dense_fisher.py</code> <pre><code>def update(\n    state: DenseLaplaceState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    per_sample: bool = False,\n    inplace: bool = False,\n) -&gt; tuple[DenseLaplaceState, TensorTree]:\n    \"\"\"Adds empirical Fisher information matrix of covariance summed over\n    given batch.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n        per_sample: If True, then log_posterior is assumed to return a vector of\n            log posteriors for each sample in the batch. If False, then log_posterior\n            is assumed to return a scalar log posterior for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_posterior to be per sample.\n        inplace: If True, the state is updated in place. Otherwise, a new\n            state is returned.\n\n    Returns:\n        Updated DenseLaplaceState and auxiliary information.\n    \"\"\"\n    if not per_sample:\n        log_posterior = per_samplify(log_posterior)\n\n    with torch.no_grad(), CatchAuxError():\n        fisher, aux = empirical_fisher(\n            lambda p: log_posterior(p, batch), has_aux=True, normalize=False\n        )(state.params)\n\n    if inplace:\n        state.prec.data += fisher\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    else:\n        return DenseLaplaceState(state.params, state.prec + fisher, state.step + 1), aux\n</code></pre>"},{"location":"api/laplace/dense_fisher/#posteriors.laplace.dense_fisher.sample","title":"<code>posteriors.laplace.dense_fisher.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Sample from Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DenseLaplaceState</code> <p>State encoding mean and precision matrix.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from the Normal distribution.</p> Source code in <code>posteriors/laplace/dense_fisher.py</code> <pre><code>def sample(\n    state: DenseLaplaceState,\n    sample_shape: torch.Size = torch.Size([]),\n) -&gt; TensorTree:\n    \"\"\"Sample from Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and precision matrix.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from the Normal distribution.\n    \"\"\"\n    samples = torch.distributions.MultivariateNormal(\n        loc=torch.zeros(state.prec.shape[0], device=state.prec.device),\n        precision_matrix=state.prec,\n        validate_args=False,\n    ).sample(sample_shape)\n    samples = samples.flatten(end_dim=-2)  # ensure samples is 2D\n    mean_flat, unravel_func = tree_ravel(state.params)\n    samples += mean_flat\n    samples = torch.vmap(unravel_func)(samples)\n    samples = tree_map(lambda x: x.reshape(sample_shape + x.shape[-1:]), samples)\n    return samples\n</code></pre>"},{"location":"api/laplace/dense_ggn/","title":"Laplace Dense GGN","text":""},{"location":"api/laplace/dense_ggn/#posteriors.laplace.dense_ggn.build","title":"<code>posteriors.laplace.dense_ggn.build(forward, outer_log_likelihood, init_prec=0.0)</code>","text":"<p>Builds a transform for a Generalized Gauss-Newton (GGN) Laplace approximation.</p> <p>Equivalent to the (non-empirical) Fisher information matrix when the <code>outer_log_likelihood</code> is exponential family with natural parameter equal to the output from <code>forward</code>.</p> <p><code>forward</code> should output auxiliary information (or <code>torch.tensor([])</code>), <code>outer_log_likelihood</code> should not.</p> <p>The GGN is defined as $$ G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T $$ where \\(z = f(\u03b8)\\) is the output of the forward function \\(f\\) and \\(l(z)\\) is a loss (negative log-likelihood) that maps the output of \\(f\\) to a scalar output.</p> <p>More info on Fisher and GGN matrices can be found in Martens, 2020 and their use within a Laplace approximation in Daxberger et al, 2021.</p> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>ForwardFn</code> <p>Function that takes parameters and input batch and returns a forward value (e.g. logits), not reduced over the batch, as well as auxiliary information.</p> required <code>outer_log_likelihood</code> <code>OuterLogProbFn</code> <p>A function that takes the output of <code>forward</code> and batch then returns the log likelihood of the model output, with no auxiliary information.</p> required <code>init_prec</code> <code>TensorTree | float</code> <p>Initial precision matrix. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>GGN Laplace approximation transform instance.</p> Source code in <code>posteriors/laplace/dense_ggn.py</code> <pre><code>def build(\n    forward: ForwardFn,\n    outer_log_likelihood: OuterLogProbFn,\n    init_prec: TensorTree | float = 0.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for a Generalized Gauss-Newton (GGN)\n    Laplace approximation.\n\n    Equivalent to the (non-empirical) Fisher information matrix when\n    the `outer_log_likelihood` is exponential family with natural parameter equal to\n    the output from `forward`.\n\n    `forward` should output auxiliary information (or `torch.tensor([])`),\n    `outer_log_likelihood` should not.\n\n    The GGN is defined as\n    $$\n    G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T\n    $$\n    where $z = f(\u03b8)$ is the output of the forward function $f$ and $l(z)$\n    is a loss (negative log-likelihood) that maps the output of $f$ to a scalar output.\n\n    More info on Fisher and GGN matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf) and\n    their use within a Laplace approximation in [Daxberger et al, 2021](https://arxiv.org/abs/2106.14806).\n\n    Args:\n        forward: Function that takes parameters and input batch and\n            returns a forward value (e.g. logits), not reduced over the batch,\n            as well as auxiliary information.\n        outer_log_likelihood: A function that takes the output of `forward` and batch\n            then returns the log likelihood of the model output,\n            with no auxiliary information.\n        init_prec: Initial precision matrix.\n            If it is a float, it is defined as an identity matrix\n            scaled by that float.\n\n    Returns:\n        GGN Laplace approximation transform instance.\n    \"\"\"\n    init_fn = partial(init, init_prec=init_prec)\n    update_fn = partial(\n        update, forward=forward, outer_log_likelihood=outer_log_likelihood\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/laplace/dense_ggn/#posteriors.laplace.dense_ggn.DenseLaplaceState","title":"<code>posteriors.laplace.dense_ggn.DenseLaplaceState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a Normal distribution over parameters, with a dense precision matrix</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>prec</code> <code>Tensor</code> <p>Precision matrix of the Normal distribution.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/laplace/dense_ggn.py</code> <pre><code>class DenseLaplaceState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a Normal distribution over parameters,\n    with a dense precision matrix\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        prec: Precision matrix of the Normal distribution.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    prec: torch.Tensor\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/laplace/dense_ggn/#posteriors.laplace.dense_ggn.init","title":"<code>posteriors.laplace.dense_ggn.init(params, init_prec=0.0)</code>","text":"<p>Initialise Normal distribution over parameters with a dense precision matrix.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> required <code>init_prec</code> <code>Tensor | float</code> <p>Initial precision matrix. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DenseLaplaceState</code> <p>Initial DenseLaplaceState.</p> Source code in <code>posteriors/laplace/dense_ggn.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_prec: torch.Tensor | float = 0.0,\n) -&gt; DenseLaplaceState:\n    \"\"\"Initialise Normal distribution over parameters\n    with a dense precision matrix.\n\n    Args:\n        params: Mean of the Normal distribution.\n        init_prec: Initial precision matrix.\n            If it is a float, it is defined as an identity matrix\n            scaled by that float.\n\n    Returns:\n        Initial DenseLaplaceState.\n    \"\"\"\n\n    if is_scalar(init_prec):\n        num_params = tree_size(params)\n        init_prec = init_prec * torch.eye(num_params, requires_grad=False)\n\n    return DenseLaplaceState(params, init_prec)\n</code></pre>"},{"location":"api/laplace/dense_ggn/#posteriors.laplace.dense_ggn.update","title":"<code>posteriors.laplace.dense_ggn.update(state, batch, forward, outer_log_likelihood, inplace=False)</code>","text":"<p>Adds GGN matrix over given batch.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DenseLaplaceState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to model.</p> required <code>forward</code> <code>ForwardFn</code> <p>Function that takes parameters and input batch and returns a forward value (e.g. logits), not reduced over the batch, as well as auxiliary information.</p> required <code>outer_log_likelihood</code> <code>OuterLogProbFn</code> <p>A function that takes the output of <code>forward</code> and batch then returns the log likelihood of the model output, with no auxiliary information.</p> required <code>inplace</code> <code>bool</code> <p>If True, then the state is updated in place, otherwise a new state is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DenseLaplaceState, TensorTree]</code> <p>Updated DenseLaplaceState and auxiliary information.</p> Source code in <code>posteriors/laplace/dense_ggn.py</code> <pre><code>def update(\n    state: DenseLaplaceState,\n    batch: Any,\n    forward: ForwardFn,\n    outer_log_likelihood: OuterLogProbFn,\n    inplace: bool = False,\n) -&gt; tuple[DenseLaplaceState, TensorTree]:\n    \"\"\"Adds GGN matrix over given batch.\n\n    Args:\n        state: Current state.\n        batch: Input data to model.\n        forward: Function that takes parameters and input batch and\n            returns a forward value (e.g. logits), not reduced over the batch,\n            as well as auxiliary information.\n        outer_log_likelihood: A function that takes the output of `forward` and batch\n            then returns the log likelihood of the model output,\n            with no auxiliary information.\n        inplace: If True, then the state is updated in place, otherwise a new state\n            is returned.\n\n    Returns:\n        Updated DenseLaplaceState and auxiliary information.\n    \"\"\"\n\n    def outer_loss(z, batch):\n        return -outer_log_likelihood(z, batch)\n\n    with torch.no_grad(), CatchAuxError():\n        ggn_batch, aux = ggn(\n            lambda params: forward(params, batch),\n            lambda z: outer_loss(z, batch),\n            forward_has_aux=True,\n            loss_has_aux=False,\n            normalize=False,\n        )(state.params)\n\n    if inplace:\n        state.prec.data += ggn_batch\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    else:\n        return DenseLaplaceState(\n            state.params, state.prec + ggn_batch, state.step + 1\n        ), aux\n</code></pre>"},{"location":"api/laplace/dense_ggn/#posteriors.laplace.dense_ggn.sample","title":"<code>posteriors.laplace.dense_ggn.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Sample from Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DenseLaplaceState</code> <p>State encoding mean and precision matrix.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from the Normal distribution.</p> Source code in <code>posteriors/laplace/dense_ggn.py</code> <pre><code>def sample(\n    state: DenseLaplaceState,\n    sample_shape: torch.Size = torch.Size([]),\n) -&gt; TensorTree:\n    \"\"\"Sample from Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and precision matrix.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from the Normal distribution.\n    \"\"\"\n    samples = torch.distributions.MultivariateNormal(\n        loc=torch.zeros(state.prec.shape[0], device=state.prec.device),\n        precision_matrix=state.prec,\n        validate_args=False,\n    ).sample(sample_shape)\n    samples = samples.flatten(end_dim=-2)  # ensure samples is 2D\n    mean_flat, unravel_func = tree_ravel(state.params)\n    samples += mean_flat\n    samples = torch.vmap(unravel_func)(samples)\n    samples = tree_map(lambda x: x.reshape(sample_shape + x.shape[-1:]), samples)\n    return samples\n</code></pre>"},{"location":"api/laplace/dense_hessian/","title":"Laplace Dense Hessian","text":""},{"location":"api/laplace/dense_hessian/#posteriors.laplace.dense_hessian.build","title":"<code>posteriors.laplace.dense_hessian.build(log_posterior, init_prec=0.0, epsilon=0.0, rescale=1.0)</code>","text":"<p>Builds a transform for dense Hessian Laplace.</p> <p>Warning: The Hessian is not guaranteed to be positive definite, so setting epsilon &gt; 0 ought to be considered.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>init_prec</code> <code>Tensor | float</code> <p>Initial precision matrix. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>0.0</code> <code>epsilon</code> <code>float</code> <p>Added to the diagonal of the Hessian for numerical stability.</p> <code>0.0</code> <code>rescale</code> <code>float</code> <p>Value to multiply the Hessian by (i.e. to normalize by batch size)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Hessian Laplace transform instance.</p> Source code in <code>posteriors/laplace/dense_hessian.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    init_prec: torch.Tensor | float = 0.0,\n    epsilon: float = 0.0,\n    rescale: float = 1.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for dense Hessian Laplace.\n\n    **Warning:**\n    The Hessian is not guaranteed to be positive definite,\n    so setting epsilon &gt; 0 ought to be considered.\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        init_prec: Initial precision matrix.\n            If it is a float, it is defined as an identity matrix\n            scaled by that float.\n        epsilon: Added to the diagonal of the Hessian\n            for numerical stability.\n        rescale: Value to multiply the Hessian by\n            (i.e. to normalize by batch size)\n\n    Returns:\n        Hessian Laplace transform instance.\n    \"\"\"\n    init_fn = partial(init, init_prec=init_prec)\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        epsilon=epsilon,\n        rescale=rescale,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/laplace/dense_hessian/#posteriors.laplace.dense_hessian.DenseLaplaceState","title":"<code>posteriors.laplace.dense_hessian.DenseLaplaceState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a Normal distribution over parameters, with a dense precision matrix</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>prec</code> <code>Tensor</code> <p>Precision matrix of the Normal distribution.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/laplace/dense_hessian.py</code> <pre><code>class DenseLaplaceState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a Normal distribution over parameters,\n    with a dense precision matrix\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        prec: Precision matrix of the Normal distribution.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    prec: torch.Tensor\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/laplace/dense_hessian/#posteriors.laplace.dense_hessian.init","title":"<code>posteriors.laplace.dense_hessian.init(params, init_prec=0.0)</code>","text":"<p>Initialise Normal distribution over parameters with a dense precision matrix.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> required <code>init_prec</code> <code>Tensor | float</code> <p>Initial precision matrix. If it is a float, it is defined as an identity matrix scaled by that float.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DenseLaplaceState</code> <p>Initial DenseLaplaceState.</p> Source code in <code>posteriors/laplace/dense_hessian.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_prec: torch.Tensor | float = 0.0,\n) -&gt; DenseLaplaceState:\n    \"\"\"Initialise Normal distribution over parameters\n    with a dense precision matrix.\n\n    Args:\n        params: Mean of the Normal distribution.\n        init_prec: Initial precision matrix.\n            If it is a float, it is defined as an identity matrix\n            scaled by that float.\n\n    Returns:\n        Initial DenseLaplaceState.\n    \"\"\"\n\n    if is_scalar(init_prec):\n        num_params = tree_size(params)\n        init_prec = init_prec * torch.eye(num_params, requires_grad=False)\n\n    return DenseLaplaceState(params, init_prec)\n</code></pre>"},{"location":"api/laplace/dense_hessian/#posteriors.laplace.dense_hessian.update","title":"<code>posteriors.laplace.dense_hessian.update(state, batch, log_posterior, epsilon=0.0, rescale=1.0, inplace=False)</code>","text":"<p>Adds the Hessian of the negative log-posterior over given batch.</p> <p>Warning: The Hessian is not guaranteed to be positive definite, so setting epsilon &gt; 0 ought to be considered.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DenseLaplaceState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised)</p> required <code>epsilon</code> <code>float</code> <p>Added to the diagonal of the Hessian for numerical stability.</p> <code>0.0</code> <code>rescale</code> <code>float</code> <p>Value to multiply the Hessian by (i.e. to normalize by batch size)</p> <code>1.0</code> <code>inplace</code> <code>bool</code> <p>If True, the state is updated in place. Otherwise, a new state is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DenseLaplaceState, TensorTree]</code> <p>Updated DenseLaplaceState and auxiliary information.</p> Source code in <code>posteriors/laplace/dense_hessian.py</code> <pre><code>def update(\n    state: DenseLaplaceState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    epsilon: float = 0.0,\n    rescale: float = 1.0,\n    inplace: bool = False,\n) -&gt; tuple[DenseLaplaceState, TensorTree]:\n    \"\"\"Adds the Hessian of the negative log-posterior over given batch.\n\n    **Warning:**\n    The Hessian is not guaranteed to be positive definite,\n    so setting epsilon &gt; 0 ought to be considered.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n        epsilon: Added to the diagonal of the Hessian\n            for numerical stability.\n        rescale: Value to multiply the Hessian by\n            (i.e. to normalize by batch size)\n        inplace: If True, the state is updated in place. Otherwise, a new\n            state is returned.\n\n    Returns:\n        Updated DenseLaplaceState and auxiliary information.\n    \"\"\"\n    with torch.no_grad(), CatchAuxError():\n        flat_params, params_unravel = tree_ravel(state.params)\n        num_params = flat_params.numel()\n\n        def neg_log_p(p_flat):\n            value, aux = log_posterior(params_unravel(p_flat), batch)\n            return -value, aux\n\n        hess, aux = jacfwd(jacrev(neg_log_p, has_aux=True), has_aux=True)(flat_params)\n        hess = hess * rescale + epsilon * torch.eye(num_params)\n\n    if inplace:\n        state.prec.data += hess\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    else:\n        return DenseLaplaceState(state.params, state.prec + hess, state.step + 1), aux\n</code></pre>"},{"location":"api/laplace/dense_hessian/#posteriors.laplace.dense_hessian.sample","title":"<code>posteriors.laplace.dense_hessian.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Sample from Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DenseLaplaceState</code> <p>State encoding mean and precision matrix.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from the Normal distribution.</p> Source code in <code>posteriors/laplace/dense_hessian.py</code> <pre><code>def sample(\n    state: DenseLaplaceState,\n    sample_shape: torch.Size = torch.Size([]),\n) -&gt; TensorTree:\n    \"\"\"Sample from Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and precision matrix.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from the Normal distribution.\n    \"\"\"\n    samples = torch.distributions.MultivariateNormal(\n        loc=torch.zeros(state.prec.shape[0], device=state.prec.device),\n        precision_matrix=state.prec,\n        validate_args=False,\n    ).sample(sample_shape)\n    samples = samples.flatten(end_dim=-2)  # ensure samples is 2D\n    mean_flat, unravel_func = tree_ravel(state.params)\n    samples += mean_flat\n    samples = torch.vmap(unravel_func)(samples)\n    samples = tree_map(lambda x: x.reshape(sample_shape + x.shape[-1:]), samples)\n    return samples\n</code></pre>"},{"location":"api/laplace/diag_fisher/","title":"Laplace Diagonal Fisher","text":""},{"location":"api/laplace/diag_fisher/#posteriors.laplace.diag_fisher.build","title":"<code>posteriors.laplace.diag_fisher.build(log_posterior, per_sample=False, init_prec_diag=0.0)</code>","text":"<p>Builds a transform for diagonal empirical Fisher information Laplace approximation.</p> <p>The empirical Fisher is defined here as: $$ F(\u03b8) = \\sum_i \u2207_\u03b8 \\log p(y_i, \u03b8 | x_i) \u2207_\u03b8 \\log p(y_i, \u03b8 | x_i)^T $$ where \\(p(y_i, \u03b8 | x_i)\\) is the joint model distribution (equivalent to the posterior up to proportionality) with parameters \\(\u03b8\\), inputs \\(x_i\\) and labels \\(y_i\\).</p> <p>More info on empirical Fisher matrices can be found in Martens, 2020 and their use within a Laplace approximation in Daxberger et al, 2021.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>per_sample</code> <code>bool</code> <p>If True, then log_posterior is assumed to return a vector of log posteriors for each sample in the batch. If False, then log_posterior is assumed to return a scalar log posterior for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_posterior to be per sample.</p> <code>False</code> <code>init_prec_diag</code> <code>TensorTree | float</code> <p>Initial diagonal precision matrix. Can be tree like params or scalar.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Diagonal empirical Fisher information Laplace approximation transform instance.</p> Source code in <code>posteriors/laplace/diag_fisher.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    per_sample: bool = False,\n    init_prec_diag: TensorTree | float = 0.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for diagonal empirical Fisher information\n    Laplace approximation.\n\n    The empirical Fisher is defined here as:\n    $$\n    F(\u03b8) = \\\\sum_i \u2207_\u03b8 \\\\log p(y_i, \u03b8 | x_i) \u2207_\u03b8 \\\\log p(y_i, \u03b8 | x_i)^T\n    $$\n    where $p(y_i, \u03b8 | x_i)$ is the joint model distribution (equivalent to the posterior\n    up to proportionality) with parameters $\u03b8$, inputs $x_i$ and labels $y_i$.\n\n    More info on empirical Fisher matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf) and\n    their use within a Laplace approximation in [Daxberger et al, 2021](https://arxiv.org/abs/2106.14806).\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        per_sample: If True, then log_posterior is assumed to return a vector of\n            log posteriors for each sample in the batch. If False, then log_posterior\n            is assumed to return a scalar log posterior for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_posterior to be per sample.\n        init_prec_diag: Initial diagonal precision matrix.\n            Can be tree like params or scalar.\n\n    Returns:\n        Diagonal empirical Fisher information Laplace approximation transform instance.\n    \"\"\"\n    init_fn = partial(init, init_prec_diag=init_prec_diag)\n    update_fn = partial(update, log_posterior=log_posterior, per_sample=per_sample)\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/laplace/diag_fisher/#posteriors.laplace.diag_fisher.DiagLaplaceState","title":"<code>posteriors.laplace.diag_fisher.DiagLaplaceState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a diagonal Normal distribution over parameters.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>prec_diag</code> <code>TensorTree</code> <p>Diagonal of the precision matrix of the Normal distribution.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/laplace/diag_fisher.py</code> <pre><code>class DiagLaplaceState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a diagonal Normal distribution over parameters.\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        prec_diag: Diagonal of the precision matrix of the Normal distribution.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    prec_diag: TensorTree\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/laplace/diag_fisher/#posteriors.laplace.diag_fisher.init","title":"<code>posteriors.laplace.diag_fisher.init(params, init_prec_diag=0.0)</code>","text":"<p>Initialise diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> required <code>init_prec_diag</code> <code>TensorTree | float</code> <p>Initial diagonal precision matrix. Can be tree like params or scalar.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DiagLaplaceState</code> <p>Initial DiagLaplaceState.</p> Source code in <code>posteriors/laplace/diag_fisher.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_prec_diag: TensorTree | float = 0.0,\n) -&gt; DiagLaplaceState:\n    \"\"\"Initialise diagonal Normal distribution over parameters.\n\n    Args:\n        params: Mean of the Normal distribution.\n        init_prec_diag: Initial diagonal precision matrix.\n            Can be tree like params or scalar.\n\n    Returns:\n        Initial DiagLaplaceState.\n    \"\"\"\n    if is_scalar(init_prec_diag):\n        init_prec_diag = tree_map(\n            lambda x: torch.full_like(x, init_prec_diag, requires_grad=x.requires_grad),\n            params,\n        )\n\n    return DiagLaplaceState(params, init_prec_diag)\n</code></pre>"},{"location":"api/laplace/diag_fisher/#posteriors.laplace.diag_fisher.update","title":"<code>posteriors.laplace.diag_fisher.update(state, batch, log_posterior, per_sample=False, inplace=False)</code>","text":"<p>Adds diagonal empirical Fisher information matrix of covariance summed over given batch.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DiagLaplaceState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>per_sample</code> <code>bool</code> <p>If True, then log_posterior is assumed to return a vector of log posteriors for each sample in the batch. If False, then log_posterior is assumed to return a scalar log posterior for the whole batch, in this case torch.func.vmap will be called, this is typically slower than directly writing log_posterior to be per sample.</p> <code>False</code> <code>inplace</code> <code>bool</code> <p>If True, then the state is updated in place, otherwise a new state is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DiagLaplaceState, TensorTree]</code> <p>Updated DiagLaplaceState and auxiliary information.</p> Source code in <code>posteriors/laplace/diag_fisher.py</code> <pre><code>def update(\n    state: DiagLaplaceState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    per_sample: bool = False,\n    inplace: bool = False,\n) -&gt; tuple[DiagLaplaceState, TensorTree]:\n    \"\"\"Adds diagonal empirical Fisher information matrix of covariance summed over\n    given batch.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        per_sample: If True, then log_posterior is assumed to return a vector of\n            log posteriors for each sample in the batch. If False, then log_posterior\n            is assumed to return a scalar log posterior for the whole batch, in this\n            case torch.func.vmap will be called, this is typically slower than\n            directly writing log_posterior to be per sample.\n        inplace: If True, then the state is updated in place, otherwise a new state\n            is returned.\n\n    Returns:\n        Updated DiagLaplaceState and auxiliary information.\n    \"\"\"\n    if not per_sample:\n        log_posterior = per_samplify(log_posterior)\n\n    with torch.no_grad(), CatchAuxError():\n        jac, aux = jacrev(log_posterior, has_aux=True)(state.params, batch)\n        batch_diag_score_sq = tree_map(lambda j: j.square().sum(0), jac)\n\n    def update_func(x, y):\n        return x + y\n\n    prec_diag = flexi_tree_map(\n        update_func, state.prec_diag, batch_diag_score_sq, inplace=inplace\n    )\n\n    if inplace:\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    return DiagLaplaceState(state.params, prec_diag, state.step + 1), aux\n</code></pre>"},{"location":"api/laplace/diag_fisher/#posteriors.laplace.diag_fisher.sample","title":"<code>posteriors.laplace.diag_fisher.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Sample from diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DiagLaplaceState</code> <p>State encoding mean and diagonal precision.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from Normal distribution.</p> Source code in <code>posteriors/laplace/diag_fisher.py</code> <pre><code>def sample(\n    state: DiagLaplaceState, sample_shape: torch.Size = torch.Size([])\n) -&gt; TensorTree:\n    \"\"\"Sample from diagonal Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and diagonal precision.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from Normal distribution.\n    \"\"\"\n    sd_diag = tree_map(lambda x: x.sqrt().reciprocal(), state.prec_diag)\n    return diag_normal_sample(state.params, sd_diag, sample_shape=sample_shape)\n</code></pre>"},{"location":"api/laplace/diag_ggn/","title":"Laplace Diagonal GGN","text":""},{"location":"api/laplace/diag_ggn/#posteriors.laplace.diag_ggn.build","title":"<code>posteriors.laplace.diag_ggn.build(forward, outer_log_likelihood, init_prec_diag=0.0)</code>","text":"<p>Builds a transform for a diagonal Generalized Gauss-Newton (GGN) Laplace approximation.</p> <p>Equivalent to the diagonal of the (non-empirical) Fisher information matrix when the <code>outer_log_likelihood</code> is exponential family with natural parameter equal to the output from <code>forward</code>.</p> <p><code>forward</code> should output auxiliary information (or <code>torch.tensor([])</code>), <code>outer_log_likelihood</code> should not.</p> <p>The GGN is defined as $$ G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T $$ where \\(z = f(\u03b8)\\) is the output of the forward function \\(f\\) and \\(l(z)\\) is a loss (negative log-likelihood) that maps the output of \\(f\\) to a scalar output.</p> <p>More info on Fisher and GGN matrices can be found in Martens, 2020 and their use within a Laplace approximation in Daxberger et al, 2021.</p> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>ForwardFn</code> <p>Function that takes parameters and input batch and returns a forward value (e.g. logits), not reduced over the batch, as well as auxiliary information.</p> required <code>outer_log_likelihood</code> <code>OuterLogProbFn</code> <p>A function that takes the output of <code>forward</code> and batch then returns the log likelihood of the model output, with no auxiliary information.</p> required <code>init_prec_diag</code> <code>TensorTree | float</code> <p>Initial diagonal precision matrix. Can be tree like params or scalar.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Diagonal GGN Laplace approximation transform instance.</p> Source code in <code>posteriors/laplace/diag_ggn.py</code> <pre><code>def build(\n    forward: ForwardFn,\n    outer_log_likelihood: OuterLogProbFn,\n    init_prec_diag: TensorTree | float = 0.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for a diagonal Generalized Gauss-Newton (GGN)\n    Laplace approximation.\n\n    Equivalent to the diagonal of the (non-empirical) Fisher information matrix when\n    the `outer_log_likelihood` is exponential family with natural parameter equal to\n    the output from `forward`.\n\n    `forward` should output auxiliary information (or `torch.tensor([])`),\n    `outer_log_likelihood` should not.\n\n    The GGN is defined as\n    $$\n    G(\u03b8) = J_f(\u03b8) H_l(z) J_f(\u03b8)^T\n    $$\n    where $z = f(\u03b8)$ is the output of the forward function $f$ and $l(z)$\n    is a loss (negative log-likelihood) that maps the output of $f$ to a scalar output.\n\n    More info on Fisher and GGN matrices can be found in\n    [Martens, 2020](https://jmlr.org/papers/volume21/17-678/17-678.pdf) and\n    their use within a Laplace approximation in [Daxberger et al, 2021](https://arxiv.org/abs/2106.14806).\n\n    Args:\n        forward: Function that takes parameters and input batch and\n            returns a forward value (e.g. logits), not reduced over the batch,\n            as well as auxiliary information.\n        outer_log_likelihood: A function that takes the output of `forward` and batch\n            then returns the log likelihood of the model output,\n            with no auxiliary information.\n        init_prec_diag: Initial diagonal precision matrix.\n            Can be tree like params or scalar.\n\n    Returns:\n        Diagonal GGN Laplace approximation transform instance.\n    \"\"\"\n    init_fn = partial(init, init_prec_diag=init_prec_diag)\n    update_fn = partial(\n        update, forward=forward, outer_log_likelihood=outer_log_likelihood\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/laplace/diag_ggn/#posteriors.laplace.diag_ggn.DiagLaplaceState","title":"<code>posteriors.laplace.diag_ggn.DiagLaplaceState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a diagonal Normal distribution over parameters.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> <code>prec_diag</code> <code>TensorTree</code> <p>Diagonal of the precision matrix of the Normal distribution.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/laplace/diag_ggn.py</code> <pre><code>class DiagLaplaceState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a diagonal Normal distribution over parameters.\n\n    Attributes:\n        params: Mean of the Normal distribution.\n        prec_diag: Diagonal of the precision matrix of the Normal distribution.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    prec_diag: TensorTree\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/laplace/diag_ggn/#posteriors.laplace.diag_ggn.init","title":"<code>posteriors.laplace.diag_ggn.init(params, init_prec_diag=0.0)</code>","text":"<p>Initialise diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Mean of the Normal distribution.</p> required <code>init_prec_diag</code> <code>TensorTree | float</code> <p>Initial diagonal precision matrix. Can be tree like params or scalar.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DiagLaplaceState</code> <p>Initial DiagLaplaceState.</p> Source code in <code>posteriors/laplace/diag_ggn.py</code> <pre><code>def init(\n    params: TensorTree,\n    init_prec_diag: TensorTree | float = 0.0,\n) -&gt; DiagLaplaceState:\n    \"\"\"Initialise diagonal Normal distribution over parameters.\n\n    Args:\n        params: Mean of the Normal distribution.\n        init_prec_diag: Initial diagonal precision matrix.\n            Can be tree like params or scalar.\n\n    Returns:\n        Initial DiagLaplaceState.\n    \"\"\"\n    if is_scalar(init_prec_diag):\n        init_prec_diag = tree_map(\n            lambda x: torch.full_like(x, init_prec_diag, requires_grad=x.requires_grad),\n            params,\n        )\n\n    return DiagLaplaceState(params, init_prec_diag)\n</code></pre>"},{"location":"api/laplace/diag_ggn/#posteriors.laplace.diag_ggn.update","title":"<code>posteriors.laplace.diag_ggn.update(state, batch, forward, outer_log_likelihood, inplace=False)</code>","text":"<p>Adds diagonal GGN matrix of covariance summed over given batch.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DiagLaplaceState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to model.</p> required <code>forward</code> <code>ForwardFn</code> <p>Function that takes parameters and input batch and returns a forward value (e.g. logits), not reduced over the batch, as well as auxiliary information.</p> required <code>outer_log_likelihood</code> <code>OuterLogProbFn</code> <p>A function that takes the output of <code>forward</code> and batch then returns the log likelihood of the model output, with no auxiliary information.</p> required <code>inplace</code> <code>bool</code> <p>If True, then the state is updated in place, otherwise a new state is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DiagLaplaceState, TensorTree]</code> <p>Updated DiagLaplaceState and auxiliary information.</p> Source code in <code>posteriors/laplace/diag_ggn.py</code> <pre><code>def update(\n    state: DiagLaplaceState,\n    batch: Any,\n    forward: ForwardFn,\n    outer_log_likelihood: OuterLogProbFn,\n    inplace: bool = False,\n) -&gt; tuple[DiagLaplaceState, TensorTree]:\n    \"\"\"Adds diagonal GGN matrix of covariance summed over given batch.\n\n    Args:\n        state: Current state.\n        batch: Input data to model.\n        forward: Function that takes parameters and input batch and\n            returns a forward value (e.g. logits), not reduced over the batch,\n            as well as auxiliary information.\n        outer_log_likelihood: A function that takes the output of `forward` and batch\n            then returns the log likelihood of the model output,\n            with no auxiliary information.\n        inplace: If True, then the state is updated in place, otherwise a new state\n            is returned.\n\n    Returns:\n        Updated DiagLaplaceState and auxiliary information.\n    \"\"\"\n\n    def outer_loss(z, batch):\n        return -outer_log_likelihood(z, batch)\n\n    with torch.no_grad(), CatchAuxError():\n        diag_ggn_batch, aux = diag_ggn(\n            lambda params: forward(params, batch),\n            lambda z: outer_loss(z, batch),\n            forward_has_aux=True,\n            loss_has_aux=False,\n            normalize=False,\n        )(state.params)\n\n    def update_func(x, y):\n        return x + y\n\n    prec_diag = flexi_tree_map(\n        update_func, state.prec_diag, diag_ggn_batch, inplace=inplace\n    )\n\n    if inplace:\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    return DiagLaplaceState(state.params, prec_diag, state.step + 1), aux\n</code></pre>"},{"location":"api/laplace/diag_ggn/#posteriors.laplace.diag_ggn.sample","title":"<code>posteriors.laplace.diag_ggn.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Sample from diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>DiagLaplaceState</code> <p>State encoding mean and diagonal precision.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from Normal distribution.</p> Source code in <code>posteriors/laplace/diag_ggn.py</code> <pre><code>def sample(\n    state: DiagLaplaceState, sample_shape: torch.Size = torch.Size([])\n) -&gt; TensorTree:\n    \"\"\"Sample from diagonal Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and diagonal precision.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from Normal distribution.\n    \"\"\"\n    sd_diag = tree_map(lambda x: x.sqrt().reciprocal(), state.prec_diag)\n    return diag_normal_sample(state.params, sd_diag, sample_shape=sample_shape)\n</code></pre>"},{"location":"api/sgmcmc/baoa/","title":"BAOA","text":""},{"location":"api/sgmcmc/baoa/#posteriors.sgmcmc.baoa.build","title":"<code>posteriors.sgmcmc.baoa.build(log_posterior, lr, alpha=0.01, sigma=1.0, temperature=1.0, momenta=None)</code>","text":"<p>Builds BAOA transform.</p> <p>Algorithm from Leimkuhler and Matthews, 2015 - p271.</p> <p>BAOA is conjugate to BAOAB (in Leimkuhler and Matthews' terminology) but requires only a single gradient evaluation per iteration. The two are equivalent when analyzing functions of the parameter trajectory. Unlike BAOAB, BAOA is not reversible, but since we don't apply Metropolis-Hastings  or momenta reversal, the algorithm remains functionally identical to BAOAB.</p> \\[\\begin{align} m_{t+1/2} &amp;= m_t + \u03b5 \\nabla \\log p(\u03b8_t, \\text{batch}), \\\\ \u03b8_{t+1/2} &amp;= \u03b8_t + (\u03b5 / 2) \u03c3^{-2} m_{t+1/2}, \\\\ m_{t+1} &amp;= e^{-\u03b5 \u03b3} m_{t+1/2} + N(0, \u03b6^2 \u03c3^2), \\\\ \u03b8_{t+1} &amp;= \u03b8_{t+1/2} + (\u03b5 / 2) \u03c3^{-2} m_{t+1} \\ \\end{align}\\] <p>for learning rate \\(\\epsilon\\), temperature \\(T\\), transformed friction \\(\u03b3 = \u03b1 \u03c3^{-2}\\) and transformed noise variance\\(\u03b6^2 = T(1 - e^{-2\u03b3\u03b5})\\).</p> <p>Targets \\(p_T(\u03b8, m) \\propto \\exp( (\\log p(\u03b8) - \\frac{1}{2\u03c3^2} m^Tm) / T)\\) with temperature \\(T\\).</p> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data and variable batch size.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>alpha</code> <code>float</code> <p>Friction coefficient.</p> <code>0.01</code> <code>sigma</code> <code>float</code> <p>Standard deviation of momenta target distribution.</p> <code>1.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the joint parameter + momenta distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>momenta</code> <code>TensorTree | float | None</code> <p>Initial momenta. Can be tree like params or scalar. Defaults to random iid samples from N(0, 1).</p> <code>None</code> <p>Returns:</p> Type Description <code>Transform</code> <p>BAOA transform instance.</p> Source code in <code>posteriors/sgmcmc/baoa.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    alpha: float = 0.01,\n    sigma: float = 1.0,\n    temperature: float | Schedule = 1.0,\n    momenta: TensorTree | float | None = None,\n) -&gt; Transform:\n    \"\"\"Builds BAOA transform.\n\n    Algorithm from [Leimkuhler and Matthews, 2015 - p271](https://link.springer.com/ok/10.1007/978-3-319-16375-8).\n\n    BAOA is conjugate to BAOAB (in Leimkuhler and Matthews' terminology) but requires\n    only a single gradient evaluation per iteration.\n    The two are equivalent when analyzing functions of the parameter trajectory.\n    Unlike BAOAB, BAOA is not reversible, but since we don't apply Metropolis-Hastings \n    or momenta reversal, the algorithm remains functionally identical to BAOAB.\n\n    \\\\begin{align}\n    m_{t+1/2} &amp;= m_t + \u03b5 \\\\nabla \\\\log p(\u03b8_t, \\\\text{batch}), \\\\\\\\\n    \u03b8_{t+1/2} &amp;= \u03b8_t + (\u03b5 / 2) \u03c3^{-2} m_{t+1/2}, \\\\\\\\\n    m_{t+1} &amp;= e^{-\u03b5 \u03b3} m_{t+1/2} + N(0, \u03b6^2 \u03c3^2), \\\\\\\\\n    \u03b8_{t+1} &amp;= \u03b8_{t+1/2} + (\u03b5 / 2) \u03c3^{-2} m_{t+1} \\\\\n    \\\\end{align}\n\n    for learning rate $\\\\epsilon$, temperature $T$, transformed friction $\u03b3 = \u03b1 \u03c3^{-2}$\n    and transformed noise variance$\u03b6^2 = T(1 - e^{-2\u03b3\u03b5})$.\n\n    Targets $p_T(\u03b8, m) \\\\propto \\\\exp( (\\\\log p(\u03b8) - \\\\frac{1}{2\u03c3^2} m^Tm) / T)$\n    with temperature $T$.\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data and variable batch size.\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        alpha: Friction coefficient.\n        sigma: Standard deviation of momenta target distribution.\n        temperature: Temperature of the joint parameter + momenta distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        momenta: Initial momenta. Can be tree like params or scalar.\n            Defaults to random iid samples from N(0, 1).\n\n    Returns:\n        BAOA transform instance.\n    \"\"\"\n    init_fn = partial(init, momenta=momenta)\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        lr=lr,\n        alpha=alpha,\n        sigma=sigma,\n        temperature=temperature,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/sgmcmc/baoa/#posteriors.sgmcmc.baoa.BAOAState","title":"<code>posteriors.sgmcmc.baoa.BAOAState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding params and momenta for BAOA.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Parameters.</p> <code>momenta</code> <code>TensorTree</code> <p>Momenta for each parameter.</p> <code>log_posterior</code> <code>Tensor</code> <p>Log posterior evaluation.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/sgmcmc/baoa.py</code> <pre><code>class BAOAState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding params and momenta for BAOA.\n\n    Attributes:\n        params: Parameters.\n        momenta: Momenta for each parameter.\n        log_posterior: Log posterior evaluation.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    momenta: TensorTree\n    log_posterior: torch.Tensor = torch.tensor(torch.nan)\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/sgmcmc/baoa/#posteriors.sgmcmc.baoa.init","title":"<code>posteriors.sgmcmc.baoa.init(params, momenta=None)</code>","text":"<p>Initialise momenta for BAOA.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Parameters for which to initialise.</p> required <code>momenta</code> <code>TensorTree | float | None</code> <p>Initial momenta. Can be tree like params or scalar. Defaults to random iid samples from N(0, 1).</p> <code>None</code> <p>Returns:</p> Type Description <code>BAOAState</code> <p>Initial BAOAState containing momenta.</p> Source code in <code>posteriors/sgmcmc/baoa.py</code> <pre><code>def init(params: TensorTree, momenta: TensorTree | float | None = None) -&gt; BAOAState:\n    \"\"\"Initialise momenta for BAOA.\n\n    Args:\n        params: Parameters for which to initialise.\n        momenta: Initial momenta. Can be tree like params or scalar.\n            Defaults to random iid samples from N(0, 1).\n\n    Returns:\n        Initial BAOAState containing momenta.\n    \"\"\"\n    if momenta is None:\n        momenta = tree_map(\n            lambda x: torch.randn_like(x, requires_grad=x.requires_grad),\n            params,\n        )\n    elif is_scalar(momenta):\n        momenta = tree_map(\n            lambda x: torch.full_like(x, momenta, requires_grad=x.requires_grad),\n            params,\n        )\n\n    return BAOAState(params, momenta)\n</code></pre>"},{"location":"api/sgmcmc/baoa/#posteriors.sgmcmc.baoa.update","title":"<code>posteriors.sgmcmc.baoa.update(state, batch, log_posterior, lr, alpha=0.01, sigma=1.0, temperature=1.0, inplace=False)</code>","text":"<p>Updates parameters and momenta for BAOA.</p> <p>Algorithm from Leimkuhler and Matthews, 2015 - p271.</p> <p>See build for more details.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BAOAState</code> <p>BAOAState containing params and momenta.</p> required <code>batch</code> <code>Any</code> <p>Data batch to be send to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>alpha</code> <code>float</code> <p>Friction coefficient.</p> <code>0.01</code> <code>sigma</code> <code>float</code> <p>Standard deviation of momenta target distribution.</p> <code>1.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the joint parameter + momenta distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>inplace</code> <code>bool</code> <p>Whether to modify state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>BAOAState</code> <p>Updated state</p> <code>TensorTree</code> <p>(which are pointers to the inputted state tensors if inplace=True)</p> <code>tuple[BAOAState, TensorTree]</code> <p>and auxiliary information.</p> Source code in <code>posteriors/sgmcmc/baoa.py</code> <pre><code>def update(\n    state: BAOAState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    alpha: float = 0.01,\n    sigma: float = 1.0,\n    temperature: float | Schedule = 1.0,\n    inplace: bool = False,\n) -&gt; tuple[BAOAState, TensorTree]:\n    \"\"\"Updates parameters and momenta for BAOA.\n\n    Algorithm from [Leimkuhler and Matthews, 2015 - p271](https://link.springer.com/ok/10.1007/978-3-319-16375-8).\n\n    See [build](baoa.md#posteriors.sgmcmc.baoa.build) for more details.\n\n    Args:\n        state: BAOAState containing params and momenta.\n        batch: Data batch to be send to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        alpha: Friction coefficient.\n        sigma: Standard deviation of momenta target distribution.\n        temperature: Temperature of the joint parameter + momenta distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        inplace: Whether to modify state in place.\n\n    Returns:\n        Updated state\n        (which are pointers to the inputted state tensors if inplace=True)\n        and auxiliary information.\n    \"\"\"\n    with torch.no_grad(), CatchAuxError():\n        grads, (log_post, aux) = grad_and_value(log_posterior, has_aux=True)(\n            state.params, batch\n        )\n\n    lr = lr(state.step) if callable(lr) else lr\n    temperature = temperature(state.step) if callable(temperature) else temperature\n    prec = sigma**-2\n    gamma = torch.tensor(alpha * prec)\n    zeta2 = (temperature * (1 - torch.exp(-2 * gamma * lr))) ** 0.5\n\n    def BB_step(m, g):\n        return m + lr * g\n\n    def A_step(p, m):\n        return p + (lr / 2) * prec * m\n\n    def O_step(m):\n        return torch.exp(-gamma * lr) * m + zeta2 * sigma * torch.randn_like(m)\n\n    momenta = flexi_tree_map(BB_step, state.momenta, grads, inplace=inplace)\n    params = flexi_tree_map(A_step, state.params, momenta, inplace=inplace)\n    momenta = flexi_tree_map(O_step, momenta, inplace=inplace)\n    params = flexi_tree_map(A_step, params, momenta, inplace=inplace)\n\n    if inplace:\n        tree_insert_(state.log_posterior, log_post.detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    return BAOAState(params, momenta, log_post.detach(), state.step + 1), aux\n</code></pre>"},{"location":"api/sgmcmc/sghmc/","title":"SGHMC","text":""},{"location":"api/sgmcmc/sghmc/#posteriors.sgmcmc.sghmc.build","title":"<code>posteriors.sgmcmc.sghmc.build(log_posterior, lr, alpha=0.01, beta=0.0, sigma=1.0, temperature=1.0, momenta=None)</code>","text":"<p>Builds SGHMC transform.</p> <p>Algorithm from Chen et al, 2014:</p> \\[\\begin{align} \u03b8_{t+1} &amp;= \u03b8_t + \u03b5 \u03c3^{-2} m_t \\\\ m_{t+1} &amp;= m_t + \u03b5 \\nabla \\log p(\u03b8_t, \\text{batch}) - \u03b5 \u03c3^{-2} \u03b1 m_t + N(0, \u03b5 T (2 \u03b1 - \u03b5 \u03b2 T) \\mathbb{I})\\ \\end{align}\\] <p>for learning rate \\(\\epsilon\\) and temperature \\(T\\)</p> <p>Targets \\(p_T(\u03b8, m) \\propto \\exp( (\\log p(\u03b8) - \\frac{1}{2\u03c3^2} m^Tm) / T)\\) with temperature \\(T\\).</p> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data and variable batch size.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate, scalar or schedule (callable taking step index, returning scalar).</p> required <code>alpha</code> <code>float</code> <p>Friction coefficient.</p> <code>0.01</code> <code>beta</code> <code>float</code> <p>Gradient noise coefficient (estimated variance).</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation of momenta target distribution.</p> <code>1.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the joint parameter + momenta distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>momenta</code> <code>TensorTree | float | None</code> <p>Initial momenta. Can be tree like params or scalar. Defaults to random iid samples from N(0, 1).</p> <code>None</code> <p>Returns:</p> Type Description <code>Transform</code> <p>SGHMC transform instance.</p> Source code in <code>posteriors/sgmcmc/sghmc.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    alpha: float = 0.01,\n    beta: float = 0.0,\n    sigma: float = 1.0,\n    temperature: float | Schedule = 1.0,\n    momenta: TensorTree | float | None = None,\n) -&gt; Transform:\n    \"\"\"Builds SGHMC transform.\n\n    Algorithm from [Chen et al, 2014](https://arxiv.org/abs/1402.4102):\n\n    \\\\begin{align}\n    \u03b8_{t+1} &amp;= \u03b8_t + \u03b5 \u03c3^{-2} m_t \\\\\\\\\n    m_{t+1} &amp;= m_t + \u03b5 \\\\nabla \\\\log p(\u03b8_t, \\\\text{batch}) - \u03b5 \u03c3^{-2} \u03b1 m_t\n    + N(0, \u03b5 T (2 \u03b1 - \u03b5 \u03b2 T) \\\\mathbb{I})\\\\\n    \\\\end{align}\n\n    for learning rate $\\\\epsilon$ and temperature $T$\n\n    Targets $p_T(\u03b8, m) \\\\propto \\\\exp( (\\\\log p(\u03b8) - \\\\frac{1}{2\u03c3^2} m^Tm) / T)$\n    with temperature $T$.\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data and variable batch size.\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate,\n            scalar or schedule (callable taking step index, returning scalar).\n        alpha: Friction coefficient.\n        beta: Gradient noise coefficient (estimated variance).\n        sigma: Standard deviation of momenta target distribution.\n        temperature: Temperature of the joint parameter + momenta distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        momenta: Initial momenta. Can be tree like params or scalar.\n            Defaults to random iid samples from N(0, 1).\n\n    Returns:\n        SGHMC transform instance.\n    \"\"\"\n    init_fn = partial(init, momenta=momenta)\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        lr=lr,\n        alpha=alpha,\n        beta=beta,\n        sigma=sigma,\n        temperature=temperature,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/sgmcmc/sghmc/#posteriors.sgmcmc.sghmc.SGHMCState","title":"<code>posteriors.sgmcmc.sghmc.SGHMCState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding params and momenta for SGHMC.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Parameters.</p> <code>momenta</code> <code>TensorTree</code> <p>Momenta for each parameter.</p> <code>log_posterior</code> <code>Tensor</code> <p>Log posterior evaluation.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/sgmcmc/sghmc.py</code> <pre><code>class SGHMCState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding params and momenta for SGHMC.\n\n    Attributes:\n        params: Parameters.\n        momenta: Momenta for each parameter.\n        log_posterior: Log posterior evaluation.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    momenta: TensorTree\n    log_posterior: torch.Tensor = torch.tensor(torch.nan)\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/sgmcmc/sghmc/#posteriors.sgmcmc.sghmc.init","title":"<code>posteriors.sgmcmc.sghmc.init(params, momenta=None)</code>","text":"<p>Initialise momenta for SGHMC.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Parameters for which to initialise.</p> required <code>momenta</code> <code>TensorTree | float | None</code> <p>Initial momenta. Can be tree like params or scalar. Defaults to random iid samples from N(0, 1).</p> <code>None</code> <p>Returns:</p> Type Description <code>SGHMCState</code> <p>Initial SGHMCState containing momenta.</p> Source code in <code>posteriors/sgmcmc/sghmc.py</code> <pre><code>def init(params: TensorTree, momenta: TensorTree | float | None = None) -&gt; SGHMCState:\n    \"\"\"Initialise momenta for SGHMC.\n\n    Args:\n        params: Parameters for which to initialise.\n        momenta: Initial momenta. Can be tree like params or scalar.\n            Defaults to random iid samples from N(0, 1).\n\n    Returns:\n        Initial SGHMCState containing momenta.\n    \"\"\"\n    if momenta is None:\n        momenta = tree_map(\n            lambda x: torch.randn_like(x, requires_grad=x.requires_grad),\n            params,\n        )\n    elif is_scalar(momenta):\n        momenta = tree_map(\n            lambda x: torch.full_like(x, momenta, requires_grad=x.requires_grad),\n            params,\n        )\n\n    return SGHMCState(params, momenta)\n</code></pre>"},{"location":"api/sgmcmc/sghmc/#posteriors.sgmcmc.sghmc.update","title":"<code>posteriors.sgmcmc.sghmc.update(state, batch, log_posterior, lr, alpha=0.01, beta=0.0, sigma=1.0, temperature=1.0, inplace=False)</code>","text":"<p>Updates parameters and momenta for SGHMC.</p> <p>Update rule from Chen et al, 2014, see build for details.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SGHMCState</code> <p>SGHMCState containing params and momenta.</p> required <code>batch</code> <code>Any</code> <p>Data batch to be send to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate, scalar or schedule (callable taking step index, returning scalar).</p> required <code>alpha</code> <code>float</code> <p>Friction coefficient.</p> <code>0.01</code> <code>beta</code> <code>float</code> <p>Gradient noise coefficient (estimated variance).</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation of momenta target distribution.</p> <code>1.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the joint parameter + momenta distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>inplace</code> <code>bool</code> <p>Whether to modify state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[SGHMCState, TensorTree]</code> <p>Updated state (which are pointers to the inputted state tensors if inplace=True) and auxiliary information.</p> Source code in <code>posteriors/sgmcmc/sghmc.py</code> <pre><code>def update(\n    state: SGHMCState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    alpha: float = 0.01,\n    beta: float = 0.0,\n    sigma: float = 1.0,\n    temperature: float | Schedule = 1.0,\n    inplace: bool = False,\n) -&gt; tuple[SGHMCState, TensorTree]:\n    \"\"\"Updates parameters and momenta for SGHMC.\n\n    Update rule from [Chen et al, 2014](https://arxiv.org/abs/1402.4102),\n    see [build](sghmc.md#posteriors.sgmcmc.sghmc.build) for details.\n\n    Args:\n        state: SGHMCState containing params and momenta.\n        batch: Data batch to be send to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate,\n            scalar or schedule (callable taking step index, returning scalar).\n        alpha: Friction coefficient.\n        beta: Gradient noise coefficient (estimated variance).\n        sigma: Standard deviation of momenta target distribution.\n        temperature: Temperature of the joint parameter + momenta distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        inplace: Whether to modify state in place.\n\n    Returns:\n        Updated state (which are pointers to the inputted state tensors if inplace=True)\n            and auxiliary information.\n    \"\"\"\n    with torch.no_grad(), CatchAuxError():\n        grads, (log_post, aux) = grad_and_value(log_posterior, has_aux=True)(\n            state.params, batch\n        )\n\n    lr = lr(state.step) if callable(lr) else lr\n    temperature = temperature(state.step) if callable(temperature) else temperature\n    prec = sigma**-2\n\n    def transform_params(p, m):\n        return p + lr * prec * m\n\n    def transform_momenta(m, g):\n        return (\n            m\n            + lr * g\n            - lr * prec * alpha * m\n            + (temperature * lr * (2 * alpha - temperature * lr * beta)) ** 0.5\n            * torch.randn_like(m)\n        )\n\n    params = flexi_tree_map(\n        transform_params, state.params, state.momenta, inplace=inplace\n    )\n    momenta = flexi_tree_map(transform_momenta, state.momenta, grads, inplace=inplace)\n\n    if inplace:\n        tree_insert_(state.log_posterior, log_post.detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    return SGHMCState(params, momenta, log_post.detach(), state.step + 1), aux\n</code></pre>"},{"location":"api/sgmcmc/sgld/","title":"SGLD","text":""},{"location":"api/sgmcmc/sgld/#posteriors.sgmcmc.sgld.build","title":"<code>posteriors.sgmcmc.sgld.build(log_posterior, lr, beta=0.0, temperature=1.0)</code>","text":"<p>Builds SGLD transform.</p> <p>Algorithm from Welling and Teh, 2011: $$ \u03b8_{t+1} = \u03b8_t + \u03b5 \\nabla \\log p(\u03b8_t, \\text{batch}) + N(0, \u03b5  (2 - \u03b5 \u03b2) T \\mathbb{I}) $$ for learning rate \\(\\epsilon\\) and temperature \\(T\\).</p> <p>Targets \\(p_T(\u03b8) \\propto \\exp( \\log p(\u03b8) / T)\\) with temperature \\(T\\).</p> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data and variable batch size.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate, scalar or schedule (callable taking step index, returning scalar).</p> required <code>beta</code> <code>float</code> <p>Gradient noise coefficient (estimated variance).</p> <code>0.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the sampling distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>SGLD transform (posteriors.types.Transform instance).</p> Source code in <code>posteriors/sgmcmc/sgld.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    beta: float = 0.0,\n    temperature: float | Schedule = 1.0,\n) -&gt; Transform:\n    \"\"\"Builds SGLD transform.\n\n    Algorithm from [Welling and Teh, 2011](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf):\n    $$\n    \u03b8_{t+1} = \u03b8_t + \u03b5 \\\\nabla \\\\log p(\u03b8_t, \\\\text{batch}) + N(0, \u03b5  (2 - \u03b5 \u03b2) T \\\\mathbb{I})\n    $$\n    for learning rate $\\\\epsilon$ and temperature $T$.\n\n    Targets $p_T(\u03b8) \\\\propto \\\\exp( \\\\log p(\u03b8) / T)$ with temperature $T$.\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data and variable batch size.\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate,\n            scalar or schedule (callable taking step index, returning scalar).\n        beta: Gradient noise coefficient (estimated variance).\n        temperature: Temperature of the sampling distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n\n    Returns:\n        SGLD transform (posteriors.types.Transform instance).\n    \"\"\"\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        lr=lr,\n        beta=beta,\n        temperature=temperature,\n    )\n    return Transform(init, update_fn)\n</code></pre>"},{"location":"api/sgmcmc/sgld/#posteriors.sgmcmc.sgld.SGLDState","title":"<code>posteriors.sgmcmc.sgld.SGLDState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding params for SGLD.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Parameters.</p> <code>log_posterior</code> <code>Tensor</code> <p>Log posterior evaluation.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/sgmcmc/sgld.py</code> <pre><code>class SGLDState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding params for SGLD.\n\n    Attributes:\n        params: Parameters.\n        log_posterior: Log posterior evaluation.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    log_posterior: Tensor = torch.tensor(torch.nan)\n    step: Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/sgmcmc/sgld/#posteriors.sgmcmc.sgld.init","title":"<code>posteriors.sgmcmc.sgld.init(params)</code>","text":"<p>Initialise SGLD.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Parameters for which to initialise.</p> required <p>Returns:</p> Type Description <code>SGLDState</code> <p>Initial SGLDState.</p> Source code in <code>posteriors/sgmcmc/sgld.py</code> <pre><code>def init(params: TensorTree) -&gt; SGLDState:\n    \"\"\"Initialise SGLD.\n\n    Args:\n        params: Parameters for which to initialise.\n\n    Returns:\n        Initial SGLDState.\n    \"\"\"\n\n    return SGLDState(params)\n</code></pre>"},{"location":"api/sgmcmc/sgld/#posteriors.sgmcmc.sgld.update","title":"<code>posteriors.sgmcmc.sgld.update(state, batch, log_posterior, lr, beta=0.0, temperature=1.0, inplace=False)</code>","text":"<p>Updates parameters for SGLD.</p> <p>Update rule from Welling and Teh, 2011, see build for details.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SGLDState</code> <p>SGLDState containing params.</p> required <code>batch</code> <code>Any</code> <p>Data batch to be send to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate, scalar or schedule (callable taking step index, returning scalar).</p> required <code>beta</code> <code>float</code> <p>Gradient noise coefficient (estimated variance).</p> <code>0.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the sampling distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>inplace</code> <code>bool</code> <p>Whether to modify state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>SGLDState</code> <p>Updated state (which are pointers to the input state tensors if inplace=True)</p> <code>TensorTree</code> <p>and auxiliary information.</p> Source code in <code>posteriors/sgmcmc/sgld.py</code> <pre><code>def update(\n    state: SGLDState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    beta: float = 0.0,\n    temperature: float | Schedule = 1.0,\n    inplace: bool = False,\n) -&gt; tuple[SGLDState, TensorTree]:\n    \"\"\"Updates parameters for SGLD.\n\n    Update rule from [Welling and Teh, 2011](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf),\n    see [build](sgld.md#posteriors.sgmcmc.sgld.build) for details.\n\n    Args:\n        state: SGLDState containing params.\n        batch: Data batch to be send to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate,\n            scalar or schedule (callable taking step index, returning scalar).\n        beta: Gradient noise coefficient (estimated variance).\n        temperature: Temperature of the sampling distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        inplace: Whether to modify state in place.\n\n    Returns:\n        Updated state (which are pointers to the input state tensors if inplace=True)\n        and auxiliary information.\n    \"\"\"\n    with torch.no_grad(), CatchAuxError():\n        grads, (log_post, aux) = grad_and_value(log_posterior, has_aux=True)(\n            state.params, batch\n        )\n\n    lr = lr(state.step) if callable(lr) else lr\n    temperature = temperature(state.step) if callable(temperature) else temperature\n\n    def transform_params(p, g):\n        return (\n            p\n            + lr * g\n            + (temperature * lr * (2 - temperature * lr * beta)) ** 0.5\n            * torch.randn_like(p)\n        )\n\n    params = flexi_tree_map(transform_params, state.params, grads, inplace=inplace)\n\n    if inplace:\n        tree_insert_(state.log_posterior, log_post.detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    return SGLDState(params, log_post.detach(), state.step + 1), aux\n</code></pre>"},{"location":"api/sgmcmc/sgnht/","title":"SGNHT","text":""},{"location":"api/sgmcmc/sgnht/#posteriors.sgmcmc.sgnht.build","title":"<code>posteriors.sgmcmc.sgnht.build(log_posterior, lr, alpha=0.01, beta=0.0, sigma=1.0, temperature=1.0, momenta=None, xi=None)</code>","text":"<p>Builds SGNHT transform.</p> <p>Algorithm from Ding et al, 2014:</p> \\[\\begin{align} \u03b8_{t+1} &amp;= \u03b8_t + \u03b5 \u03c3^{-2} m_t \\\\ m_{t+1} &amp;= m_t + \u03b5 \\nabla \\log p(\u03b8_t, \\text{batch}) - \u03b5 \u03c3^{-2} \u03be_t m_t + N(0, \u03b5 T (2 \u03b1 - \u03b5 \u03b2 T) \\mathbb{I})\\\\ \u03be_{t+1} &amp;= \u03be_t + \u03b5 (\u03c3^{-2} d^{-1} m_t^T m_t - T) \\end{align}\\] <p>for learning rate \\(\\epsilon\\), temperature \\(T\\) and parameter dimension \\(d\\).</p> <p>Targets \\(p_T(\u03b8, m, \u03be) \\propto \\exp( (\\log p(\u03b8) - \\frac{1}{2\u03c3^2} m^Tm - \\frac{d}{2}(\u03be - \u03b1)^2) / T)\\).</p> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data and variable batch size.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>alpha</code> <code>float</code> <p>Friction coefficient.</p> <code>0.01</code> <code>beta</code> <code>float</code> <p>Gradient noise coefficient (estimated variance).</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation of momenta target distribution.</p> <code>1.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the joint parameter + momenta distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>momenta</code> <code>TensorTree | float | None</code> <p>Initial momenta. Can be tree like params or scalar. Defaults to random iid samples from N(0, 1).</p> <code>None</code> <code>xi</code> <code>float</code> <p>Initial value for scalar thermostat \u03be. Defaults to <code>alpha</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Transform</code> <p>SGNHT transform instance.</p> Source code in <code>posteriors/sgmcmc/sgnht.py</code> <pre><code>def build(\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    alpha: float = 0.01,\n    beta: float = 0.0,\n    sigma: float = 1.0,\n    temperature: float | Schedule = 1.0,\n    momenta: TensorTree | float | None = None,\n    xi: float = None,\n) -&gt; Transform:\n    \"\"\"Builds SGNHT transform.\n\n    Algorithm from [Ding et al, 2014](https://proceedings.neurips.cc/paper/2014/file/21fe5b8ba755eeaece7a450849876228-Paper.pdf):\n\n    \\\\begin{align}\n    \u03b8_{t+1} &amp;= \u03b8_t + \u03b5 \u03c3^{-2} m_t \\\\\\\\\n    m_{t+1} &amp;= m_t + \u03b5 \\\\nabla \\\\log p(\u03b8_t, \\\\text{batch}) - \u03b5 \u03c3^{-2} \u03be_t m_t\n    + N(0, \u03b5 T (2 \u03b1 - \u03b5 \u03b2 T) \\\\mathbb{I})\\\\\\\\\n    \u03be_{t+1} &amp;= \u03be_t + \u03b5 (\u03c3^{-2} d^{-1} m_t^T m_t - T)\n    \\\\end{align}\n\n    for learning rate $\\\\epsilon$, temperature $T$ and parameter dimension $d$.\n\n    Targets $p_T(\u03b8, m, \u03be) \\\\propto \\\\exp( (\\\\log p(\u03b8) - \\\\frac{1}{2\u03c3^2} m^Tm - \\\\frac{d}{2}(\u03be - \u03b1)^2) / T)$.\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data and variable batch size.\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        alpha: Friction coefficient.\n        beta: Gradient noise coefficient (estimated variance).\n        sigma: Standard deviation of momenta target distribution.\n        temperature: Temperature of the joint parameter + momenta distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        momenta: Initial momenta. Can be tree like params or scalar.\n            Defaults to random iid samples from N(0, 1).\n        xi: Initial value for scalar thermostat \u03be. Defaults to `alpha`.\n\n    Returns:\n        SGNHT transform instance.\n    \"\"\"\n    init_fn = partial(init, momenta=momenta, xi=xi or alpha)\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        lr=lr,\n        alpha=alpha,\n        beta=beta,\n        sigma=sigma,\n        temperature=temperature,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/sgmcmc/sgnht/#posteriors.sgmcmc.sgnht.SGNHTState","title":"<code>posteriors.sgmcmc.sgnht.SGNHTState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding params and momenta for SGNHT.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Parameters.</p> <code>momenta</code> <code>TensorTree</code> <p>Momenta for each parameter.</p> <code>xi</code> <code>Tensor</code> <p>Scalar thermostat.</p> <code>log_posterior</code> <code>Tensor</code> <p>Log posterior evaluation.</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/sgmcmc/sgnht.py</code> <pre><code>class SGNHTState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding params and momenta for SGNHT.\n\n    Attributes:\n        params: Parameters.\n        momenta: Momenta for each parameter.\n        xi: Scalar thermostat.\n        log_posterior: Log posterior evaluation.\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    momenta: TensorTree\n    xi: torch.Tensor\n    log_posterior: torch.Tensor = torch.tensor(torch.nan)\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/sgmcmc/sgnht/#posteriors.sgmcmc.sgnht.init","title":"<code>posteriors.sgmcmc.sgnht.init(params, momenta=None, xi=0.01)</code>","text":"<p>Initialise momenta for SGNHT.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Parameters for which to initialise.</p> required <code>momenta</code> <code>TensorTree | float | None</code> <p>Initial momenta. Can be tree like params or scalar. Defaults to random iid samples from N(0, 1).</p> <code>None</code> <code>xi</code> <code>float | Tensor</code> <p>Initial value for scalar thermostat \u03be.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>SGNHTState</code> <p>Initial SGNHTState containing params, momenta and xi (thermostat).</p> Source code in <code>posteriors/sgmcmc/sgnht.py</code> <pre><code>def init(\n    params: TensorTree,\n    momenta: TensorTree | float | None = None,\n    xi: float | torch.Tensor = 0.01,\n) -&gt; SGNHTState:\n    \"\"\"Initialise momenta for SGNHT.\n\n    Args:\n        params: Parameters for which to initialise.\n        momenta: Initial momenta. Can be tree like params or scalar.\n            Defaults to random iid samples from N(0, 1).\n        xi: Initial value for scalar thermostat \u03be.\n\n    Returns:\n        Initial SGNHTState containing params, momenta and xi (thermostat).\n    \"\"\"\n    if momenta is None:\n        momenta = tree_map(\n            lambda x: torch.randn_like(x, requires_grad=x.requires_grad),\n            params,\n        )\n    elif is_scalar(momenta):\n        momenta = tree_map(\n            lambda x: torch.full_like(x, momenta, requires_grad=x.requires_grad),\n            params,\n        )\n\n    return SGNHTState(params, momenta, torch.tensor(xi))\n</code></pre>"},{"location":"api/sgmcmc/sgnht/#posteriors.sgmcmc.sgnht.update","title":"<code>posteriors.sgmcmc.sgnht.update(state, batch, log_posterior, lr, alpha=0.01, beta=0.0, sigma=1.0, temperature=1.0, inplace=False)</code>","text":"<p>Updates parameters, momenta and xi for SGNHT.</p> <p>Update rule from Ding et al, 2014, see build for details.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SGNHTState</code> <p>SGNHTState containing params, momenta and xi.</p> required <code>batch</code> <code>Any</code> <p>Data batch to be send to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior value (which can be unnormalised) as well as auxiliary information, e.g. from the model call.</p> required <code>lr</code> <code>float | Schedule</code> <p>Learning rate. Scalar or schedule (callable taking step index, returning scalar).</p> required <code>alpha</code> <code>float</code> <p>Friction coefficient.</p> <code>0.01</code> <code>beta</code> <code>float</code> <p>Gradient noise coefficient (estimated variance).</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>Standard deviation of momenta target distribution.</p> <code>1.0</code> <code>temperature</code> <code>float | Schedule</code> <p>Temperature of the joint parameter + momenta distribution. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>inplace</code> <code>bool</code> <p>Whether to modify state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>SGNHTState</code> <p>Updated SGNHTState (which are pointers to the inputted state tensors if</p> <code>TensorTree</code> <p>inplace=True) and auxiliary information.</p> Source code in <code>posteriors/sgmcmc/sgnht.py</code> <pre><code>def update(\n    state: SGNHTState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    lr: float | Schedule,\n    alpha: float = 0.01,\n    beta: float = 0.0,\n    sigma: float = 1.0,\n    temperature: float | Schedule = 1.0,\n    inplace: bool = False,\n) -&gt; tuple[SGNHTState, TensorTree]:\n    \"\"\"Updates parameters, momenta and xi for SGNHT.\n\n    Update rule from [Ding et al, 2014](https://proceedings.neurips.cc/paper/2014/file/21fe5b8ba755eeaece7a450849876228-Paper.pdf),\n    see [build](sgnht.md#posteriors.sgmcmc.sgnht.build) for details.\n\n    Args:\n        state: SGNHTState containing params, momenta and xi.\n        batch: Data batch to be send to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior value (which can be unnormalised)\n            as well as auxiliary information, e.g. from the model call.\n        lr: Learning rate.\n            Scalar or schedule (callable taking step index, returning scalar).\n        alpha: Friction coefficient.\n        beta: Gradient noise coefficient (estimated variance).\n        sigma: Standard deviation of momenta target distribution.\n        temperature: Temperature of the joint parameter + momenta distribution.\n            Scalar or schedule (callable taking step index, returning scalar).\n        inplace: Whether to modify state in place.\n\n    Returns:\n        Updated SGNHTState (which are pointers to the inputted state tensors if\n        inplace=True) and auxiliary information.\n    \"\"\"\n    with torch.no_grad(), CatchAuxError():\n        grads, (log_post, aux) = grad_and_value(log_posterior, has_aux=True)(\n            state.params, batch\n        )\n\n    lr = lr(state.step) if callable(lr) else lr\n    temperature = temperature(state.step) if callable(temperature) else temperature\n    prec = sigma**-2\n\n    def transform_params(p, m):\n        return p + lr * prec * m\n\n    def transform_momenta(m, g):\n        return (\n            m\n            + lr * g\n            - lr * prec * state.xi * m\n            + (temperature * lr * (2 * alpha - temperature * lr * beta)) ** 0.5\n            * torch.randn_like(m)\n        )\n\n    m_flat, _ = tree_ravel(state.momenta)\n    xi_new = state.xi + lr * (prec * torch.mean(m_flat**2) - temperature)\n\n    params = flexi_tree_map(\n        transform_params, state.params, state.momenta, inplace=inplace\n    )\n    momenta = flexi_tree_map(transform_momenta, state.momenta, grads, inplace=inplace)\n\n    if inplace:\n        tree_insert_(state.xi, xi_new)\n        tree_insert_(state.log_posterior, log_post.detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n    return SGNHTState(params, momenta, xi_new, log_post.detach(), state.step + 1), aux\n</code></pre>"},{"location":"api/vi/dense/","title":"VI Dense","text":""},{"location":"api/vi/dense/#posteriors.vi.dense.build","title":"<code>posteriors.vi.dense.build(log_posterior, optimizer, temperature=1.0, n_samples=1, stl=True, init_L=1.0)</code>","text":"<p>Builds a transform for variational inference with a Normal distribution over parameters.</p> <p>Find \\(\\mu\\) and \\(\\Sigma\\) that mimimize \\(\\text{KL}(N(\u03b8| \\mu, \\Sigma) || p_T(\u03b8))\\) where \\(p_T(\u03b8) \\propto \\exp( \\log p(\u03b8) / T)\\) with temperature \\(T\\).</p> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data.</p> <p>For more information on variational inference see Blei et al, 2017.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>Callable[[TensorTree, Any], float]</code> <p>Function that takes parameters and input batch and returns the log posterior (which can be unnormalised).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer for updating the variational parameters. Make sure to use lower case like torchopt.adam()</p> required <code>temperature</code> <code>float | Schedule</code> <p>Temperature to rescale (divide) log_posterior. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>n_samples</code> <code>int</code> <p>Number of samples to use for Monte Carlo estimate.</p> <code>1</code> <code>stl</code> <code>bool</code> <p>Whether to use the stick-the-landing estimator from Roeder et al.</p> <code>True</code> <code>init_L</code> <code>Tensor | float</code> <p>Initial lower triangular matrix \\(L\\) satisfying \\(LL^T\\) = \\(\\Sigma\\).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Dense VI transform instance.</p> Source code in <code>posteriors/vi/dense.py</code> <pre><code>def build(\n    log_posterior: Callable[[TensorTree, Any], float],\n    optimizer: torchopt.base.GradientTransformation,\n    temperature: float | Schedule = 1.0,\n    n_samples: int = 1,\n    stl: bool = True,\n    init_L: torch.Tensor | float = 1.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for variational inference with a Normal\n    distribution over parameters.\n\n    Find $\\\\mu$ and $\\\\Sigma$ that mimimize $\\\\text{KL}(N(\u03b8| \\\\mu, \\\\Sigma) || p_T(\u03b8))$\n    where $p_T(\u03b8) \\\\propto \\\\exp( \\\\log p(\u03b8) / T)$ with temperature $T$.\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data.\n\n    For more information on variational inference see [Blei et al, 2017](https://arxiv.org/abs/1601.00670).\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior (which can be unnormalised).\n        optimizer: TorchOpt functional optimizer for updating the variational\n            parameters. Make sure to use lower case like torchopt.adam()\n        temperature: Temperature to rescale (divide) log_posterior.\n            Scalar or schedule (callable taking step index, returning scalar).\n        n_samples: Number of samples to use for Monte Carlo estimate.\n        stl: Whether to use the stick-the-landing estimator\n            from [Roeder et al](https://arxiv.org/abs/1703.09194).\n        init_L: Initial lower triangular matrix $L$ satisfying $LL^T$ = $\\\\Sigma$.\n\n    Returns:\n        Dense VI transform instance.\n    \"\"\"\n    init_fn = partial(init, optimizer=optimizer, init_L=init_L)\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        optimizer=optimizer,\n        temperature=temperature,\n        n_samples=n_samples,\n        stl=stl,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/vi/dense/#posteriors.vi.dense.VIDenseState","title":"<code>posteriors.vi.dense.VIDenseState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a diagonal Normal variational distribution over parameters.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the variational distribution.</p> <code>L_factor</code> <code>Tensor</code> <p>Flat representation of the nonzero values of the lower triangular matrix \\(L\\) satisfying \\(LL^T\\) = \\(\\Sigma\\), where \\(\\Sigma\\) is the covariance matrix of the variational distribution.</p> <code>opt_state</code> <code>OptState</code> <p>TorchOpt state storing optimizer data for updating the variational parameters.</p> <code>nelbo</code> <code>Tensor</code> <p>Negative evidence lower bound (lower is better).</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/vi/dense.py</code> <pre><code>class VIDenseState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a diagonal Normal variational distribution over parameters.\n\n    Attributes:\n        params: Mean of the variational distribution.\n        L_factor: Flat representation of the nonzero values of the lower\n            triangular matrix $L$ satisfying $LL^T$ = $\\\\Sigma$, where $\\\\Sigma$\n            is the covariance matrix of the variational distribution.\n        opt_state: TorchOpt state storing optimizer data for updating the\n            variational parameters.\n        nelbo: Negative evidence lower bound (lower is better).\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    L_factor: torch.Tensor\n    opt_state: torchopt.typing.OptState\n    nelbo: torch.Tensor = torch.tensor([])\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/vi/dense/#posteriors.vi.dense.init","title":"<code>posteriors.vi.dense.init(params, optimizer, init_L=1.0)</code>","text":"<p>Initialise diagonal Normal variational distribution over parameters.</p> <p>optimizer.init will be called on flattened variational parameters so hyperparameters such as learning rate need to pre-specified through TorchOpt's functional API:</p> <pre><code>import torchopt\n\noptimizer = torchopt.adam(lr=1e-2)\nvi_state = init(init_mean, optimizer)\n</code></pre> <p>It's assumed maximize=False for the optimizer, so that we minimize the NELBO.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Initial mean of the variational distribution.</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer for updating the variational parameters. Make sure to use lower case like torchopt.adam()</p> required <code>init_L</code> <code>Tensor | float</code> <p>Initial lower triangular matrix \\(L\\) satisfying \\(LL^T\\) = \\(\\Sigma\\), where \\(\\Sigma\\) is the covariance matrix of the variational distribution.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>VIDenseState</code> <p>Initial DenseVIState.</p> Source code in <code>posteriors/vi/dense.py</code> <pre><code>def init(\n    params: TensorTree,\n    optimizer: torchopt.base.GradientTransformation,\n    init_L: torch.Tensor | float = 1.0,\n) -&gt; VIDenseState:\n    \"\"\"Initialise diagonal Normal variational distribution over parameters.\n\n    optimizer.init will be called on flattened variational parameters so hyperparameters\n    such as learning rate need to pre-specified through TorchOpt's functional API:\n\n    ```\n    import torchopt\n\n    optimizer = torchopt.adam(lr=1e-2)\n    vi_state = init(init_mean, optimizer)\n    ```\n\n    It's assumed maximize=False for the optimizer, so that we minimize the NELBO.\n\n    Args:\n        params: Initial mean of the variational distribution.\n        optimizer: TorchOpt functional optimizer for updating the variational\n            parameters. Make sure to use lower case like torchopt.adam()\n        init_L: Initial lower triangular matrix $L$ satisfying $LL^T$ = $\\\\Sigma$,\n            where $\\\\Sigma$ is the covariance matrix of the variational distribution.\n\n    Returns:\n        Initial DenseVIState.\n    \"\"\"\n\n    num_params = tree_size(params)\n    if is_scalar(init_L):\n        init_L = init_L * torch.eye(num_params, requires_grad=True)\n\n    init_L = L_to_flat(init_L)\n    opt_state = optimizer.init([params, init_L])\n    return VIDenseState(params, init_L, opt_state)\n</code></pre>"},{"location":"api/vi/dense/#posteriors.vi.dense.update","title":"<code>posteriors.vi.dense.update(state, batch, log_posterior, optimizer, temperature=1.0, n_samples=1, stl=True, inplace=False)</code>","text":"<p>Updates the variational parameters to minimize the NELBO.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>VIDenseState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior (which can be unnormalised).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer for updating the variational parameters. Make sure to use lower case like torchopt.adam()</p> required <code>temperature</code> <code>float</code> <p>Temperature to rescale (divide) log_posterior. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>n_samples</code> <code>int</code> <p>Number of samples to use for Monte Carlo estimate.</p> <code>1</code> <code>stl</code> <code>bool</code> <p>Whether to use the stick-the-landing estimator from (Roeder et al](https://arxiv.org/abs/1703.09194).</p> <code>True</code> <code>inplace</code> <code>bool</code> <p>Whether to modify state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[VIDenseState, TensorTree]</code> <p>Updated DenseVIState and auxiliary information.</p> Source code in <code>posteriors/vi/dense.py</code> <pre><code>def update(\n    state: VIDenseState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    optimizer: torchopt.base.GradientTransformation,\n    temperature: float = 1.0,\n    n_samples: int = 1,\n    stl: bool = True,\n    inplace: bool = False,\n) -&gt; tuple[VIDenseState, TensorTree]:\n    \"\"\"Updates the variational parameters to minimize the NELBO.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior (which can be unnormalised).\n        optimizer: TorchOpt functional optimizer for updating the variational\n            parameters. Make sure to use lower case like torchopt.adam()\n        temperature: Temperature to rescale (divide) log_posterior.\n            Scalar or schedule (callable taking step index, returning scalar).\n        n_samples: Number of samples to use for Monte Carlo estimate.\n        stl: Whether to use the stick-the-landing estimator\n            from (Roeder et al](https://arxiv.org/abs/1703.09194).\n        inplace: Whether to modify state in place.\n\n    Returns:\n        Updated DenseVIState and auxiliary information.\n    \"\"\"\n\n    temperature = temperature(state.step) if callable(temperature) else temperature\n\n    def nelbo_L_factor(m, L_flat):\n        return nelbo(m, L_flat, batch, log_posterior, temperature, n_samples, stl)\n\n    with torch.no_grad(), CatchAuxError():\n        nelbo_grads, (nelbo_val, aux) = grad_and_value(\n            nelbo_L_factor, argnums=(0, 1), has_aux=True\n        )(state.params, state.L_factor)\n\n    updates, opt_state = optimizer.update(\n        nelbo_grads,\n        state.opt_state,\n        params=[state.params, state.L_factor],\n        inplace=inplace,\n    )\n    mean, L_factor = torchopt.apply_updates(\n        (state.params, state.L_factor), updates, inplace=inplace\n    )\n\n    if inplace:\n        tree_insert_(state.nelbo, nelbo_val.detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n\n    return VIDenseState(\n        mean, L_factor, opt_state, nelbo_val.detach(), state.step + 1\n    ), aux\n</code></pre>"},{"location":"api/vi/dense/#posteriors.vi.dense.nelbo","title":"<code>posteriors.vi.dense.nelbo(mean, L_factor, batch, log_posterior, temperature=1.0, n_samples=1, stl=True)</code>","text":"<p>Returns the negative evidence lower bound (NELBO) for a Normal variational distribution over the parameters of a model.</p> <p>Monte Carlo estimate with <code>n_samples</code> from q. $$ \\text{NELBO} = - \ud835\udd3c_{q(\u03b8)}[\\log p(y|x, \u03b8) + \\log p(\u03b8) - \\log q(\u03b8) * T]) $$ for temperature \\(T\\).</p> <p><code>log_posterior</code> expects to take parameters and input batch and return a scalar as well as a TensorTree of any auxiliary information:</p> <pre><code>log_posterior_eval, aux = log_posterior(params, batch)\n</code></pre> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data and variable batch size.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>TensorTree</code> <p>Mean of the variational distribution.</p> required <code>L_factor</code> <code>Tensor</code> <p>Flat representation of the nonzero values of the lower triangular matrix \\(L\\) satisfying \\(LL^T\\) = \\(\\Sigma\\), where \\(\\Sigma\\) is the covariance matrix of the variational distribution.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior (which can be unnormalised).</p> required <code>temperature</code> <code>float</code> <p>Temperature to rescale (divide) log_posterior.</p> <code>1.0</code> <code>n_samples</code> <code>int</code> <p>Number of samples to use for Monte Carlo estimate.</p> <code>1</code> <code>stl</code> <code>bool</code> <p>Whether to use the stick-the-landing estimator from (Roeder et al](https://arxiv.org/abs/1703.09194).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Any]</code> <p>The sampled approximate NELBO averaged over the batch.</p> Source code in <code>posteriors/vi/dense.py</code> <pre><code>def nelbo(\n    mean: TensorTree,\n    L_factor: torch.Tensor,\n    batch: Any,\n    log_posterior: LogProbFn,\n    temperature: float = 1.0,\n    n_samples: int = 1,\n    stl: bool = True,\n) -&gt; Tuple[float, Any]:\n    \"\"\"Returns the negative evidence lower bound (NELBO) for a Normal\n    variational distribution over the parameters of a model.\n\n    Monte Carlo estimate with `n_samples` from q.\n    $$\n    \\\\text{NELBO} = - \ud835\udd3c_{q(\u03b8)}[\\\\log p(y|x, \u03b8) + \\\\log p(\u03b8) - \\\\log q(\u03b8) * T])\n    $$\n    for temperature $T$.\n\n    `log_posterior` expects to take parameters and input batch and return a scalar\n    as well as a TensorTree of any auxiliary information:\n\n    ```\n    log_posterior_eval, aux = log_posterior(params, batch)\n    ```\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data and variable batch size.\n\n    Args:\n        mean: Mean of the variational distribution.\n        L_factor: Flat representation of the nonzero values of the lower\n            triangular matrix $L$ satisfying $LL^T$ = $\\\\Sigma$, where $\\\\Sigma$\n            is the covariance matrix of the variational distribution.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior (which can be unnormalised).\n        temperature: Temperature to rescale (divide) log_posterior.\n        n_samples: Number of samples to use for Monte Carlo estimate.\n        stl: Whether to use the stick-the-landing estimator\n            from (Roeder et al](https://arxiv.org/abs/1703.09194).\n\n    Returns:\n        The sampled approximate NELBO averaged over the batch.\n    \"\"\"\n\n    mean_flat, unravel_func = tree_ravel(mean)\n    L = L_from_flat(L_factor)\n    cov = L @ L.T\n    dist = torch.distributions.MultivariateNormal(\n        loc=mean_flat,\n        covariance_matrix=cov,\n        validate_args=False,\n    )\n\n    sampled_params = dist.rsample((n_samples,))\n    sampled_params_tree = torch.vmap(lambda s: unravel_func(s))(sampled_params)\n\n    if stl:\n        mean_flat.detach()\n        L = L_from_flat(L_factor.detach())\n        cov = L @ L.T\n        # Redefine distribution to sample from after stl\n        dist = torch.distributions.MultivariateNormal(\n            loc=mean_flat,\n            covariance_matrix=cov,\n            validate_args=False,\n        )\n\n    # Don't use vmap for single sample, since vmap doesn't work with lots of models\n    if n_samples == 1:\n        single_param = tree_map(lambda x: x[0], sampled_params_tree)\n        single_param_flat, _ = tree_ravel(single_param)\n        log_p, aux = log_posterior(single_param, batch)\n        log_q = dist.log_prob(single_param_flat)\n\n    else:\n        log_p, aux = vmap(log_posterior, (0, None), (0, 0))(sampled_params_tree, batch)\n        log_q = dist.log_prob(sampled_params)\n\n    return -(log_p - log_q * temperature).mean(), aux\n</code></pre>"},{"location":"api/vi/dense/#posteriors.vi.dense.sample","title":"<code>posteriors.vi.dense.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Single sample from Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>VIDenseState</code> <p>State encoding mean and covariance matrix.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from Normal distribution.</p> Source code in <code>posteriors/vi/dense.py</code> <pre><code>def sample(\n    state: VIDenseState, sample_shape: torch.Size = torch.Size([])\n) -&gt; TensorTree:\n    \"\"\"Single sample from Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and covariance matrix.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from Normal distribution.\n    \"\"\"\n\n    mean_flat, unravel_func = tree_ravel(state.params)\n    L = L_from_flat(state.L_factor)\n    cov = L @ L.T\n\n    samples = torch.distributions.MultivariateNormal(\n        loc=mean_flat,\n        covariance_matrix=cov,\n        validate_args=False,\n    ).rsample(sample_shape)\n\n    samples = torch.vmap(unravel_func)(samples)\n    return samples\n</code></pre>"},{"location":"api/vi/diag/","title":"VI Diag","text":""},{"location":"api/vi/diag/#posteriors.vi.diag.build","title":"<code>posteriors.vi.diag.build(log_posterior, optimizer, temperature=1.0, n_samples=1, stl=True, init_log_sds=0.0)</code>","text":"<p>Builds a transform for variational inference with a diagonal Normal distribution over parameters.</p> <p>Find \\(\\mu\\) and diagonal \\(\\Sigma\\) that mimimize \\(\\text{KL}(N(\u03b8| \\mu, \\Sigma) || p_T(\u03b8))\\) where \\(p_T(\u03b8) \\propto \\exp( \\log p(\u03b8) / T)\\) with temperature \\(T\\).</p> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data.</p> <p>For more information on variational inference see Blei et al, 2017.</p> <p>Parameters:</p> Name Type Description Default <code>log_posterior</code> <code>Callable[[TensorTree, Any], float]</code> <p>Function that takes parameters and input batch and returns the log posterior (which can be unnormalised).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer for updating the variational parameters. Make sure to use lower case like torchopt.adam()</p> required <code>temperature</code> <code>float | Schedule</code> <p>Temperature to rescale (divide) log_posterior. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>n_samples</code> <code>int</code> <p>Number of samples to use for Monte Carlo estimate.</p> <code>1</code> <code>stl</code> <code>bool</code> <p>Whether to use the stick-the-landing estimator from Roeder et al.</p> <code>True</code> <code>init_log_sds</code> <code>TensorTree | float</code> <p>Initial log of the square-root diagonal of the covariance matrix of the variational distribution. Can be a tree matching params or scalar.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Transform</code> <p>Diagonal VI transform instance.</p> Source code in <code>posteriors/vi/diag.py</code> <pre><code>def build(\n    log_posterior: Callable[[TensorTree, Any], float],\n    optimizer: torchopt.base.GradientTransformation,\n    temperature: float | Schedule = 1.0,\n    n_samples: int = 1,\n    stl: bool = True,\n    init_log_sds: TensorTree | float = 0.0,\n) -&gt; Transform:\n    \"\"\"Builds a transform for variational inference with a diagonal Normal\n    distribution over parameters.\n\n    Find $\\\\mu$ and diagonal $\\\\Sigma$ that mimimize $\\\\text{KL}(N(\u03b8| \\\\mu, \\\\Sigma) || p_T(\u03b8))$\n    where $p_T(\u03b8) \\\\propto \\\\exp( \\\\log p(\u03b8) / T)$ with temperature $T$.\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data.\n\n    For more information on variational inference see [Blei et al, 2017](https://arxiv.org/abs/1601.00670).\n\n    Args:\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior (which can be unnormalised).\n        optimizer: TorchOpt functional optimizer for updating the variational\n            parameters. Make sure to use lower case like torchopt.adam()\n        temperature: Temperature to rescale (divide) log_posterior.\n            Scalar or schedule (callable taking step index, returning scalar).\n        n_samples: Number of samples to use for Monte Carlo estimate.\n        stl: Whether to use the stick-the-landing estimator\n            from [Roeder et al](https://arxiv.org/abs/1703.09194).\n        init_log_sds: Initial log of the square-root diagonal of the covariance matrix\n            of the variational distribution. Can be a tree matching params or scalar.\n\n    Returns:\n        Diagonal VI transform instance.\n    \"\"\"\n    init_fn = partial(init, optimizer=optimizer, init_log_sds=init_log_sds)\n    update_fn = partial(\n        update,\n        log_posterior=log_posterior,\n        optimizer=optimizer,\n        temperature=temperature,\n        n_samples=n_samples,\n        stl=stl,\n    )\n    return Transform(init_fn, update_fn)\n</code></pre>"},{"location":"api/vi/diag/#posteriors.vi.diag.VIDiagState","title":"<code>posteriors.vi.diag.VIDiagState</code>","text":"<p>               Bases: <code>TensorClass['frozen']</code></p> <p>State encoding a diagonal Normal variational distribution over parameters.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>TensorTree</code> <p>Mean of the variational distribution.</p> <code>log_sd_diag</code> <code>TensorTree</code> <p>Log of the square-root diagonal of the covariance matrix of the variational distribution.</p> <code>opt_state</code> <code>OptState</code> <p>TorchOpt state storing optimizer data for updating the variational parameters.</p> <code>nelbo</code> <code>Tensor</code> <p>Negative evidence lower bound (lower is better).</p> <code>step</code> <code>Tensor</code> <p>Current step count.</p> Source code in <code>posteriors/vi/diag.py</code> <pre><code>class VIDiagState(TensorClass[\"frozen\"]):\n    \"\"\"State encoding a diagonal Normal variational distribution over parameters.\n\n    Attributes:\n        params: Mean of the variational distribution.\n        log_sd_diag: Log of the square-root diagonal of the covariance matrix of the\n            variational distribution.\n        opt_state: TorchOpt state storing optimizer data for updating the\n            variational parameters.\n        nelbo: Negative evidence lower bound (lower is better).\n        step: Current step count.\n    \"\"\"\n\n    params: TensorTree\n    log_sd_diag: TensorTree\n    opt_state: torchopt.typing.OptState\n    nelbo: torch.Tensor = torch.tensor([])\n    step: torch.Tensor = torch.tensor(0)\n</code></pre>"},{"location":"api/vi/diag/#posteriors.vi.diag.init","title":"<code>posteriors.vi.diag.init(params, optimizer, init_log_sds=0.0)</code>","text":"<p>Initialise diagonal Normal variational distribution over parameters.</p> <p>optimizer.init will be called on flattened variational parameters so hyperparameters such as learning rate need to pre-specified through TorchOpt's functional API:</p> <pre><code>import torchopt\n\noptimizer = torchopt.adam(lr=1e-2)\nvi_state = init(init_mean, optimizer)\n</code></pre> <p>It's assumed maximize=False for the optimizer, so that we minimize the NELBO.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TensorTree</code> <p>Initial mean of the variational distribution.</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer for updating the variational parameters. Make sure to use lower case like torchopt.adam()</p> required <code>init_log_sds</code> <code>TensorTree | float</code> <p>Initial log of the square-root diagonal of the covariance matrix of the variational distribution. Can be a tree matching params or scalar.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>VIDiagState</code> <p>Initial DiagVIState.</p> Source code in <code>posteriors/vi/diag.py</code> <pre><code>def init(\n    params: TensorTree,\n    optimizer: torchopt.base.GradientTransformation,\n    init_log_sds: TensorTree | float = 0.0,\n) -&gt; VIDiagState:\n    \"\"\"Initialise diagonal Normal variational distribution over parameters.\n\n    optimizer.init will be called on flattened variational parameters so hyperparameters\n    such as learning rate need to pre-specified through TorchOpt's functional API:\n\n    ```\n    import torchopt\n\n    optimizer = torchopt.adam(lr=1e-2)\n    vi_state = init(init_mean, optimizer)\n    ```\n\n    It's assumed maximize=False for the optimizer, so that we minimize the NELBO.\n\n    Args:\n        params: Initial mean of the variational distribution.\n        optimizer: TorchOpt functional optimizer for updating the variational\n            parameters. Make sure to use lower case like torchopt.adam()\n        init_log_sds: Initial log of the square-root diagonal of the covariance matrix\n            of the variational distribution. Can be a tree matching params or scalar.\n\n    Returns:\n        Initial DiagVIState.\n    \"\"\"\n    if is_scalar(init_log_sds):\n        init_log_sds = tree_map(\n            lambda x: torch.full_like(x, init_log_sds, requires_grad=x.requires_grad),\n            params,\n        )\n\n    opt_state = optimizer.init([params, init_log_sds])\n    return VIDiagState(params, init_log_sds, opt_state)\n</code></pre>"},{"location":"api/vi/diag/#posteriors.vi.diag.update","title":"<code>posteriors.vi.diag.update(state, batch, log_posterior, optimizer, temperature=1.0, n_samples=1, stl=True, inplace=False)</code>","text":"<p>Updates the variational parameters to minimize the NELBO.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>VIDiagState</code> <p>Current state.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior (which can be unnormalised).</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>TorchOpt functional optimizer for updating the variational parameters. Make sure to use lower case like torchopt.adam()</p> required <code>temperature</code> <code>float</code> <p>Temperature to rescale (divide) log_posterior. Scalar or schedule (callable taking step index, returning scalar).</p> <code>1.0</code> <code>n_samples</code> <code>int</code> <p>Number of samples to use for Monte Carlo estimate.</p> <code>1</code> <code>stl</code> <code>bool</code> <p>Whether to use the stick-the-landing estimator from (Roeder et al](https://arxiv.org/abs/1703.09194).</p> <code>True</code> <code>inplace</code> <code>bool</code> <p>Whether to modify state in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[VIDiagState, TensorTree]</code> <p>Updated DiagVIState and auxiliary information.</p> Source code in <code>posteriors/vi/diag.py</code> <pre><code>def update(\n    state: VIDiagState,\n    batch: Any,\n    log_posterior: LogProbFn,\n    optimizer: torchopt.base.GradientTransformation,\n    temperature: float = 1.0,\n    n_samples: int = 1,\n    stl: bool = True,\n    inplace: bool = False,\n) -&gt; tuple[VIDiagState, TensorTree]:\n    \"\"\"Updates the variational parameters to minimize the NELBO.\n\n    Args:\n        state: Current state.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior (which can be unnormalised).\n        optimizer: TorchOpt functional optimizer for updating the variational\n            parameters. Make sure to use lower case like torchopt.adam()\n        temperature: Temperature to rescale (divide) log_posterior.\n            Scalar or schedule (callable taking step index, returning scalar).\n        n_samples: Number of samples to use for Monte Carlo estimate.\n        stl: Whether to use the stick-the-landing estimator\n            from (Roeder et al](https://arxiv.org/abs/1703.09194).\n        inplace: Whether to modify state in place.\n\n    Returns:\n        Updated DiagVIState and auxiliary information.\n    \"\"\"\n    temperature = temperature(state.step) if callable(temperature) else temperature\n\n    def nelbo_log_sd(m, lsd):\n        sd_diag = tree_map(torch.exp, lsd)\n        return nelbo(m, sd_diag, batch, log_posterior, temperature, n_samples, stl)\n\n    with torch.no_grad(), CatchAuxError():\n        nelbo_grads, (nelbo_val, aux) = grad_and_value(\n            nelbo_log_sd, argnums=(0, 1), has_aux=True\n        )(state.params, state.log_sd_diag)\n\n    updates, opt_state = optimizer.update(\n        nelbo_grads,\n        state.opt_state,\n        params=[state.params, state.log_sd_diag],\n        inplace=inplace,\n    )\n    mean, log_sd_diag = torchopt.apply_updates(\n        (state.params, state.log_sd_diag), updates, inplace=inplace\n    )\n\n    if inplace:\n        tree_insert_(state.nelbo, nelbo_val.detach())\n        tree_insert_(state.step, state.step + 1)\n        return state, aux\n\n    return VIDiagState(\n        mean, log_sd_diag, opt_state, nelbo_val.detach(), state.step + 1\n    ), aux\n</code></pre>"},{"location":"api/vi/diag/#posteriors.vi.diag.nelbo","title":"<code>posteriors.vi.diag.nelbo(mean, sd_diag, batch, log_posterior, temperature=1.0, n_samples=1, stl=True)</code>","text":"<p>Returns the negative evidence lower bound (NELBO) for a diagonal Normal variational distribution over the parameters of a model.</p> <p>Monte Carlo estimate with <code>n_samples</code> from q. $$ \\text{NELBO} = - \ud835\udd3c_{q(\u03b8)}[\\log p(y|x, \u03b8) + \\log p(\u03b8) - \\log q(\u03b8) * T]) $$ for temperature \\(T\\).</p> <p><code>log_posterior</code> expects to take parameters and input batch and return a scalar as well as a TensorTree of any auxiliary information:</p> <pre><code>log_posterior_eval, aux = log_posterior(params, batch)\n</code></pre> <p>The log posterior and temperature are recommended to be constructed in tandem to ensure robust scaling for a large amount of data and variable batch size.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>TensorTree</code> <p>Mean of the variational distribution.</p> required <code>sd_diag</code> <code>TensorTree</code> <p>Square-root diagonal of the covariance matrix of the variational distribution.</p> required <code>batch</code> <code>Any</code> <p>Input data to log_posterior.</p> required <code>log_posterior</code> <code>LogProbFn</code> <p>Function that takes parameters and input batch and returns the log posterior (which can be unnormalised).</p> required <code>temperature</code> <code>float</code> <p>Temperature to rescale (divide) log_posterior.</p> <code>1.0</code> <code>n_samples</code> <code>int</code> <p>Number of samples to use for Monte Carlo estimate.</p> <code>1</code> <code>stl</code> <code>bool</code> <p>Whether to use the stick-the-landing estimator from (Roeder et al](https://arxiv.org/abs/1703.09194).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Any]</code> <p>The sampled approximate NELBO averaged over the batch.</p> Source code in <code>posteriors/vi/diag.py</code> <pre><code>def nelbo(\n    mean: TensorTree,\n    sd_diag: TensorTree,\n    batch: Any,\n    log_posterior: LogProbFn,\n    temperature: float = 1.0,\n    n_samples: int = 1,\n    stl: bool = True,\n) -&gt; Tuple[float, Any]:\n    \"\"\"Returns the negative evidence lower bound (NELBO) for a diagonal Normal\n    variational distribution over the parameters of a model.\n\n    Monte Carlo estimate with `n_samples` from q.\n    $$\n    \\\\text{NELBO} = - \ud835\udd3c_{q(\u03b8)}[\\\\log p(y|x, \u03b8) + \\\\log p(\u03b8) - \\\\log q(\u03b8) * T])\n    $$\n    for temperature $T$.\n\n    `log_posterior` expects to take parameters and input batch and return a scalar\n    as well as a TensorTree of any auxiliary information:\n\n    ```\n    log_posterior_eval, aux = log_posterior(params, batch)\n    ```\n\n    The log posterior and temperature are recommended to be [constructed in tandem](../../log_posteriors.md)\n    to ensure robust scaling for a large amount of data and variable batch size.\n\n    Args:\n        mean: Mean of the variational distribution.\n        sd_diag: Square-root diagonal of the covariance matrix of the\n            variational distribution.\n        batch: Input data to log_posterior.\n        log_posterior: Function that takes parameters and input batch and\n            returns the log posterior (which can be unnormalised).\n        temperature: Temperature to rescale (divide) log_posterior.\n        n_samples: Number of samples to use for Monte Carlo estimate.\n        stl: Whether to use the stick-the-landing estimator\n            from (Roeder et al](https://arxiv.org/abs/1703.09194).\n\n    Returns:\n        The sampled approximate NELBO averaged over the batch.\n    \"\"\"\n    sampled_params = diag_normal_sample(mean, sd_diag, sample_shape=(n_samples,))\n    if stl:\n        mean = tree_map(lambda x: x.detach(), mean)\n        sd_diag = tree_map(lambda x: x.detach(), sd_diag)\n\n    # Don't use vmap for single sample, since vmap doesn't work with lots of models\n    if n_samples == 1:\n        single_param = tree_map(lambda x: x[0], sampled_params)\n        log_p, aux = log_posterior(single_param, batch)\n        log_q = diag_normal_log_prob(single_param, mean, sd_diag)\n    else:\n        log_p, aux = vmap(log_posterior, (0, None), (0, 0))(sampled_params, batch)\n        log_q = vmap(diag_normal_log_prob, (0, None, None))(\n            sampled_params, mean, sd_diag\n        )\n    return -(log_p - log_q * temperature).mean(), aux\n</code></pre>"},{"location":"api/vi/diag/#posteriors.vi.diag.sample","title":"<code>posteriors.vi.diag.sample(state, sample_shape=torch.Size([]))</code>","text":"<p>Single sample from diagonal Normal distribution over parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>VIDiagState</code> <p>State encoding mean and log standard deviations.</p> required <code>sample_shape</code> <code>Size</code> <p>Shape of the desired samples.</p> <code>Size([])</code> <p>Returns:</p> Type Description <code>TensorTree</code> <p>Sample(s) from Normal distribution.</p> Source code in <code>posteriors/vi/diag.py</code> <pre><code>def sample(state: VIDiagState, sample_shape: torch.Size = torch.Size([])) -&gt; TensorTree:\n    \"\"\"Single sample from diagonal Normal distribution over parameters.\n\n    Args:\n        state: State encoding mean and log standard deviations.\n        sample_shape: Shape of the desired samples.\n\n    Returns:\n        Sample(s) from Normal distribution.\n    \"\"\"\n    sd_diag = tree_map(torch.exp, state.log_sd_diag)\n    return diag_normal_sample(state.params, sd_diag, sample_shape=sample_shape)\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We list here a view small examples, that are easy to run locally on a CPU,  to show how to use <code>posteriors</code>:</p> <ul> <li>Visualizing VI and SGHMC </li> <li>EKF for Premier League football </li> <li>Autoencoder with Lightning </li> </ul> <p>Additionally, we provide more in depth examples in the examples directory on GitHub, many of which use larger models that require a GPU to run:</p> <p></p> <ul> <li><code>bayes_llama3</code>: Uses <code>posteriors</code> and <code>lightning</code> to train a bayesian ensemble language model Llama-3-8b on the TQA dataset.</li> </ul> <p></p> <ul> <li><code>continual_lora</code>: Uses <code>posteriors.laplace.diag_fisher</code> to avoid catastrophic forgetting in fine-tuning Llama-2-7b on a series of books from the pg19 dataset.</li> </ul> <p></p> <ul> <li><code>yelp</code>: Compares a host of <code>posteriors</code> methods (highlighting the easy exchangeability) on a sentiment analysis task adapted from the Hugging Face tutorial.</li> </ul> <p></p> <ul> <li><code>imdb</code>: Investigates cold posterior effect for a range of approximate Bayesian methods with a CNN-LSTM model on IMDB data, with some interesting takeaways.</li> </ul> <p></p> <ul> <li><code>continual_regression</code>: Variational continual learning notebook for a simple regression task that's easy to visualize.</li> </ul> <p></p> <ul> <li><code>pyro_pima_indians_sghmc</code>: An accessible notebook demonstrating the use of <code>posteriors</code> with a <code>pyro</code>-defined Bayesian logistic regression model, as well as convergence diagnostics from <code>pyro</code>.</li> </ul>"},{"location":"tutorials/ekf_premier_league/","title":"Extended Kalman filter for Premier League football","text":"<p>In this example, we'll use the extended Kalman filter implementation from <code>posteriors</code> to infer the skills of Premier League football teams. We'll use a simple Elo-style Bayesian model from Duffield et al to model the outcome of matches.</p>"},{"location":"tutorials/ekf_premier_league/#data","title":"Data","text":"<p>We'll use the football-data.co.uk and  <code>pandas</code> to load the Premier League results for the 2021/22 and 2022/2023 seasons.</p> Code to download Premier League data <pre><code>import torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport posteriors\n\n\ndef download_data(start=21, end=23):\n    urls = [\n        f\"https://www.football-data.co.uk/mmz4281/{y}{y+1}/E0.csv\"\n        for y in range(start, end)\n    ]\n\n    origin_date = pd.to_datetime(f\"20{start}-08-01\")\n    data = pd.concat(pd.read_csv(url) for url in urls)\n    data = data.dropna()\n    data[\"Timestamp\"] = pd.to_datetime(data[\"Date\"], dayfirst=True)\n    data[\"Timestamp\"] = pd.to_datetime(data[\"Timestamp\"], unit=\"D\")\n    data[\"TimestampDays\"] = (data[\"Timestamp\"] - origin_date).dt.days.astype(int)\n\n    players_arr = pd.unique(pd.concat([data[\"HomeTeam\"], data[\"AwayTeam\"]]))\n    players_arr.sort()\n    players_name_to_id_dict = {a: i for i, a in enumerate(players_arr)}\n    players_id_to_name_dict = {i: a for i, a in enumerate(players_arr)}\n\n    data[\"HomeTeamID\"] = data[\"HomeTeam\"].apply(lambda s: players_name_to_id_dict[s])\n    data[\"AwayTeamID\"] = data[\"AwayTeam\"].apply(lambda s: players_name_to_id_dict[s])\n\n    match_times = torch.tensor(data[\"TimestampDays\"].to_numpy(), dtype=torch.float64)\n    match_player_indices = torch.tensor(data[[\"HomeTeamID\", \"AwayTeamID\"]].to_numpy())\n\n    home_goals = torch.tensor(data[\"FTHG\"].to_numpy())\n    away_goals = torch.tensor(data[\"FTAG\"].to_numpy())\n\n    match_results = torch.where(\n        home_goals &gt; away_goals, 1, torch.where(home_goals &lt; away_goals, 2, 0)\n    )\n\n    dataset = torch.utils.data.StackDataset(\n        match_times=match_times,\n        match_player_indices=match_player_indices,\n        match_results=match_results,\n    )\n\n    return (\n        dataset,\n        players_id_to_name_dict,\n        players_name_to_id_dict,\n    )\n</code></pre> <p>We can load the dataset into a torch <code>DataLoader</code> as follows:</p> <pre><code>(\n    dataset,\n    players_id_to_name_dict,\n    players_name_to_id_dict,\n) = download_data()\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n</code></pre>"},{"location":"tutorials/ekf_premier_league/#define-the-model","title":"Define the model","text":"<p>We'll define a likelihood function that maps the skills of the teams to the probability of a draw, home win or away win. Following Duffield et al, we'll use a simple sigmoidal model:</p> \\[\\begin{aligned} p\\left(y \\mid x^h, x^a\\right)  = \\begin{cases}     \\textbf{sigmoid} \\left(x^h - x^a + \\epsilon\\right) - \\textbf{sigmoid}\\left(x^h - x^a - \\epsilon \\right) &amp; \\text{if } y = \\text{draw}, \\\\     \\textbf{sigmoid} \\left(x^h - x^a - \\epsilon \\right) &amp; \\text{if } y = \\mathrm{h},\\\\     1-\\textbf{sigmoid} \\left(x^h - x^a + \\epsilon \\right) &amp; \\text{if } y = \\mathrm{a}. \\end{cases} \\end{aligned}\\] <p>Where \\(y\\) is the match result, \\(x^h\\) and \\(x^a\\) are the (latent) skills of the home and away teams. \\(\\epsilon\\) is a static parameter controlling the probability of a draw and  \\(\\textbf{sigmoid}\\) is a sigmoid function that maps real values to the interval \\([0, 1]\\).</p> <p>In code: <pre><code>epsilon = 1.0\n\ndef log_likelihood(params, batch):\n    player_indices = batch[\"match_player_indices\"]\n    match_results = batch[\"match_results\"]\n\n    player_skills = params[player_indices]\n\n    home_win_prob = torch.sigmoid(player_skills[:, 0] - player_skills[:, 1] - epsilon)\n    away_win_prob = 1 - torch.sigmoid(\n        player_skills[:, 0] - player_skills[:, 1] + epsilon\n    )\n    draw_prob = 1 - home_win_prob - away_win_prob\n    result_probs = torch.vstack([draw_prob, home_win_prob, away_win_prob]).T\n    log_liks = torch.log(result_probs[torch.arange(len(match_results)), match_results])\n    return log_liks, result_probs\n</code></pre> We've chosen \\(\\epsilon = 1.0\\) for this example, this gives a draw probability\u22480.5 for equally skilled teams which seems reasonable but ideally we'd like to learn this parameter from data too.</p>"},{"location":"tutorials/ekf_premier_league/#extended-kalman-time","title":"Extended Kalman time!","text":"<p>Now we'll run an extended Kalman filter to infer the skills of the teams sequentially over the matches.</p> <p>Because the matches are not equally spaced in time, we'll use a <code>transition_sd</code> parameter that varies as we move through the matches. This means we can't use <code>posteriors.ekf.diag_fisher.build</code> since this would globally configure the <code>transition_sd</code>, luckily we can use <code>posteriors.ekf.diag_fisher.init</code> and <code>posteriors.ekf.diag_fisher.update</code> directly instead.</p> <p><pre><code>transition_sd_scale = 0.1\nnum_teams = len(players_id_to_name_dict)\n\ninit_means = torch.zeros((num_teams,))\ninit_sds = torch.ones((num_teams,))\n\nstate = posteriors.ekf.diag_fisher.init(init_means, init_sds)\nall_means = init_means.unsqueeze(0)\nall_sds = init_sds.unsqueeze(0)\nprevious_time = 0.0\nfor match in dataloader:\n    match_time = match[\"match_times\"]\n    state, aux = posteriors.ekf.diag_fisher.update(\n        state,\n        match,\n        log_likelihood,\n        lr=1.0,\n        per_sample=True,\n        transition_sd=torch.sqrt(transition_sd_scale**2 * (match_time - previous_time)),\n    )\n    all_means = torch.vstack([all_means, state.params.unsqueeze(0)])\n    all_sds = torch.vstack([all_sds, state.sd_diag.unsqueeze(0)])\n    previous_time = match_time\n</code></pre> Again we'd ideally like to learn the <code>transition_sd_scale</code> parameter from data, but for this example we'll set it to 0.1 and see how it looks.</p> <p>Here we've stored the means and standard deviations of the skills of the teams at each time step, so we can plot them. This is doable as there's only 20 teams in the Premier League.</p>"},{"location":"tutorials/ekf_premier_league/#plot-the-skills","title":"Plot the skills","text":"<p>We initialized the skills of the teams to zero, which is not realistic, so we'll use the 2021/22 season as a warm-up period and plot the skills of the teams in the 2022/23 season.</p> Code to plot the skills <pre><code>last_season_start = (len(all_means) - 1) // 2\ntimes = dataset.datasets[\"match_times\"][last_season_start:]\nmeans = all_means[last_season_start + 1 :]\nsds = all_sds[last_season_start + 1 :]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nfor i in range(num_teams):\n    team_name = players_id_to_name_dict[i]\n    if team_name in (\"Arsenal\", \"Man City\"):\n        c = \"skyblue\" if team_name == \"Man City\" else \"red\"\n        ax.plot(times, means[:, i], c=c, zorder=2, label=team_name)\n        ax.fill_between(\n            times,\n            means[:, i] - sds[:, i],\n            means[:, i] + sds[:, i],\n            color=c,\n            alpha=0.3,\n            zorder=1,\n        )\n    else:\n        ax.plot(times, means[:, i], c=\"grey\", alpha=0.2, zorder=0, linewidth=0.75)\n\nax.set_xticklabels([])\nax.set_xlabel(\"2022/23 season\")\nax.set_ylabel(\"Skill\")\nax.legend()\nfig.tight_layout()\n</code></pre> <p></p> <p>Here we can see the inferred skills (and one standard deviation) for Arsenal and Manchester City with the skills of the other teams in grey. We can see that the extended Kalman filter has inferred that Arsenal were the strongest team through most of the season before deteriorating towards the end of the season where Manchester City came through as champions. Note the vacant period mid-season where the 2022 world cup took place.</p> <p>Observe the standard deviations increasing between matches, this is natural due to the increasing uncertainty as we move further from the last match. But as we observe more matches, the uncertainty decreases.</p> <p>Note</p> <p>The raw code for this example can be found in the repo at examples/ekf_premier_league.py.</p>"},{"location":"tutorials/lightning_autoencoder/","title":"Autoencoder with Lightning","text":"<p>In this example, we'll adapt the Lightning tutorial to use UQ methods with <code>posteriors</code> and logging + device handling with <code>lightning</code>.</p>"},{"location":"tutorials/lightning_autoencoder/#pytorch-model","title":"PyTorch model","text":"<p>We begin by defining the PyTorch model. This is unchanged from the Lightning tutorial:</p> <pre><code>import os\nfrom torch import nn, utils\nimport torch\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport lightning as L\nimport torchopt\n\nimport posteriors\n\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\nencoder_function = posteriors.model_to_function(encoder)\ndecoder_function = posteriors.model_to_function(decoder)\n</code></pre>"},{"location":"tutorials/lightning_autoencoder/#log-posterior","title":"Log posterior","text":"<p>As mentioned in the constructing log posteriors page, the <code>log_posterior</code> function depends on the amount of data we have in the training set, <code>num_data</code>. We don't know that yet so we'll define it later.</p> <p>Otherwise, the negative log likelihood is the same reconstruction loss as in the Lightning tutorial:</p> <pre><code>def log_posterior(params, batch, num_data):\n    x, y = batch\n    x = x.view(x.size(0), -1)\n    z = encoder_function(params[0], x)\n    x_hat = decoder_function(params[1], z)\n    log_lik = (\n        torch.distributions.Normal(x_hat, 1, validate_args=False)\n        .log_prob(x)\n        .sum(-1)\n        .mean()\n    )\n    log_prior = posteriors.diag_normal_log_prob(\n        params[0]\n    ) + posteriors.diag_normal_log_prob(params[1])\n    return log_lik + log_prior / num_data, x_hat\n</code></pre>"},{"location":"tutorials/lightning_autoencoder/#posteriors-method","title":"<code>posteriors</code> method","text":"<p>We can now define the <code>posteriors</code> method. For example, we could us <code>vi.diag</code> <pre><code>method, config_args = posteriors.vi.diag, {\"optimizer\": torchopt.adam(lr=1e-3)}\n</code></pre> or <code>sgmcmc.sghmc</code>: <pre><code>method, config_args = posteriors.sgmcmc.sghmc, {\"lr\": 1e-3}\n</code></pre> We can easily swap methods using <code>posteriors</code>'s unified interface.</p>"},{"location":"tutorials/lightning_autoencoder/#lightning-module","title":"Lightning module","text":"<p>The <code>LightningModule</code> is the same as in the Lightning tutorial, with a few minor modifications:</p> <ul> <li>We add a <code>num_data</code> attribute to the class, as well as a <code>log_posterior</code> method that depends on it.</li> <li>The training step now simply calls <code>update</code> and logs the float attributes.</li> <li>We use <code>configure_optimizers</code> to build the <code>transform</code> object and the <code>state</code>. We do not return optimizers, as we do not want lightning's automatic optimization.</li> <li>We load the number of data points in the training set in <code>on_train_start</code>, when the module has <code>train_dataloader</code> available.</li> <li>We save and load the state of the transform in <code>on_save_checkpoint</code> and <code>on_load_checkpoint</code> so that we can resume training if needed.</li> </ul> <pre><code>class LitAutoEncoderUQ(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.num_data = None\n\n    def log_posterior(self, params, batch):\n        return log_posterior(params, batch, self.num_data)\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        self.state, _ = self.transform.update(self.state, batch, inplace=True)\n        # Logging to TensorBoard (if installed) by default\n        for k, v in self.state._asdict().items():\n            if isinstance(v, float) or (isinstance(v, torch.Tensor) and v.numel() == 1):\n                self.log(k, v)\n\n    def configure_optimizers(self):\n        # We don't need to return optimizers here, as we are using the `transform` object\n        # rather than lightning's automatic optimization\n        self.transform = method.build(self.log_posterior, **config_args)\n        all_params = [\n            dict(self.encoder.named_parameters()),\n            dict(self.decoder.named_parameters()),\n        ]\n        self.state = self.transform.init(all_params)\n\n    def on_train_start(self) -&gt; None:\n        # Load the number of data points used for log_posterior\n        self.num_data = len(self.trainer.train_dataloader.dataset)\n\n    def on_save_checkpoint(self, checkpoint):\n        # Save the state of the transform\n        checkpoint[\"state\"] = self.state\n\n    def on_load_checkpoint(self, checkpoint):\n        # Load the state of the transform\n        self.state = checkpoint[\"state\"]\n</code></pre>"},{"location":"tutorials/lightning_autoencoder/#load-dataset-and-train","title":"Load dataset and train!","text":"<p>Just as in the Lightning tutorial:</p> <pre><code># init the autoencoder\nautoencoderuq = LitAutoEncoderUQ(encoder, decoder)\n\n# setup data\ndataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\ntrain_loader = utils.data.DataLoader(dataset)\n\n# train the model\ntrainer = L.Trainer(limit_train_batches=100, max_epochs=1)\ntrainer.fit(model=autoencoderuq, train_dataloaders=train_loader)\n</code></pre>"},{"location":"tutorials/visualizing_vi_sghmc/","title":"Visualizing VI and SGHMC on a Multimodal Distribution","text":"<p>In this example, we'll compare <code>vi.diag</code> and <code>sgmcmc.sghmc</code> for approximating a two dimensional distribution which we can visualize.</p>"},{"location":"tutorials/visualizing_vi_sghmc/#target-distribution","title":"Target distribution","text":"<p>We'll start by defining the target distribution, a two dimensional double well:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport torchopt\nimport posteriors\n\ntorch.manual_seed(42)\n\ndef log_posterior(x, batch):\n    log_prob = -torch.sum(x**4, axis=-1) / 10.0 + torch.sum(x**2, axis=-1)\n    return log_prob, torch.tensor([])\n</code></pre> <p>Note that the <code>log_posterior</code> has to conform to the signature <code>log_posterior(params, batch) -&gt; log_prob, aux</code> where <code>params</code> is a tensor of shape <code>(batch_size, num_params)</code> and  <code>batch</code> is a dictionary of tensors. More info on the constructing log posteriors page.</p> <p>In this simple example we don't have varying batches so we'll just ignore that input. We also don't have any auxiliary information we'd like to keep hold of, so we just return an empty tensor.</p>"},{"location":"tutorials/visualizing_vi_sghmc/#variational-inference","title":"Variational Inference","text":"<p>Now we'll fit a diagonal Gaussian variational distribution. This is easy with <code>posteriors</code>:</p> <pre><code>vi_transform = posteriors.vi.diag.build(\n    log_posterior, optimizer=torchopt.adam(lr=1e-2), init_log_sds=-2.0\n)\nn_vi_steps = 2000\nvi_state = vi_transform.init(torch.zeros(2))\n\nnelbos = []\nfor _ in range(n_vi_steps):\n    vi_state, _ = vi_transform.update(vi_state, None)\n    nelbos.append(vi_state.nelbo.item())\n</code></pre> <p>Here, we've tracked the values of the negative ELBO (NELBO), let's have a look at them:</p> <pre><code>plt.plot(nelbos)\nplt.ylabel(\"NELBO\")\n</code></pre> <p></p> <p>Looks like it converged, but there's a fair amount of variance around the minima, maybe a Gaussian isn't a great fit for our target distribution....</p>"},{"location":"tutorials/visualizing_vi_sghmc/#sghmc","title":"SGHMC","text":"<p>Let's generate samples with <code>posteriors.vi.sgmcmc.sghmc</code> instead:</p> <pre><code>sghmc_transform = posteriors.sgmcmc.sghmc.build(log_posterior, lr=5e-2, alpha=1.0)\nn_sghmc_steps = 10000\nsghmc_state = sghmc_transform.init(torch.zeros(2))\n\nsamples = torch.zeros(1, 2)\nlog_posts = []\nfor _ in range(n_sghmc_steps):\n    sghmc_state, _ = sghmc_transform.update(sghmc_state, None)\n    samples = torch.cat([samples, sghmc_state.params.unsqueeze(0)], axis=0)\n    log_posts.append(sghmc_state.log_posterior.item())\n</code></pre> <p>Here we've tracked the values of the log posterior, let's have a look at them:</p> <pre><code>plt.plot(log_posts)\nplt.ylabel(\"SGHMC Log Posterior\")\n</code></pre> <p></p> <p>Certainly some exploration going on! We can see that the log posterior evaluations vary quite a lot during sampling but in the most part stay significantly higher that the initialization value. Hopefully this means we're exploring the modes of the distribution, but we'll find out for sure when we visualize the samples.</p>"},{"location":"tutorials/visualizing_vi_sghmc/#visualizing","title":"Visualizing","text":"<p>Time to visualize the learnt variational distribution and the samples generated by SGHMC:</p> Code to plot the distributions <pre><code>lim = 4\nx = torch.linspace(-lim, lim, 1000)\nX, Y = torch.meshgrid(x, x)\nZ = torch.vmap(log_posterior, in_dims=(0, None))(torch.stack([X, Y], axis=-1), None)[0]\nplt.contourf(X, Y, Z, levels=50, cmap=\"Purples\", alpha=0.3, zorder=-1)\n\nmean = vi_state.params\nsd_diag = torch.exp(vi_state.log_sd_diag)\nZ_gauss = torch.vmap(\n    lambda z: -torch.sum(torch.square((z - mean) / sd_diag), axis=-1) / 2.0,\n)(torch.stack([X, Y], axis=-1))\n\nplt.contour(X, Y, Z_gauss, levels=5, colors=\"black\", alpha=0.5)\nsghmc_samps = plt.scatter(\n    samples[:, 0], samples[:, 1], c=\"r\", s=0.5, alpha=0.5, label=\"SGHMC Samples\"\n)\n\nvi_legend_line = mlines.Line2D(\n    [], [], color=\"black\", label=\"VI\", alpha=0.5, linestyle=\"--\"\n)\nplt.legend(handles=[vi_legend_line, sghmc_samps])\nplt.xlim(-lim, lim)\nplt.ylim(-lim, lim)\n</code></pre> <p></p> <p>We can see the variational Gaussian ignores the multiple modes,  but SGHMC explores them well.</p> <p>Note</p> <p>The raw code for this example can be found in the repo at examples/double_well.py.</p>"}]}